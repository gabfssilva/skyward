{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Skyward","text":"<p> Cloud accelerators with a single decorator </p> <p> </p> <p>Skyward is a Python library for ephemeral accelerator compute. Spin up cloud accelerators (GPUs, TPUs, Trainium, and more), run your ML training code, and tear them down automatically. No infrastructure to manage.</p> <ul> <li>One decorator, any cloud. <code>@compute</code> makes any function remotely executable. AWS, RunPod, VastAI, and Verda with a unified API.</li> <li>Operators, not boilerplate. <code>&gt;&gt;</code> executes on one node, <code>@</code> broadcasts to all, <code>&amp;</code> runs in parallel. No job configs, no YAML.</li> <li>Ephemeral by default. Instances provision on demand and terminate automatically. Context managers guarantee cleanup.</li> <li>Multi-accelerator out of the box. GPUs, TPUs, Trainium \u2014 with PyTorch DDP, Keras 3, JAX, TensorFlow, and HuggingFace plugins.</li> <li>Spot-aware. Automatic spot instance selection, preemption detection, and replacement. Save 60-90% on compute costs.</li> <li>Built on Casty. Powered by a distributed, asyncio-based actor model for non-blocking orchestration.</li> </ul>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code>import skyward as sky\n\n@sky.compute\ndef train(data):\n    import torch\n    model = create_model().cuda()\n    return model.fit(data)\n\nwith sky.ComputePool(provider=sky.AWS(), accelerator=sky.accelerators.A100()) as pool:\n    result = train(my_data) &gt;&gt; pool\n# Accelerator terminated automatically\n</code></pre>"},{"location":"#execution-operators","title":"Execution operators","text":"Operator Syntax Description <code>&gt;&gt;</code> <code>fn() &gt;&gt; pool</code> Execute on single worker <code>@</code> <code>fn() @ pool</code> Broadcast to ALL workers <code>&amp;</code> <code>fn1() &amp; fn2() &gt;&gt; pool</code> Parallel execution <code>gather()</code> <code>gather(fn1(), fn2()) &gt;&gt; pool</code> Dynamic parallel execution"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li> Getting Started \u2014 Installation, credentials, and first examples</li> <li> Core Concepts \u2014 Programming model and ephemeral compute</li> <li> Providers \u2014 AWS, RunPod, VastAI, and Verda</li> <li> Accelerators \u2014 Accelerator selection guide</li> <li> Distributed Training \u2014 Multi-node with PyTorch, Keras, JAX</li> <li> API Reference \u2014 Full autodoc of all public types</li> </ul>"},{"location":"accelerators/","title":"Accelerators","text":"<p>Every cloud provider has its own naming scheme for GPU instances. AWS calls an A100 machine a <code>p4d.24xlarge</code>. RunPod uses a <code>gpuTypeId</code>. VastAI filters marketplace offers by GPU model. The <code>accelerator</code> parameter on <code>ComputePool</code> is Skyward's answer to this fragmentation: you describe the hardware you want, and the provider figures out how to get it.</p>"},{"location":"accelerators/#two-ways-to-specify","title":"Two ways to specify","text":"<p>The simplest form is a string:</p> <pre><code>sky.ComputePool(provider=sky.AWS(), accelerator=\"A100\")\n</code></pre> <p>This works, but it carries no metadata \u2014 no memory size, no count, no type safety. The richer form uses the factory functions under <code>sky.accelerators</code>:</p> <pre><code>sky.ComputePool(provider=sky.AWS(), accelerator=sky.accelerators.A100())\nsky.ComputePool(provider=sky.AWS(), accelerator=sky.accelerators.H100(count=4))\nsky.ComputePool(provider=sky.AWS(), accelerator=sky.accelerators.A100(memory=\"40GB\"))\n</code></pre> <p>Each factory returns an <code>Accelerator</code> dataclass \u2014 frozen, immutable, with <code>name</code>, <code>memory</code>, <code>count</code>, and optional <code>metadata</code> (CUDA versions, form factors). The factory populates defaults from an internal catalog, so <code>sky.accelerators.H100()</code> already knows it has 80GB of HBM3 without you specifying it.</p>"},{"location":"accelerators/#nvidia-datacenter-gpus","title":"NVIDIA datacenter GPUs","text":"<p>These are the workhorses of ML training and inference \u2014 available on AWS, RunPod, Verda, and VastAI.</p>"},{"location":"accelerators/#hopper-and-blackwell-current-generation","title":"Hopper and Blackwell (current generation)","text":"Factory Memory Architecture Notes <code>H100()</code> 40/80GB HBM3 Hopper Flagship training GPU. FP8 support. <code>H100(form_factor=\"SXM\")</code> 80GB Hopper High-bandwidth SXM variant. <code>H100(form_factor=\"NVL\")</code> 94GB Hopper NVLink 2-GPU module. <code>H200()</code> 141GB HBM3e Hopper 1.4-1.9x inference speedup vs H100. <code>GH200()</code> 96GB HBM3 Grace Hopper CPU+GPU superchip via NVLink-C2C. <code>B100()</code> 192GB HBM3e Blackwell FP4 support, 2nd-gen transformer engine. <code>B200()</code> 192GB HBM3e Blackwell Flagship Blackwell. 2.5x inference vs H100. <code>GB200()</code> 384GB HBM3e Grace Blackwell Grace CPU + 2x Blackwell GPUs."},{"location":"accelerators/#ampere-and-ada-lovelace","title":"Ampere and Ada Lovelace","text":"Factory Memory Notes <code>A100()</code> 80GB HBM2e Default. First GPU with TF32 and structural sparsity. <code>A100(memory=\"40GB\")</code> 40GB PCIe variant. <code>A100(count=8)</code> 8x 80GB Multi-GPU. <code>A800()</code> 80GB China-specific A100 variant. <code>A40()</code> 48GB GDDR6 Professional visualization + compute. <code>A10G()</code> 24GB GDDR6 AWS g5 instances. <code>L4()</code> 24GB GDDR6 Ada Lovelace. Replaces T4 for inference. <code>L40S()</code> 48GB GDDR6 Ada Lovelace. Compute-optimized."},{"location":"accelerators/#legacy-still-widely-available","title":"Legacy (still widely available)","text":"Factory Memory Notes <code>T4()</code> 16GB GDDR6 Turing. Cheapest option for dev/inference. <code>T4G()</code> 16GB ARM64 variant for AWS Graviton. <code>V100()</code> 16/32GB HBM2 Volta. <code>V100(memory=\"32GB\")</code> for the larger variant. <code>P100()</code> 16GB HBM2 Pascal. First HBM GPU for deep learning."},{"location":"accelerators/#consumer-gpus","title":"Consumer GPUs","text":"<p>Available primarily on marketplace providers like VastAI and RunPod. Useful for cost-effective inference and fine-tuning:</p> <pre><code># RTX 50 series (Blackwell)\nsky.accelerators.RTX_5090()    # 32GB\nsky.accelerators.RTX_5080()    # 16GB\n\n# RTX 40 series (Ada Lovelace)\nsky.accelerators.RTX_4090()    # 24GB \u2014 popular for fine-tuning\nsky.accelerators.RTX_4080()\nsky.accelerators.RTX_4070_Ti()\n\n# RTX 30 series (Ampere)\nsky.accelerators.RTX_3090()    # 24GB\nsky.accelerators.RTX_3080()\n\n# Older generations also available: RTX 20xx, GTX 16xx, GTX 10xx\n</code></pre> <p>Workstation GPUs like <code>RTX_A6000()</code> (48GB), <code>RTX_6000_Ada()</code>, and <code>RTX_PRO_6000()</code> are also supported.</p>"},{"location":"accelerators/#amd-instinct","title":"AMD Instinct","text":"<p>AMD's datacenter compute accelerators. Supported through VastAI and other marketplace providers:</p> <pre><code>pool = sky.ComputePool(\n    provider=sky.VastAI(),\n    accelerator=sky.accelerators.MI300X(),\n)\n</code></pre> Factory Memory Architecture Notes <code>MI300X()</code> 192GB HBM3 CDNA 3 Designed for LLMs. <code>MI300A()</code> 128GB CDNA 3 (APU) Integrated CPU+GPU. <code>MI250X()</code> 128GB HBM2e CDNA 2 HPC workloads. <code>MI250()</code> 128GB HBM2e CDNA 2 Training. <code>MI210()</code> 64GB HBM2e CDNA 2 Training. <code>MI100()</code> 32GB HBM2 CDNA 1 Compute."},{"location":"accelerators/#aws-trainium","title":"AWS Trainium","text":"<p>Custom silicon designed for training. Requires the NeuronX SDK:</p> <pre><code>pool = sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=sky.accelerators.Trainium2(),\n    image=sky.Image(pip=[\"torch-neuronx\"]),\n)\n</code></pre> Factory Memory Instance Notes <code>Trainium1()</code> 32GB trn1.* First gen. <code>Trainium2()</code> 64GB trn2.* 4x performance vs v1. <code>Trainium3()</code> 128GB \u2014 Latest generation."},{"location":"accelerators/#aws-inferentia","title":"AWS Inferentia","text":"<p>Custom silicon for cost-effective inference:</p> <pre><code>pool = sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=sky.accelerators.Inferentia2(),\n    image=sky.Image(pip=[\"torch-neuronx\"]),\n)\n</code></pre> Factory Memory Instance Notes <code>Inferentia1()</code> 8GB inf1.* Single model serving. <code>Inferentia2()</code> 32GB inf2.* High throughput."},{"location":"accelerators/#google-tpus","title":"Google TPUs","text":"<p>Tensor Processing Units for JAX and TensorFlow workloads. Available as individual chips or pre-configured slices:</p> <pre><code># Single TPU chip\nsky.accelerators.TPUv5p()\n\n# Multi-chip slices\nsky.accelerators.TPUv5p_8()    # 8-chip slice\nsky.accelerators.TPUv4_64()    # 64-chip slice\nsky.accelerators.TPUv3_32()    # 32-chip slice\n</code></pre> Factory Generation Notes <code>TPUv6()</code> 6th gen (2024) Latest. <code>TPUv5p()</code> 5th gen perf Training-optimized. <code>TPUv5e()</code> 5th gen eff Inference-optimized. <code>TPUv4()</code> 4th gen General. <code>TPUv3()</code> / <code>TPUv2()</code> Legacy Still available."},{"location":"accelerators/#habana-gaudi","title":"Habana Gaudi","text":"<p>Intel's Habana accelerators for deep learning:</p> Factory Memory Notes <code>Gaudi3()</code> 128GB HBM2e Latest generation. <code>Gaudi2()</code> 96GB HBM2e 2x performance vs Gaudi. <code>Gaudi()</code> \u2014 First gen."},{"location":"accelerators/#custom-accelerators","title":"Custom accelerators","text":"<p>For hardware not in the catalog \u2014 experimental chips, private clouds, or overriding defaults:</p> <pre><code>import skyward as sky\n\nmy_gpu = sky.accelerators.Custom(\"My-GPU\", memory=\"48GB\")\nmy_gpu = sky.accelerators.Custom(\"H100-Custom\", memory=\"80GB\", count=8, cuda_min=\"12.0\")\n</code></pre>"},{"location":"accelerators/#selection-guide","title":"Selection guide","text":""},{"location":"accelerators/#by-workload","title":"By workload","text":"Workload Recommended Why Inference (small models) T4, L4 Cost-effective, good throughput Inference (large models) L40S, A10G More memory Fine-tuning (LoRA) A100-40GB Sufficient for adapters Full fine-tuning A100-80GB, H100 Need full model in memory Pre-training 8x H100 Maximum compute Development T4 Cheapest option"},{"location":"accelerators/#by-model-size","title":"By model size","text":"Parameters Minimum Recommended &lt; 1B T4 (16GB) T4 1-7B A10G (24GB) A100-40GB 7-13B A100-40GB A100-80GB 13-70B A100-80GB 2x H100 70B+ 4x H100 8x H100"},{"location":"accelerators/#detecting-accelerators-at-runtime","title":"Detecting accelerators at runtime","text":"<p>Inside a <code>@sky.compute</code> function, <code>sky.instance_info()</code> reports what hardware the function is running on:</p> <pre><code>@sky.compute\ndef check_gpu():\n    import torch\n\n    info = sky.instance_info()\n\n    return {\n        \"cuda_available\": torch.cuda.is_available(),\n        \"device_count\": torch.cuda.device_count(),\n        \"device_name\": torch.cuda.get_device_name(0),\n        \"accelerators\": info.accelerators,\n        \"accelerator_info\": info.accelerator,\n    }\n</code></pre>"},{"location":"accelerators/#related-topics","title":"Related topics","text":"<ul> <li>Providers \u2014 AWS, RunPod, VastAI, Verda, and Container configuration</li> <li>Distributed Training \u2014 Multi-node training guides</li> <li>API Reference \u2014 Complete accelerator API</li> </ul>"},{"location":"architecture/","title":"Clustering","text":"<p>Skyward uses Casty as its distributed runtime. Casty is an actor-based framework for Python 3.12+ \u2014 a lightweight implementation of the actor model, inspired by Akka, built on top of asyncio. Every compute node in a Skyward pool runs a Casty <code>ClusteredActorSystem</code>, and together they form a peer-to-peer cluster that handles task execution, distributed state, and inter-node communication without any external coordination service.</p> <p>This page explains how the cluster is structured and why the actor model is a natural fit for ephemeral cloud orchestration.</p>"},{"location":"architecture/#why-actors","title":"Why actors","text":"<p>The pool lifecycle involves a lot of concurrent, independent activity. Multiple cloud instances boot at different speeds. SSH connections are established in parallel. Bootstrap scripts run simultaneously on different machines. Worker processes start up and begin accepting tasks. Some nodes might fail and need replacement. All of this happens concurrently, and different nodes progress through their state machines at different rates.</p> <p>The actor model handles this naturally. An actor is an isolated unit of state that communicates exclusively through messages. There are no shared locks, no thread pools coordinating through mutexes, no callbacks mutating global state. An actor receives a message, decides what to do, and optionally sends messages to other actors. When you need to track the lifecycle of 8 nodes \u2014 each at a different stage of booting, connecting, and bootstrapping \u2014 8 independent actors with their own state machines are easier to reason about than 8 threads sharing a pool of mutable state.</p> <p>Both Casty and Skyward are built on asyncio, so actors are cheap and message passing doesn't block threads. The system scales to hundreds of nodes without requiring proportional memory or CPU on your laptop.</p>"},{"location":"architecture/#how-the-cluster-forms","title":"How the cluster forms","text":"<p>When you enter a <code>ComputePool</code> context manager, Skyward creates a local Casty actor system on your machine and spawns a pool actor \u2014 the root of the supervision hierarchy. The pool actor asks the provider to launch instances, and for each one, it spawns a node actor to manage that instance's lifecycle.</p> <p>Each node actor, in turn, spawns an instance actor that handles the low-level work: polling the cloud API until the machine is running, opening an SSH tunnel, transferring the bootstrap script, installing dependencies, and starting the worker process. When the worker is ready, the instance actor reports back to the node actor, which reports back to the pool actor. Once all nodes are ready, the pool is open for business.</p> <p>On the remote side, each worker runs its own <code>ClusteredActorSystem</code> on port 25520. The instance actor on your laptop establishes an SSH tunnel \u2014 a local port forward to the remote machine's port 25520 \u2014 and connects a <code>ClusterClient</code> to it. It discovers the worker actor via Casty's service discovery (<code>ServiceKey(\"skyward-worker\")</code>) and from that point on, tasks are sent as actor messages over the tunneled TCP connection. There is no HTTP server, no REST API, no message broker \u2014 just Casty actor messaging over SSH.</p> <pre><code>graph LR\n    pool[\"&lt;b&gt;ComputePool&lt;/b&gt;&lt;br/&gt;(your machine)\"]\n    pool --&gt; provider\n\n    subgraph provider [\"Cloud Provider\"]\n        subgraph cluster [\"Casty Cluster\"]\n            direction LR\n            n0[\"Node 0\"] --- n1[\"Node 1\"] --- n2[\"Node 2\"] --- n3[\"Node 3\"]\n        end\n    end</code></pre> <p>Node 0 plays a special role: it's the head node. Once its instance is ready, it broadcasts its address to all other nodes so they can form a cluster. This is how distributed training frameworks (PyTorch DDP, JAX, etc.) discover each other \u2014 <code>MASTER_ADDR</code> always points to node 0.</p>"},{"location":"architecture/#the-actor-hierarchy","title":"The actor hierarchy","text":"<p>The full hierarchy on your local machine consists of four layers, each with a well-defined responsibility.</p> <p>The pool actor is the state machine that orchestrates everything. It progresses through <code>idle \u2192 requesting \u2192 provisioning \u2192 ready \u2192 stopping</code>. It holds references to all node actors and the task manager, and it's the entry point for all task submissions.</p> <p>The task manager dispatches tasks to nodes using round-robin scheduling. It handles backpressure through per-node concurrency slots (from the <code>worker</code> configuration), preventing the workers from being overwhelmed. When you call <code>train(10) &gt;&gt; pool</code>, the task manager picks the next node in the rotation and forwards the task.</p> <p>Node actors \u2014 one per instance \u2014 track each node's lifecycle: <code>idle \u2192 waiting \u2192 active</code>. If a spot instance is preempted, the node detects the loss, notifies the pool, and requests a replacement from the provider. Tasks that were in flight on the lost node are re-queued.</p> <p>Instance actors are the workhorses. Each one manages a single remote machine through its full lifecycle: <code>polling \u2192 connecting \u2192 bootstrapping \u2192 ready \u2192 joined</code>. It handles SSH tunnel establishment, bootstrap monitoring, worker startup, and serves as the bridge for task execution. When you send a task with <code>&gt;&gt;</code>, the instance actor serializes the function and arguments (cloudpickle + zlib), sends them over the SSH tunnel to the worker actor, waits for the result, and returns it through the reply chain.</p> <p>Communication between actors uses Casty's <code>tell</code> (fire-and-forget) and <code>ask</code> (request-reply with timeout) patterns. A <code>tell</code> is a one-way message \u2014 the sender doesn't wait for a response. An <code>ask</code> creates a temporary reply-to reference, sends it along with the message, and blocks until the recipient responds. The full path of a task \u2014 from <code>&gt;&gt; pool</code> on your laptop to the remote worker and back \u2014 is a chain of <code>ask</code> calls: pool actor \u2192 task manager \u2192 node actor \u2192 instance actor \u2192 remote worker \u2192 back through <code>reply_to</code> references.</p>"},{"location":"architecture/#distributed-state","title":"Distributed state","text":"<p>The cluster also powers Skyward's distributed collections. When you call <code>sky.dict(\"cache\")</code> inside a <code>@sky.compute</code> function, Casty creates a distributed map that is replicated across the cluster. Every node can read and write to it, and Casty handles replication and consistency automatically.</p> <p>This works because the worker processes on each node are part of the same <code>ClusteredActorSystem</code>. Distributed data structures are a natural extension of the actor model: each key-value pair (in the case of <code>sky.dict</code>) is managed by its own actor, and reads and writes are messages. The same message-passing infrastructure that carries task payloads between your laptop and the workers also carries collection operations between nodes.</p>"},{"location":"architecture/#why-casty","title":"Why Casty","text":"<p>Skyward needs a runtime that can form ad-hoc clusters from ephemeral cloud instances, execute functions remotely with request-reply semantics, provide distributed data structures, and handle node failures through heartbeats and failure detection. Casty provides all of this with a small footprint and Python-native async support.</p> <p>The alternative would be running a separate coordination service on every ephemeral cluster \u2014 Redis for state, a message broker for task routing, a custom protocol for function execution. That's more moving parts, more dependencies to install on each worker, and more failure modes to handle during the brief life of a training job. Casty collapses all of these into a single actor system that starts in milliseconds and communicates over plain TCP. For a cluster that might live for ten minutes to run a training job, that simplicity matters.</p>"},{"location":"architecture/#further-reading","title":"Further reading","text":"<ul> <li>Casty Documentation \u2014 Full reference for the actor framework</li> <li>Distributed Collections \u2014 Dict, set, counter, queue, barrier, lock</li> <li>Distributed Training \u2014 Multi-node training with PyTorch, Keras, and JAX</li> </ul>"},{"location":"concepts/","title":"Core concepts","text":"<p>Skyward's programming model is built on a simple idea: computation and location should be separate concerns. You write ordinary Python functions. You decide where they run \u2014 on which cloud, on how many machines, on which accelerators \u2014 at the call site, not inside the function itself. This separation is what makes the same code work on a laptop, a single GPU, or a cluster of eight H100s.</p> <p>This page walks through the concepts that make this possible.</p>"},{"location":"concepts/#lazy-computation","title":"Lazy computation","text":"<p>The central abstraction in Skyward is <code>@sky.compute</code>. It transforms a regular function into a lazy computation \u2014 calling the function no longer executes it. Instead, it produces a <code>PendingCompute</code> object: a frozen, serializable description of what to compute, without committing to where or when.</p> <pre><code>import skyward as sky\n\n@sky.compute\ndef train(epochs: int) -&gt; float:\n    model = build_model()\n    return model.fit(epochs=epochs)\n\npending = train(10)   # nothing runs \u2014 returns PendingCompute[float]\n</code></pre> <p>In functional programming, this pattern is known as reifying an effect. Calling <code>train(10)</code> doesn't produce a result \u2014 it produces a description of a computation that, when interpreted, will produce a result. The side effects (provisioning a GPU, transferring data, executing remotely) are not performed at the call site. They are deferred to an interpreter \u2014 in this case, the pool and its operators. This is the same idea behind <code>IO</code> in Haskell, <code>Effect</code> in ZIO, or <code>suspend</code> in Kotlin: separating the description of a program from its execution so that the runtime can decide how, where, and when to run it.</p> <p>Because <code>PendingCompute</code> is a value \u2014 not a running process \u2014 it can be serialized, sent over the network, and executed on a remote machine. It can also be composed: combined with other computations via <code>&amp;</code>, collected into a <code>gather()</code>, or stored and dispatched later. None of this triggers execution. The program you're building is a data structure that only materializes when you commit to a target with <code>&gt;&gt;</code> or <code>@</code>.</p> <p>The generic type is preserved throughout \u2014 <code>train(10)</code> produces <code>PendingCompute[float]</code>, and dispatching it returns <code>float</code>. Your type checker sees the same types whether the function runs locally or on a remote GPU.</p> <p>If you need to bypass this and run the original function directly (for testing, debugging, or local profiling), every decorated function exposes the unwrapped version via <code>.local</code>:</p> <pre><code>result = train.local(10)  # executes immediately, returns float\n</code></pre>"},{"location":"concepts/#the-pool","title":"The pool","text":"<p>A <code>ComputePool</code> is a context manager that represents a set of cloud instances with a defined lifecycle. When you enter the block, Skyward provisions the machines, installs dependencies, and establishes connectivity. When you exit \u2014 whether normally or through an exception \u2014 everything is torn down.</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"A100\",\n    nodes=4,\n    image=sky.Image(pip=[\"torch\", \"transformers\"]),\n) as pool:\n    result = train(10) &gt;&gt; pool\n# all instances terminated\n</code></pre> <p>This is what it means for compute to be ephemeral: the infrastructure exists only for the duration of the work. There are no machines to forget about, no environments that drift over time, no idle costs accumulating overnight. The pool's lifetime is the job's lifetime.</p> <p>This model fits ML workloads naturally. Training runs, fine-tuning jobs, hyperparameter sweeps, batch inference \u2014 these are all tasks with a beginning and an end. The pool captures that shape: provision what you need, do the work, release everything.</p>"},{"location":"concepts/#lifecycle","title":"Lifecycle","text":"<p>Behind the <code>with</code> block, the pool orchestrates a hierarchy of components \u2014 each with its own lifecycle \u2014 to bring a cluster from zero to ready.</p> <p>Internally, this orchestration is built on the actor model \u2014 specifically on Casty, a lightweight actor framework for Python. Each component in the hierarchy (pool, node, instance) is an actor: an isolated unit of state that communicates exclusively through messages. There are no shared locks, no thread pools coordinating through mutexes, no callbacks mutating global state. An actor receives a message, decides what to do, and optionally sends messages to other actors. This makes the concurrent lifecycle of multiple nodes and instances \u2014 each progressing through their own state machines at different speeds \u2014 straightforward to reason about. Both Casty and Skyward are built on top of asyncio, so actors are cheap and message passing doesn't block threads \u2014 the system scales to hundreds of nodes without requiring proportional memory or CPU on your laptop.</p> <p>Despite being fully asynchronous internally, Skyward exposes a synchronous API. The asyncio event loop runs in a background daemon thread, and every public method bridges into it via <code>run_coroutine_threadsafe</code>. This means you write normal, blocking Python \u2014 <code>result = task() &gt;&gt; pool</code> \u2014 while the orchestration underneath remains non-blocking and concurrent.</p> <p>A pool actor manages the overall process: it asks the provider (AWS, RunPod, VastAI, etc.) to prepare infrastructure and launch instances. Each launched instance becomes a node actor, and each node supervises an instance actor that handles the low-level work \u2014 polling the cloud API until the machine is running, opening an SSH tunnel, bootstrapping the environment, and starting the worker process.</p> <p>The full sequence looks like this:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Pool\n    participant Provider\n    participant Node\n    participant Instance\n\n    User-&gt;&gt;Pool: __enter__\n    Pool-&gt;&gt;Provider: offers(spec)\n    Provider--&gt;&gt;Pool: Offer (selected)\n    Pool-&gt;&gt;Provider: prepare(spec, offer)\n    Provider--&gt;&gt;Pool: Cluster\n    Pool-&gt;&gt;Provider: provision(cluster, N)\n    Provider--&gt;&gt;Pool: Instances\n\n    loop for each instance\n        Pool-&gt;&gt;Node: Provision\n        Node-&gt;&gt;Instance: spawn\n        Instance-&gt;&gt;Provider: get_instance() (poll)\n        Provider--&gt;&gt;Instance: running + IP\n        Instance-&gt;&gt;Instance: SSH tunnel\n        Instance-&gt;&gt;Instance: bootstrap environment\n        Instance-&gt;&gt;Instance: start worker\n        Instance--&gt;&gt;Node: InstanceBecameReady\n        Node--&gt;&gt;Pool: NodeBecameReady\n    end\n\n    Pool--&gt;&gt;User: pool ready\n\n    User-&gt;&gt;Pool: __exit__\n    Pool-&gt;&gt;Provider: terminate + teardown\n    Pool--&gt;&gt;User: done</code></pre> <p>The provider is only involved at the boundaries: preparing infrastructure, launching instances, polling status, and tearing down. Everything in between \u2014 SSH, bootstrap, worker startup, cluster formation \u2014 is handled by the instance actor, which means the same orchestration logic works identically across all providers.</p> <p>Node 0 plays a special role: it's the head node. Once its instance is ready, it broadcasts its address to all other nodes so they can form a cluster. This is how distributed training frameworks (PyTorch DDP, JAX, etc.) discover each other \u2014 and Skyward's plugins (<code>sky.plugins.torch()</code>, <code>sky.plugins.jax()</code>) configure these frameworks using the head node's address automatically.</p> <p>If a spot instance is preempted, the node detects the loss, notifies the pool, and requests a replacement from the provider. Tasks that were in flight are re-queued. From the user's perspective, the pool self-heals \u2014 though of course repeated preemptions will slow things down.</p> <p>The pool is also the dispatch target for computations. You don't configure job queues or submit YAML \u2014 you use Python operators to express how computations should be distributed across the pool's nodes.</p>"},{"location":"concepts/#workers","title":"Workers","text":"<p>Each node in the pool runs a worker \u2014 a long-lived process that receives serialized tasks, executes them, and sends results back. The <code>Worker</code> dataclass controls two things: how many tasks a node handles concurrently, and which execution backend runs them.</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=4,\n    worker=sky.Worker(concurrency=2),\n) as pool:\n    result = train(10) &gt;&gt; pool\n</code></pre> <p><code>concurrency</code> sets the number of task slots per node. Total parallelism across the cluster is <code>nodes * concurrency</code> \u2014 four nodes with <code>concurrency=2</code> means eight tasks running simultaneously.</p> <p>The <code>executor</code> field determines how those tasks run. The default is <code>\"process\"</code>: each task executes in a separate OS process via <code>ProcessPoolExecutor</code>, which means each has its own Python interpreter and its own GIL. CPU-bound Python code saturates all available cores \u2014 a 2-vCPU machine with two concurrent tasks reaches 100% utilization. This is the right choice for numerical computation, data transformation, and training loops.</p> <p>The alternative is <code>\"thread\"</code>, which runs tasks as threads inside the worker process. Threads share the GIL, so CPU-bound Python code is limited to one core regardless of concurrency. On the same 2-vCPU machine, you'd see ~50% CPU. Threads are the right choice for I/O-bound work (network calls, disk reads) and for C extensions that release the GIL (NumPy, PyTorch inference). Distributed collections (<code>sky.dict()</code>, <code>sky.counter()</code>, etc.) work with both executors.</p> <p>For a practical comparison with benchmarks, see the Worker Executors guide.</p>"},{"location":"concepts/#operators","title":"Operators","text":"<p>Most distributed computing frameworks require you to express execution through configuration files, job submission APIs, or method calls like <code>pool.submit(fn, args)</code>. Skyward takes a different approach: it uses Python's operator overloading to create a small vocabulary where the syntax itself communicates intent. The expression <code>train(10) &gt;&gt; pool</code> reads as \"send this computation to the pool\" \u2014 and that's exactly what it does.</p> <p>This works because <code>@sky.compute</code> returns a <code>PendingCompute</code> value, not a result. Operators are defined on <code>PendingCompute</code> (via <code>__rshift__</code>, <code>__matmul__</code>, <code>__and__</code>, <code>__gt__</code>), and each one triggers a different dispatch strategy on the pool. The pool serializes the function and arguments with cloudpickle, sends the payload to the appropriate worker(s) over SSH, waits for the result, deserializes it, and returns it to the caller.</p>"},{"location":"concepts/#execute-on-one-node","title":"<code>&gt;&gt;</code> \u2014 Execute on one node","text":"<p>The most common operation. Sends the computation to a single worker and blocks until the result is available:</p> <pre><code>result = train(10) &gt;&gt; pool\n</code></pre> <p>The pool selects the target node using round-robin scheduling. If you send ten tasks with <code>&gt;&gt;</code>, they'll be distributed evenly across the available nodes. This is the right operator for independent tasks that don't need to run on every node \u2014 hyperparameter trials, inference on different inputs, or any embarrassingly parallel workload.</p>"},{"location":"concepts/#broadcast-to-all-nodes","title":"<code>@</code> \u2014 Broadcast to all nodes","text":"<p>Sends the same computation to every node in the pool. Returns a list with one result per node:</p> <pre><code>with sky.ComputePool(provider=sky.AWS(), nodes=4) as pool:\n    # runs on all 4 nodes, returns list of 4 results\n    results = initialize_model(config) @ pool\n</code></pre> <p>Broadcast is the foundation for distributed training patterns. When combined with <code>sky.shard()</code> inside the function, each node receives the full arguments but operates on its own partition of the data. The function body is identical across nodes \u2014 the differentiation happens at runtime based on <code>sky.instance_info()</code>.</p> <p>This is also the natural way to run setup or teardown operations that must happen on every machine: loading a model into GPU memory, warming up a cache, or writing checkpoints to local disk.</p>"},{"location":"concepts/#parallel-composition","title":"<code>&amp;</code> \u2014 Parallel composition","text":"<p>Combines multiple different computations into a group that executes in parallel. Results are returned as a tuple with full type inference:</p> <pre><code>a, b, c = (preprocess() &amp; train() &amp; evaluate()) &gt;&gt; pool\n</code></pre> <p>The distinction from broadcast is important: <code>@</code> runs the same function on all nodes, while <code>&amp;</code> runs different functions concurrently. Each computation in the group may go to a different node (round-robin), and the group blocks until all of them complete. The types are preserved individually \u2014 if <code>preprocess</code> returns <code>DataFrame</code>, <code>train</code> returns <code>Model</code>, and <code>evaluate</code> returns <code>float</code>, the destructured result is <code>tuple[DataFrame, Model, float]</code>.</p>"},{"location":"concepts/#async-dispatch","title":"<code>&gt;</code> \u2014 Async dispatch","text":"<p>Like <code>&gt;&gt;</code>, but returns a <code>Future[T]</code> immediately instead of blocking. This lets you overlap remote computation with local work:</p> <pre><code>future = train(10) &gt; pool\n# ... do local work while the remote computation runs ...\nresult = future.result()  # blocks only when you need the result\n</code></pre> <p>The <code>Future</code> follows Python's <code>concurrent.futures</code> protocol, so it integrates with existing code that already works with futures \u2014 including <code>as_completed()</code>, <code>wait()</code>, and executor patterns.</p>"},{"location":"concepts/#gather-dynamic-parallelism","title":"<code>gather()</code> \u2014 Dynamic parallelism","text":"<p><code>&amp;</code> works when the number of parallel tasks is known at write time. When it isn't \u2014 because you're iterating over a dataset, a list of configurations, or any dynamic collection \u2014 <code>gather</code> groups an arbitrary number of computations:</p> <pre><code>tasks = [process(chunk) for chunk in chunks]\nresults = sky.gather(*tasks) &gt;&gt; pool\n</code></pre> <p>Under the hood, <code>gather</code> creates a <code>PendingComputeGroup</code> \u2014 the same type that <code>&amp;</code> produces \u2014 so the dispatch behavior is identical. The difference is purely syntactic: <code>&amp;</code> is for a fixed set of typed computations, <code>gather</code> is for a dynamic collection.</p> <p>With <code>stream=True</code>, results are yielded as they complete rather than waiting for all of them. This is useful when tasks have variable duration and you want to start processing results early:</p> <pre><code>for result in sky.gather(*tasks, stream=True) &gt;&gt; pool:\n    save(result)\n</code></pre> <p>By default, streaming preserves the original order (the first yielded result corresponds to the first task). With <code>ordered=False</code>, results arrive in completion order \u2014 faster overall, but you lose the positional correspondence.</p>"},{"location":"concepts/#image","title":"Image","text":"<p>Remote workers start as bare cloud instances \u2014 a fresh OS with no Python, no libraries, no knowledge of your project. The <code>Image</code> dataclass describes the full environment that should exist on each worker before any computation runs. It's a declarative specification: you state what you need, and Skyward generates an idempotent bootstrap script that provisions it.</p> <pre><code>image = sky.Image(\n    python=\"3.13\",\n    pip=[\"torch\", \"numpy\", \"transformers\"],\n    apt=[\"ffmpeg\", \"libsndfile1\"],\n    env={\"KERAS_BACKEND\": \"jax\"},\n    includes=[\"./my_module/\"],\n)\n\nwith sky.ComputePool(provider=sky.AWS(), image=image) as pool:\n    ...\n</code></pre> <p>Each field maps to a phase of the bootstrap process:</p> <ul> <li><code>python</code> \u2014 The Python version to install. Defaults to <code>\"auto\"</code>, which detects your local version and matches it on the worker. Workers use <code>uv</code> as the package manager, so installation is fast.</li> <li><code>pip</code> \u2014 Python packages to install in the worker's virtual environment.</li> <li><code>apt</code> \u2014 System-level packages installed before Python setup.</li> <li><code>env</code> \u2014 Environment variables set before your function executes. Useful for framework configuration (<code>KERAS_BACKEND</code>, <code>HF_TOKEN</code>, etc.).</li> <li><code>includes</code> \u2014 Local directories to sync to the workers via SSH. This is how your own code reaches the remote machines without being published as a package.</li> <li><code>excludes</code> \u2014 Glob patterns to skip during sync (e.g., <code>[\"__pycache__\", \"*.pyc\"]</code>).</li> </ul> <p>The bootstrap script is generated once and runs identically on every instance. Because the <code>Image</code> is a frozen dataclass, two pools created with the same image produce the same environment \u2014 same Python version, same packages, same system dependencies. This is reproducibility without Docker: the environment specification lives in your Python code, versioned alongside your experiments.</p> <p>There's also <code>skyward_source</code>, which controls how Skyward itself reaches the workers. In most cases the default <code>\"auto\"</code> is what you want: it detects whether you're running from an editable install (development mode) and, if so, copies your local Skyward source to the workers instead of installing from PyPI. This means changes to Skyward's own code are reflected immediately on remote machines during development, without publishing a new version.</p>"},{"location":"concepts/#runtime-context","title":"Runtime context","text":"<p>Skyward operates in two worlds. The client side is your local machine \u2014 where <code>ComputePool</code> runs, where operators dispatch tasks, where results come back. The worker side is the remote instance \u2014 where your <code>@sky.compute</code> function actually executes. These are separate processes on separate machines, connected by SSH tunnels.</p> <p>Inside a <code>@sky.compute</code> function, you're in the worker world. The function has access to the remote machine's resources (GPUs, local disk, network), but it doesn't have a reference to the pool object or the client's memory. What it does have is <code>sky.instance_info()</code>: a view of the cluster topology from this node's perspective.</p> <pre><code>@sky.compute\ndef distributed_task(data):\n    info = sky.instance_info()\n    print(f\"Node {info.node} of {info.total_nodes}\")\n\n    if info.is_head:\n        coordinate_others()\n    return process(data)\n</code></pre> <p><code>InstanceInfo</code> is populated from a <code>COMPUTE_POOL</code> environment variable that Skyward injects before starting the worker process. It contains the node's index (<code>node</code>), the total cluster size (<code>total_nodes</code>), whether this is the head node (<code>is_head</code>), the head node's address and port (for coordination protocols), the number of accelerators on this node, and a list of all peers with their private IPs.</p> <p>This is the same mechanism that Skyward's plugins use internally. The <code>torch</code> plugin, for example, reads <code>instance_info()</code> to configure <code>MASTER_ADDR</code>, <code>WORLD_SIZE</code>, and <code>RANK</code> before calling <code>init_process_group</code>. You don't need plugins to access this information \u2014 <code>instance_info()</code> is always available inside any <code>@sky.compute</code> function, whether you're using a plugin or writing raw distributed logic.</p>"},{"location":"concepts/#data-sharding","title":"Data sharding","text":"<p>A common pattern in distributed computing is to send the same function to all nodes but have each node operate on a different slice of the data. <code>sky.shard()</code> automates this: it reads the current node's position from <code>instance_info()</code> and returns only the portion of the data that belongs to this node.</p> <pre><code>@sky.compute\ndef process(full_dataset):\n    local_data = sky.shard(full_dataset)\n    return analyze(local_data)\n\nwith sky.ComputePool(provider=sky.AWS(), nodes=4) as pool:\n    # each node gets 1/4 of the data\n    results = process(dataset) @ pool\n</code></pre> <p>The function receives the full dataset as an argument \u2014 the serialization cost is paid once \u2014 but each node only processes its shard. The sharding algorithm uses modulo striding by default (<code>indices[node::total_nodes]</code>), which distributes elements evenly regardless of whether the total is divisible by the node count.</p> <p>Sharding is type-preserving: lists produce lists, tuples produce tuples, NumPy arrays produce arrays, PyTorch tensors produce tensors. This means you can shard a tensor and immediately pass it to a model without type conversions.</p> <p>When sharding multiple arrays, indices are aligned \u2014 the same positions are selected from each array, so paired data (features and labels, inputs and targets) stays consistent:</p> <pre><code>@sky.compute\ndef train(x_full, y_full):\n    x, y = sky.shard(x_full, y_full, shuffle=True, seed=42)\n    # x[i] still corresponds to y[i]\n    return fit(x, y)\n</code></pre> <p>The <code>shuffle</code> parameter randomizes the order before sharding, with a fixed <code>seed</code> ensuring all nodes agree on the same permutation. <code>drop_last=True</code> switches from striding to contiguous blocks and discards leftover elements, guaranteeing equal shard sizes \u2014 useful when your training loop requires fixed batch dimensions.</p>"},{"location":"concepts/#streaming","title":"Streaming","text":"<p>The patterns above \u2014 <code>&gt;&gt;</code>, <code>@</code>, <code>&amp;</code>, <code>gather</code> \u2014 all follow the same shape: serialize the function and arguments, ship them to a worker, execute, serialize the result, ship it back. The full result materializes on the worker before anything crosses the network. For most workloads this is fine. But some computations produce results incrementally \u2014 a training loop that yields metrics every epoch, a data pipeline that emits rows one at a time, a search that finds matches progressively. Waiting for the entire result before returning anything wastes time and memory.</p> <p>Streaming changes this. If a <code>@sky.compute</code> function is a generator \u2014 it uses <code>yield</code> instead of <code>return</code> \u2014 Skyward streams the values back to the caller as they're produced. The dispatch expression <code>task() &gt;&gt; pool</code> returns a synchronous iterator instead of a single value, and each element arrives as soon as the worker yields it:</p> <pre><code>@sky.compute\ndef generate_samples(n: int):\n    for i in range(n):\n        yield expensive_sample(i)\n\nwith sky.ComputePool(provider=sky.AWS(), nodes=1) as pool:\n    for sample in generate_samples(1000) &gt;&gt; pool:\n        save(sample)  # processes each sample as it arrives\n</code></pre> <p>The inverse also works: parameters annotated with <code>Iterator[T]</code> are streamed to the worker instead of being serialized as a single blob. This means a 10GB dataset doesn't need to fit in a single cloudpickle payload \u2014 it flows to the worker element by element, and the worker consumes it as a regular <code>for</code> loop:</p> <pre><code>from collections.abc import Iterator\n\n@sky.compute\ndef process(data: Iterator[dict]) -&gt; int:\n    count = 0\n    for record in data:\n        transform(record)\n        count += 1\n    return count\n\nresult = process(iter(huge_dataset)) &gt;&gt; pool\n</code></pre> <p>Both directions can be combined: a function that takes an <code>Iterator</code> input and yields results creates a bidirectional stream \u2014 data flows in, transformed results flow out, and neither side buffers the full dataset in memory.</p> <p>Under the hood, streaming is built on Casty's <code>stream_producer</code> and <code>stream_consumer</code> actors, which provide backpressure-aware message passing over the SSH-tunneled TCP connection. If the consumer falls behind, the producer pauses automatically. The detection is fully implicit: Skyward inspects the function at dispatch time \u2014 if it's a generator, it wraps the output in a stream; if a parameter has an <code>Iterator[T]</code> annotation, it wraps the argument in an input stream. No configuration, no special classes.</p> <p>For a practical walkthrough with code examples, see the Streaming guide.</p>"},{"location":"concepts/#providers","title":"Providers","text":"<p>Throughout this page, the <code>provider</code> parameter has appeared in every <code>ComputePool</code> example \u2014 <code>sky.AWS()</code>, <code>sky.RunPod()</code>, <code>sky.VastAI()</code>, <code>sky.Verda()</code>, <code>sky.Container()</code>. The provider is the bridge between Skyward's orchestration model and a specific cloud's API. It knows how to translate abstract requests (\"I need 4 machines with A100 GPUs\") into the concrete API calls that each cloud requires.</p> <p>All providers implement the same protocol \u2014 <code>Provider[C, S]</code> \u2014 which defines six operations that map directly to the pool lifecycle discussed earlier:</p> <ul> <li><code>offers(spec)</code> \u2014 Query available machine types and pricing that match the spec. Returns an async iterator of <code>Offer</code> objects \u2014 normalized descriptions of hardware and pricing that can be compared across providers. This is what enables cross-provider selection.</li> <li><code>prepare(spec, offer)</code> \u2014 Set up cluster-level infrastructure for the selected offer: register SSH keys, create VPCs and security groups (AWS), resolve GPU types, configure overlay networks. Returns an immutable <code>Cluster</code> context that flows through all subsequent calls.</li> <li><code>provision(cluster, count)</code> \u2014 Launch the requested number of instances. Returns them in a \"provisioning\" state \u2014 the machines exist, but may not be running yet.</li> <li><code>get_instance(cluster, id)</code> \u2014 Poll the status of a single instance. The instance actor calls this repeatedly until the machine is running and has an IP address.</li> <li><code>terminate(cluster, ids)</code> \u2014 Destroy specific instances.</li> <li><code>teardown(cluster)</code> \u2014 Clean up everything <code>prepare</code> created: delete security groups, deregister keys, remove temporary infrastructure.</li> </ul> <p>This protocol is what makes the orchestration layer provider-agnostic. The pool actor, node actors, and instance actors don't know whether they're talking to AWS, RunPod, or a local Docker daemon \u2014 they only know the six methods above. Adding a new cloud provider means implementing this protocol; nothing else in the system needs to change.</p> <p>Provider configs (<code>sky.AWS()</code>, <code>sky.RunPod()</code>, etc.) are deliberately lightweight. They're plain dataclasses that hold configuration \u2014 region, credentials path, API keys \u2014 but don't import any cloud SDK at module level. The actual SDK (<code>boto3</code>, <code>aioboto3</code>, RunPod's GraphQL client, etc.) is loaded lazily when <code>create_provider()</code> is called at pool start. This means <code>import skyward</code> stays fast regardless of which providers are installed, and you only pay the import cost for the provider you're actually using.</p> <p>For details on configuring each provider, see the Providers page.</p>"},{"location":"concepts/#accelerators","title":"Accelerators","text":"<p>The <code>accelerator</code> parameter on <code>ComputePool</code> tells the provider what hardware you need. You can pass a plain string (<code>\"A100\"</code>) or use the typed factory functions under <code>sky.accelerators</code>, which provide IDE autocomplete and carry catalog metadata like VRAM size and CUDA compatibility:</p> <pre><code># String \u2014 simple, works everywhere\nsky.ComputePool(provider=sky.AWS(), accelerator=\"A100\")\n\n# Factory function \u2014 type-safe, with defaults from the catalog\nsky.ComputePool(provider=sky.AWS(), accelerator=sky.accelerators.A100())\nsky.ComputePool(provider=sky.AWS(), accelerator=sky.accelerators.H100(count=4))\nsky.ComputePool(provider=sky.AWS(), accelerator=sky.accelerators.A100(memory=\"40GB\"))\n</code></pre> <p>Both forms produce an <code>Accelerator</code> dataclass \u2014 a frozen, immutable specification with <code>name</code>, <code>memory</code>, <code>count</code>, and optional <code>metadata</code>. The factory functions look up defaults from an internal catalog (VRAM sizes, CUDA versions, form factors), so <code>sky.accelerators.H100()</code> already knows it has 80GB of memory without you specifying it.</p> <p>The recommended approach is the factory function (<code>sky.accelerators.A100()</code>), not the string. If an accelerator isn't available as a factory, it means Skyward hasn't mapped it yet \u2014 and that mapping matters. Each cloud provider uses its own naming conventions and instance type structures: an \"A100\" on AWS is a <code>p4d.24xlarge</code>, on RunPod it's a pod with a specific <code>gpuTypeId</code>, on VastAI it's a marketplace offer filtered by GPU model. The translation from a logical accelerator name to a provider-specific resource isn't a simple string match \u2014 it involves resolving instance types, memory variants, multi-GPU configurations, and availability constraints. The catalog centralizes this complexity so that <code>sky.accelerators.A100(count=4)</code> resolves correctly on any provider that supports it.</p> <p>The catalog covers NVIDIA datacenter GPUs (H100, H200, A100, L40S, T4, etc.), consumer cards (RTX 4090 down to GTX 1060), AMD Instinct (MI300X, MI250X), AWS Trainium and Inferentia, Habana Gaudi, and Google TPUs.</p> <p>For a complete list, see the Accelerators page.</p>"},{"location":"concepts/#allocation-strategies","title":"Allocation strategies","text":"<p>Cloud providers offer different pricing models for the same hardware. The most important distinction is between on-demand instances (guaranteed availability, full price) and spot instances (surplus capacity sold at a discount, but reclaimable by the provider with short notice).</p> <p>Skyward exposes this as an <code>allocation</code> parameter on the pool:</p> <pre><code># Try spot instances first, fall back to on-demand if unavailable (default)\nsky.ComputePool(allocation=\"spot-if-available\")\n\n# Always use spot \u2014 cheaper, but can be interrupted\nsky.ComputePool(allocation=\"spot\")\n\n# Always on-demand \u2014 more expensive, but guaranteed availability\nsky.ComputePool(allocation=\"on-demand\")\n\n# Compare all options and pick the cheapest\nsky.ComputePool(allocation=\"cheapest\")\n</code></pre> <p>Spot instances are typically 60-90% cheaper than on-demand. The trade-off is that the cloud provider can reclaim them with short notice. For fault-tolerant workloads \u2014 checkpointed training, idempotent batch jobs, embarrassingly parallel inference \u2014 spot is a practical default. For time-sensitive or non-resumable work, on-demand is the safer choice.</p> <p>The default <code>\"spot-if-available\"</code> tries to get the best of both: it requests spot capacity first and falls back to on-demand if none is available. This means your pool always starts, even during periods of high spot demand, without requiring you to handle the fallback logic yourself.</p>"},{"location":"concepts/#resource-selection","title":"Resource selection","text":"<p>The previous sections described choosing a single provider and a single allocation strategy. But accelerator availability and pricing vary across providers and change constantly. An A100 on VastAI might cost $1.50/hr right now while the same accelerator on AWS is $3.00/hr \u2014 or VastAI might have no capacity at all. Skyward's resource selection system lets you describe multiple hardware preferences and let the system find the best option.</p>"},{"location":"concepts/#spec","title":"Spec","text":"<p>A <code>Spec</code> is a frozen dataclass that bundles a provider with hardware preferences into a single, composable unit:</p> <pre><code>sky.Spec(\n    provider=sky.VastAI(),\n    accelerator=\"A100\",\n    nodes=4,\n    allocation=\"spot\",\n    max_hourly_cost=2.50,\n)\n</code></pre> <p>It carries the same fields you'd normally pass to <code>ComputePool</code> \u2014 <code>accelerator</code>, <code>nodes</code>, <code>vcpus</code>, <code>memory_gb</code>, <code>architecture</code>, <code>allocation</code>, <code>region</code>, <code>max_hourly_cost</code>, <code>ttl</code> \u2014 but scoped to a specific provider. <code>ComputePool</code> accepts multiple <code>Spec</code> objects as positional arguments:</p> <pre><code>with sky.ComputePool(\n    sky.Spec(provider=sky.VastAI(), accelerator=\"A100\"),\n    sky.Spec(provider=sky.AWS(), accelerator=\"A100\"),\n    selection=\"cheapest\",\n) as pool:\n    result = train(data) &gt;&gt; pool\n</code></pre> <p>The single-provider form still works \u2014 <code>ComputePool(provider=sky.AWS(), accelerator=\"A100\")</code> internally creates a single <code>Spec</code> and wraps it in a tuple.</p>"},{"location":"concepts/#offers-and-instance-types","title":"Offers and instance types","text":"<p>Cross-provider comparison requires a common representation of what each provider offers. Skyward models this with two types:</p> <ul> <li><code>InstanceType</code> \u2014 A normalized, cacheable hardware description: machine name, accelerator, vCPUs, memory, architecture. This is the same across spot and on-demand pricing for the same machine.</li> <li><code>Offer</code> \u2014 Ephemeral pricing and availability on top of an <code>InstanceType</code>: spot price, on-demand price, billing unit (second, minute, or hour). Each provider's <code>offers()</code> method yields <code>Offer</code> objects for a given spec.</li> </ul> <p>Because different providers produce structurally identical <code>Offer</code> objects, Skyward can compare an AWS <code>p4d.24xlarge</code> against a VastAI marketplace listing against a Verda H100 \u2014 all as normalized offers with comparable prices.</p>"},{"location":"concepts/#selection-strategies","title":"Selection strategies","text":"<p>The <code>selection</code> parameter on <code>ComputePool</code> controls how Skyward chooses among the offers from multiple specs:</p> <ul> <li><code>\"cheapest\"</code> (default) \u2014 Queries all specs, collects all available offers, and picks the one with the lowest price. Best when you want cost optimization and don't have a strong provider preference.</li> <li><code>\"first\"</code> \u2014 Tries specs in the order you listed them, and stops at the first provider with available offers. Best when you have a preferred provider and want deterministic fallback.</li> </ul>"},{"location":"concepts/#the-selection-flow","title":"The selection flow","text":"<p>Before provisioning anything, <code>ComputePool</code> runs a selection phase to determine where to provision:</p> <pre><code>sequenceDiagram\n    participant User\n    participant Pool\n    participant Provider A\n    participant Provider B\n\n    User-&gt;&gt;Pool: __enter__\n\n    Pool-&gt;&gt;Provider A: offers(spec_a)\n    Provider A--&gt;&gt;Pool: Offer[]\n    Pool-&gt;&gt;Provider B: offers(spec_b)\n    Provider B--&gt;&gt;Pool: Offer[]\n\n    Pool-&gt;&gt;Pool: select best offer\n\n    Pool-&gt;&gt;Provider B: prepare(spec, offer)\n    Provider B--&gt;&gt;Pool: Cluster\n    Pool-&gt;&gt;Provider B: provision(cluster, N)\n    Note over Pool,Provider B: (lifecycle continues as usual)</code></pre> <p>The key insight is that offer querying is fast (API calls to check availability and pricing) while provisioning is slow (launching machines). By querying all providers before committing to one, Skyward makes an informed decision without wasting time on failed provisioning attempts.</p> <p>For a practical walkthrough with code examples, see the Multi-Provider Selection guide.</p>"},{"location":"concepts/#next-steps","title":"Next steps","text":"<ul> <li>Getting Started \u2014 Installation, credentials, and first example</li> <li>Distributed Training \u2014 Multi-node training with PyTorch, Keras, and JAX</li> <li>Providers \u2014 AWS, GCP, RunPod, VastAI, and Verda</li> <li>Distributed Collections \u2014 Shared state across nodes</li> <li>Streaming \u2014 Incremental input and output with generators</li> <li>Multi-Provider Selection \u2014 Cross-provider price comparison</li> <li>API Reference \u2014 Complete API documentation</li> </ul>"},{"location":"distributed-collections/","title":"Distributed collections","text":"<p>When compute functions run across multiple nodes, they sometimes need to share state. A training loop might track global progress. A batch processor might deduplicate work across workers. A distributed pipeline might need a synchronization point where all nodes wait before proceeding. A checkpoint routine might need mutual exclusion so two nodes don't write conflicting state simultaneously.</p> <p>Skyward provides distributed data structures for these cases. They look like their Python counterparts \u2014 <code>dict</code>, <code>set</code>, <code>counter</code>, <code>queue</code>, <code>barrier</code>, <code>lock</code> \u2014 but they're backed by Casty's distributed actor system and replicated across the cluster. Any node can read and write to them, and the cluster handles synchronization automatically.</p> <p>Collections are created lazily by name. Calling <code>sky.dict(\"cache\")</code> on any node returns a proxy to the same shared dict \u2014 if the collection doesn't exist yet, the cluster creates it; if it already exists, you get a reference to the existing one. Inside <code>@sky.compute</code> functions, use the module-level shortcuts (<code>sky.dict</code>, <code>sky.counter</code>, <code>sky.set</code>, etc.) which resolve the active pool automatically through a <code>ContextVar</code>. You can also access them via the pool directly: <code>pool.dict(\"cache\")</code>, <code>pool.counter(\"steps\")</code>, etc.</p>"},{"location":"distributed-collections/#dict","title":"Dict","text":"<p>The distributed dict is a shared key-value store across all workers. The most common use case is a distributed cache: if one node computes an expensive result, other nodes can read it instead of recomputing.</p> <pre><code>@sky.compute\ndef process_with_cache(items: list[str]) -&gt; dict:\n    cache = sky.dict(\"embeddings\")\n\n    for item in sky.shard(items):\n        if item in cache:\n            result = cache[item]\n        else:\n            result = compute_embedding(item)\n            cache[item] = result\n\n    return {\"processed\": len(items)}\n</code></pre> <p>The dict supports standard Python syntax: <code>cache[key]</code>, <code>cache[key] = value</code>, <code>del cache[key]</code>, <code>key in cache</code>, <code>cache.get(key, default)</code>, <code>cache.update(...)</code>, and <code>cache.pop(key, default)</code>.</p> <p>Internally, each key is managed by a separate actor (entity-per-key), which means reads and writes to different keys don't contend with each other. This design makes the dict highly concurrent \u2014 hundreds of nodes can read and write to different keys simultaneously without coordination overhead. The trade-off is that enumeration methods \u2014 <code>keys()</code>, <code>values()</code>, <code>items()</code>, <code>len()</code> \u2014 are not available, because there's no single actor that knows about all keys. Think of it as a distributed cache with get/put/delete/contains semantics, not a full <code>dict</code> replacement.</p>"},{"location":"distributed-collections/#counter","title":"Counter","text":"<p>The distributed counter is an atomic integer shared across all workers. Every node can increment and decrement it, and all operations are serialized through the counter's backing actor \u2014 no lost updates, no race conditions.</p> <pre><code>@sky.compute\ndef train_step(batch) -&gt; dict:\n    progress = sky.counter(\"global_steps\")\n    progress.increment()\n    return {\"step\": int(progress)}\n</code></pre> <p>The counter supports <code>progress.value</code> (current value), <code>progress.increment(n=1)</code>, <code>progress.decrement(n=1)</code>, <code>progress.reset(value=0)</code>, and <code>int(progress)</code> for casting. It's useful for tracking global progress across workers, counting processed items, or implementing simple distributed coordination where all you need is a shared integer.</p>"},{"location":"distributed-collections/#set","title":"Set","text":"<p>The distributed set tracks unique values across workers. The typical use case is deduplication \u2014 ensuring that a batch isn't processed twice even when multiple nodes receive overlapping work.</p> <pre><code>@sky.compute\ndef deduplicate(batch_id: int) -&gt; str:\n    seen = sky.set(\"processed_batches\")\n    key = f\"batch:{batch_id}\"\n\n    if key in seen:\n        return \"skipped\"\n\n    seen.add(key)\n    return \"processed\"\n</code></pre> <p>The set supports <code>value in s</code>, <code>len(s)</code>, <code>s.add(value)</code>, and <code>s.discard(value)</code>. Unlike the dict, the set does support <code>len()</code> because it's backed by a single replicated actor rather than entity-per-key.</p>"},{"location":"distributed-collections/#queue","title":"Queue","text":"<p>The distributed queue is a FIFO work queue for dynamic task distribution. Unlike the static partitioning that <code>shard()</code> provides \u2014 where each node gets a predetermined slice of the data \u2014 a queue lets workers pull tasks at their own pace. Fast workers process more items, slow workers process fewer, and the overall throughput adapts to heterogeneous performance.</p> <pre><code>@sky.compute\ndef producer(tasks: list[int]):\n    queue = sky.queue(\"work\")\n    info = sky.instance_info()\n    if info.is_head:\n        for task in tasks:\n            queue.put(task)\n\n@sky.compute\ndef worker() -&gt; list:\n    queue = sky.queue(\"work\")\n    results = []\n    while True:\n        task = queue.get(timeout=0.5)\n        if task is None:\n            break\n        results.append(task * 2)\n    return results\n</code></pre> <p>The queue supports <code>queue.put(value)</code>, <code>queue.get(timeout=None)</code> (returns <code>None</code> on timeout), <code>queue.empty()</code>, and <code>len(queue)</code>. The producer-consumer pattern shown above is a common way to implement dynamic load balancing: the head node fills the queue, all workers drain it at their own pace, and the work naturally distributes based on each worker's processing speed.</p>"},{"location":"distributed-collections/#barrier","title":"Barrier","text":"<p>The distributed barrier is a synchronization point where all workers must arrive before any can proceed. This is useful when your distributed computation has phases that must complete globally before the next phase begins \u2014 for example, ensuring all nodes have finished an epoch before aggregating results, or waiting for all nodes to load a model before starting inference.</p> <pre><code>@sky.compute\ndef synchronized_epoch(epoch: int) -&gt; dict:\n    info = sky.instance_info()\n    sync = sky.barrier(\"epoch_sync\", n=info.total_nodes)\n\n    loss = train_one_epoch(epoch)\n    sync.wait()  # blocks until all n workers arrive\n\n    return {\"node\": info.node, \"loss\": loss}\n</code></pre> <p>The barrier is created with a participant count <code>n</code>. When <code>n</code> workers have called <code>wait()</code>, all of them are released simultaneously. If any worker fails to arrive \u2014 because of an error, a timeout, or a preempted spot instance \u2014 the others will block until their task times out. Barriers work best when all nodes are expected to reach the synchronization point reliably.</p>"},{"location":"distributed-collections/#lock","title":"Lock","text":"<p>The distributed lock provides mutual exclusion across the cluster. When multiple nodes need to update shared state atomically \u2014 like writing the best checkpoint so far, or coordinating access to an external resource \u2014 a lock ensures only one node enters the critical section at a time.</p> <pre><code>@sky.compute\ndef safe_checkpoint(step: int) -&gt; bool:\n    lock = sky.lock(\"checkpoint_lock\")\n    state = sky.dict(\"checkpoint\")\n\n    with lock:\n        current_best = state.get(\"best_loss\", float(\"inf\"))\n        my_loss = evaluate(step)\n        if my_loss &lt; current_best:\n            state[\"best_loss\"] = my_loss\n            state[\"best_step\"] = step\n            return True\n    return False\n</code></pre> <p>The lock supports <code>lock.acquire()</code>, <code>lock.release()</code>, and the context manager protocol (<code>with lock:</code>). Casty's distributed locking ensures that only one holder exists across the cluster at any time \u2014 if node 0 holds the lock, node 1's <code>acquire()</code> blocks until node 0 releases it, regardless of which physical machine each node runs on.</p>"},{"location":"distributed-collections/#consistency","title":"Consistency","text":"<p>Collections support two consistency levels, configured at creation time:</p> <pre><code># Default: eventual consistency (faster)\ncache = sky.dict(\"cache\")\n\n# Strong consistency (slower, always up-to-date)\ncache = sky.dict(\"cache\", consistency=\"strong\")\n</code></pre> <p>With eventual consistency (the default), reads are fast but may see slightly stale values. A write on node 0 might not be visible on node 1 for a brief window while replication propagates. This is sufficient for most use cases \u2014 caches, progress counters, deduplication sets \u2014 where reading a slightly outdated value doesn't affect correctness.</p> <p>With strong consistency, every read returns the latest written value. This is slower because it requires coordination with the actor managing the data, but it's necessary when correctness depends on seeing the most recent state \u2014 for example, when using a lock and a dict together to coordinate checkpoint writes, you want the dict reads inside the critical section to be strongly consistent.</p>"},{"location":"distributed-collections/#async-interface","title":"Async interface","text":"<p>All collections expose async methods for use in async contexts. The naming convention adds <code>_async</code> to each operation:</p> <pre><code>await cache.get_async(key)\nawait cache.set_async(key, value)\nawait counter.increment_async()\nawait queue.put_async(value)\nawait lock.acquire_async()\n</code></pre> <p>The sync interface (the default, used in most <code>@sky.compute</code> functions) blocks the calling thread while waiting for the actor response. The async interface returns awaitables, which is useful if you're writing custom async logic inside a worker or integrating with an existing async codebase.</p>"},{"location":"distributed-collections/#next-steps","title":"Next steps","text":"<ul> <li>Distributed Training \u2014 Multi-node training with shared state across workers</li> <li>Clustering \u2014 How the Casty actor cluster powers distributed collections</li> <li>API Reference \u2014 Full API documentation</li> </ul>"},{"location":"distributed-training/","title":"Distributed training","text":"<p>Distributed training across multiple machines requires solving two problems simultaneously. First, the environment: every node needs to know the cluster topology \u2014 who the master is, how many peers exist, what rank each process holds. Second, the data: each node should train on a different subset, but the model parameters need to stay synchronized across all of them.</p> <p>Skyward handles the first problem automatically. When you provision a multi-node pool and broadcast a function with <code>@</code>, every worker receives the same function and arguments, but each one sees a different <code>instance_info()</code> \u2014 its own position in the cluster. Plugins like <code>sky.plugins.torch()</code> and <code>sky.plugins.jax()</code> read this topology and configure the framework's distributed environment before your function runs. The second problem \u2014 data partitioning \u2014 is handled either by <code>sky.shard()</code> or by the framework's own distributed sampler.</p> <p>This page explains the concepts. For step-by-step tutorials with runnable code, see the guides: PyTorch Distributed, Keras Training, and HuggingFace Fine-tuning.</p>"},{"location":"distributed-training/#how-it-works","title":"How it works","text":"<p>When a function is broadcast to a pool with <code>@</code>, Skyward sends the same serialized payload to every node. Each node deserializes and executes the function independently. From the framework's perspective, this looks like <code>N</code> separate processes running the same script \u2014 exactly what tools like <code>torchrun</code> or <code>jax.distributed.initialize()</code> expect.</p> <p>The difference is how the environment gets configured. In a traditional setup, you'd write a launch script that sets <code>MASTER_ADDR</code>, <code>WORLD_SIZE</code>, and <code>RANK</code> on each machine, then starts the training process. With Skyward, plugins do this for you. They read the cluster topology from <code>instance_info()</code> \u2014 which is populated from a <code>COMPUTE_POOL</code> environment variable that Skyward injects on each worker \u2014 and set the appropriate variables before your function body runs.</p> <pre><code>@sky.compute\ndef train():\n    import torch.distributed as dist\n    # dist.is_initialized() is True \u2014 process group already configured\n    ...\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=4,\n    accelerator=\"A100\",\n    plugins=[sky.plugins.torch()],\n) as pool:\n    results = train() @ pool  # runs on all 4 nodes\n</code></pre> <p>This is roughly equivalent to running <code>torchrun --nnodes=4 --nproc_per_node=1 train.py</code> on a pre-configured cluster \u2014 except there's no cluster to pre-configure. Skyward provisions the machines, installs dependencies, configures the distributed environment, runs your function, collects the results, and tears everything down when the <code>with</code> block exits.</p>"},{"location":"distributed-training/#plugins","title":"Plugins","text":"<p>Each supported framework has its own plugin. They all follow the same pattern: transform the worker image to install dependencies, then configure the distributed runtime at task execution time by reading <code>instance_info()</code> and setting environment variables. Plugins are specified on the pool, not on individual functions.</p>"},{"location":"distributed-training/#pytorch","title":"PyTorch","text":"<p><code>sky.plugins.torch()</code> adds <code>torch</code> to the worker's pip dependencies and configures <code>MASTER_ADDR</code>, <code>MASTER_PORT</code>, <code>WORLD_SIZE</code>, <code>RANK</code>, <code>LOCAL_RANK</code>, and calls <code>torch.distributed.init_process_group()</code>. The backend defaults to <code>nccl</code> for GPU nodes and <code>gloo</code> for CPU. Once initialized, you wrap your model with <code>DistributedDataParallel</code> and PyTorch handles gradient synchronization automatically \u2014 each node computes gradients on its own data, and DDP averages them across all nodes before each optimizer step.</p> <p>The plugin also configures <code>LOCAL_WORLD_SIZE</code> and <code>NODE_RANK</code> for multi-GPU-per-node setups, though the most common Skyward pattern is one process per node.</p> <p>See the PyTorch Distributed guide for a complete training example with DDP, <code>DistributedSampler</code>, and metric aggregation.</p>"},{"location":"distributed-training/#keras-3","title":"Keras 3","text":"<p><code>sky.plugins.keras(backend=\"jax\")</code> sets the <code>KERAS_BACKEND</code> environment variable on the worker before Keras is imported \u2014 this is critical because Keras reads the backend at import time. When using the JAX backend, combine with <code>sky.plugins.jax()</code>:</p> <pre><code>plugins=[sky.plugins.jax(), sky.plugins.keras(backend=\"jax\")]\n</code></pre> <p>Keras 3 is backend-agnostic, but Skyward's automatic distribution (<code>DataParallel</code> with device discovery) is currently JAX-only. For the <code>torch</code> and <code>tensorflow</code> backends, the plugin delegates to those frameworks' native distributed init. For data-parallel training where each node trains independently on its shard (the most common pattern), the <code>keras</code> plugin alone is sufficient regardless of backend.</p> <p>See the Keras Training guide for a complete MNIST example with data sharding.</p>"},{"location":"distributed-training/#jax","title":"JAX","text":"<p><code>sky.plugins.jax()</code> adds <code>jax[cuda12]</code> to pip and configures <code>JAX_COORDINATOR_ADDRESS</code>, <code>JAX_NUM_PROCESSES</code>, <code>JAX_PROCESS_ID</code>, and <code>JAX_LOCAL_DEVICE_COUNT</code>, then calls <code>jax.distributed.initialize()</code>. After initialization, JAX sees all devices across all nodes as a single device mesh, and operations like <code>pmap</code> and <code>pjit</code> distribute computation automatically.</p>"},{"location":"distributed-training/#huggingface-transformers","title":"HuggingFace Transformers","text":"<p><code>sky.plugins.huggingface(token=\"...\")</code> adds <code>transformers</code>, <code>datasets</code>, and <code>tokenizers</code> to pip, sets <code>HF_TOKEN</code>, and runs <code>huggingface-cli login</code> during bootstrap. For multi-node training, combine with <code>sky.plugins.torch()</code>. The HuggingFace <code>Trainer</code> auto-detects the distributed setup and handles gradient synchronization, mixed-precision training, and distributed evaluation internally.</p> <p>For single-node fine-tuning, the <code>Trainer</code> manages device placement on its own \u2014 the <code>huggingface</code> plugin handles authentication and dependencies. For multi-node, combine with <code>sky.plugins.torch()</code>.</p> <p>See the HuggingFace Fine-tuning guide for a complete example.</p>"},{"location":"distributed-training/#data-partitioning","title":"Data partitioning","text":"<p>In distributed training, each node should process different data but the same model. There are two approaches, and which one you use depends on the framework.</p> <p><code>sky.shard()</code> is Skyward's built-in data partitioning. It works inside any <code>@sky.compute</code> function and is framework-agnostic. You pass the full dataset as an argument, call <code>shard()</code> inside the function, and each node gets its portion based on <code>instance_info()</code>. The sharding is type-preserving (lists produce lists, tensors produce tensors) and supports synchronized shuffling with a fixed seed. This is the natural choice for Keras, JAX, and any workflow where you load data inside the function.</p> <pre><code>@sky.compute\ndef train(x_full, y_full):\n    x, y = sky.shard(x_full, y_full, shuffle=True, seed=42)\n    # x[i] still corresponds to y[i]\n    return fit(x, y)\n</code></pre> <p><code>DistributedSampler</code> is PyTorch's native approach. It integrates with <code>DataLoader</code> and handles shuffling per-epoch (via <code>set_epoch()</code>), uneven dataset sizes, and drop-last semantics within the DataLoader pipeline. If you're using PyTorch DDP, <code>DistributedSampler</code> is the idiomatic choice.</p> <p>Both approaches achieve the same goal: each node trains on different data. The choice is primarily about which framework's idioms you prefer. For a detailed explanation of sharding mechanics \u2014 modulo striding, multi-array alignment, <code>shuffle</code>, <code>drop_last</code> \u2014 see Data Sharding.</p>"},{"location":"distributed-training/#runtime-context","title":"Runtime context","text":"<p>Inside a <code>@sky.compute</code> function, <code>sky.instance_info()</code> returns an <code>InstanceInfo</code> describing this node's position in the cluster. Plugins use this internally, but you can also use it directly for custom distributed logic \u2014 coordinating checkpoints, conditional logging, role-based execution.</p> <pre><code>@sky.compute\ndef distributed_task(data):\n    info = sky.instance_info()\n    print(f\"Node {info.node} of {info.total_nodes}\")\n\n    if info.is_head:\n        coordinate_others()\n\n    return process(data)\n</code></pre> <p>The key fields are <code>node</code> (0 to N-1), <code>total_nodes</code>, <code>is_head</code> (true for node 0), <code>head_addr</code> (private IP of the head node), <code>head_port</code> (coordination port), <code>accelerators</code> (GPU count on this node), and <code>peers</code> (list of all nodes with their addresses). This is the same information that plugins use to set <code>MASTER_ADDR</code>, <code>WORLD_SIZE</code>, and <code>RANK</code> \u2014 you can read it directly when building custom coordination logic or when using a framework that Skyward doesn't have a built-in plugin for.</p> <p>The head node pattern is especially common in distributed training: only the head node saves checkpoints, logs to experiment trackers, or prints progress. Other nodes do the same computation but stay silent. This avoids duplicate writes and noisy output.</p>"},{"location":"distributed-training/#output-control","title":"Output control","text":"<p>In distributed training, having every node print progress is noisy \u2014 four nodes produce four copies of every log line. Skyward provides output control decorators that silence stdout or stderr based on the node's identity:</p> <pre><code>@sky.compute\n@sky.stdout(only=\"head\")\ndef train():\n    print(f\"Epoch {epoch}: loss={loss:.4f}\")  # only head node prints\n</code></pre> <p><code>only=\"head\"</code> silences all non-head nodes. You can also pass a predicate \u2014 <code>only=lambda info: info.node &lt; 2</code> \u2014 for finer control (for example, printing from only the first two nodes for debugging). <code>@sky.silent</code> suppresses both stdout and stderr on all nodes entirely. These decorators are implemented by redirecting output streams to <code>StringIO()</code> based on <code>instance_info()</code> at function entry.</p> <p>Output control decorators go below <code>@sky.compute</code>:</p> <pre><code>@sky.compute\n@sky.stdout(only=\"head\")\ndef train():\n    ...\n</code></pre>"},{"location":"distributed-training/#next-steps","title":"Next steps","text":"<ul> <li>PyTorch Distributed \u2014 DDP training with <code>DistributedSampler</code> and metric aggregation</li> <li>Keras Training \u2014 MNIST across multiple GPUs with JAX backend</li> <li>HuggingFace Fine-tuning \u2014 Transformer fine-tuning on cloud GPUs</li> <li>Data Sharding \u2014 How <code>shard()</code> partitions data across nodes</li> <li>Plugins \u2014 Full plugin reference including joblib and scikit-learn</li> </ul>"},{"location":"getting-started/","title":"Getting started","text":"<p>This page covers installation, credential setup, and your first remote computation. By the end, you'll have run a Python function on a cloud instance and seen the full lifecycle \u2014 provision, execute, return, tear down \u2014 in action.</p> <p>Skyward requires Python 3.12 or higher and credentials for at least one cloud provider (AWS, RunPod, VastAI, or Verda). For local development and testing without cloud credentials, the <code>Container</code> provider works with Docker or Podman.</p>"},{"location":"getting-started/#installation","title":"Installation","text":"<p>The recommended way to install Skyward is with <code>uv</code>:</p> <pre><code>uv add skyward\n</code></pre> <p>Or with pip:</p> <pre><code>pip install skyward\n</code></pre> <p>Skyward also provides optional extras for framework-specific type hints and dependencies. These are not required \u2014 the frameworks themselves only need to be installed on the remote workers via the <code>Image</code> \u2014 but the extras add local type stubs and tooling support:</p> <pre><code>uv add skyward[pytorch]      # PyTorch type hints\nuv add skyward[huggingface]  # HuggingFace type hints\nuv add skyward[aws]          # AWS (boto3) type hints\nuv add skyward[all]          # All extras\n</code></pre>"},{"location":"getting-started/#provider-credentials","title":"Provider credentials","text":"<p>Each provider needs credentials before Skyward can provision instances. You only need to configure the provider you intend to use.</p>"},{"location":"getting-started/#aws","title":"AWS","text":"<p>Skyward uses standard AWS credential resolution \u2014 the same chain that <code>boto3</code> and the AWS CLI use. The simplest approach is environment variables:</p> <pre><code>export AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre> <p>You can also use <code>aws configure</code> to write credentials to <code>~/.aws/credentials</code>, or rely on IAM roles if running from an EC2 instance. Any method that <code>boto3</code> recognizes will work.</p> <p>The AWS provider needs <code>ec2:*</code> permissions for instance management and <code>iam:PassRole</code> for instance profiles. SSM permissions (<code>ssm:*</code>) are optional but recommended \u2014 they enable Session Manager connectivity as a fallback when direct SSH isn't available.</p>"},{"location":"getting-started/#runpod","title":"RunPod","text":"<pre><code>export RUNPOD_API_KEY=your_api_key\n</code></pre> <p>Get your API key from Settings &gt; API Keys at runpod.io.</p>"},{"location":"getting-started/#vastai","title":"VastAI","text":"<pre><code>pip install vastai\nvastai set api-key YOUR_API_KEY\n</code></pre> <p>Get your API key at cloud.vast.ai/account.</p>"},{"location":"getting-started/#verda","title":"Verda","text":"<pre><code>export VERDA_CLIENT_ID=your_client_id\nexport VERDA_CLIENT_SECRET=your_client_secret\n</code></pre>"},{"location":"getting-started/#your-first-remote-function","title":"Your first remote function","text":"<p>Create a file called <code>hello.py</code>:</p> <pre><code>import skyward as sky\n\n@sky.compute\ndef hello() -&gt; str:\n    \"\"\"This function runs on a remote instance.\"\"\"\n    import socket\n    return f\"Hello from {socket.gethostname()}!\"\n\nwith sky.ComputePool(provider=sky.AWS()) as pool:\n    result = hello() &gt;&gt; pool\n    print(result)\n</code></pre> <p>Run it:</p> <pre><code>uv run python hello.py\n</code></pre> <p>When you execute this, Skyward provisions an EC2 instance, opens an SSH tunnel to it, installs Python and Skyward on the remote machine via an idempotent bootstrap script, serializes your <code>hello</code> function with cloudpickle, sends it over the tunnel, executes it on the remote instance, serializes the result back, and returns it to your local process. When the <code>with</code> block exits, the instance is terminated. The entire lifecycle \u2014 from bare metal to running Python to cleanup \u2014 happens automatically.</p> <p>The output will look something like this:</p> <pre><code>[ClusterProvisioned] Cluster ready in us-east-1\n[InstanceLaunched] Launching instance i-0abc123...\n[InstanceRunning] Instance running (52.1.2.3)\n[InstanceProvisioned] Instance provisioned, starting bootstrap\n[BootstrapPhase] Phase 'apt' started\n[BootstrapPhase] Phase 'apt' completed (12s)\n[BootstrapPhase] Phase 'pip' started\n[BootstrapPhase] Phase 'pip' completed (45s)\n[InstanceBootstrapped] Bootstrap complete\n[NodeReady] Node 0 ready\n[ClusterReady] Cluster ready with 1 node(s)\nHello from ip-172-31-0-1!\n[TaskCompleted] Task completed in 2.3s\n[ClusterDestroyed] Cluster terminated\n</code></pre> <p>Each line is an event from the pool's lifecycle. The sequence reflects the stages described in Core Concepts \u2014 the pool actor asks the provider to launch an instance, the instance actor polls until it's running, opens the SSH tunnel, runs the bootstrap script phase by phase, starts the worker, and reports ready. After the task completes, everything is torn down.</p>"},{"location":"getting-started/#your-first-accelerator-job","title":"Your first accelerator job","text":"<p>To run on a GPU, add the <code>accelerator</code> and <code>image</code> parameters to the pool. The <code>accelerator</code> specifies what hardware you need, and the <code>image</code> describes what software should be installed on the remote worker:</p> <pre><code>import skyward as sky\n\n@sky.compute\ndef gpu_info() -&gt; dict:\n    import torch\n    return {\n        \"cuda_available\": torch.cuda.is_available(),\n        \"device_count\": torch.cuda.device_count(),\n        \"device_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,\n    }\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=sky.accelerators.T4(),\n    image=sky.Image(pip=[\"torch\"]),\n    allocation=\"spot\",\n) as pool:\n    info = gpu_info() &gt;&gt; pool\n    print(f\"GPU: {info['device_name']}\")\n    print(f\"CUDA devices: {info['device_count']}\")\n</code></pre> <p>Notice that <code>torch</code> is imported inside the function, not at the top of the file. This is intentional: <code>torch</code> doesn't need to be installed on your local machine \u2014 it only needs to exist on the remote worker, where the function actually runs. The <code>Image(pip=[\"torch\"])</code> tells Skyward to install it there during bootstrap. This pattern \u2014 importing heavy dependencies inside <code>@sky.compute</code> functions \u2014 keeps your local environment lightweight.</p> <p>The <code>allocation=\"spot\"</code> parameter requests spot instances, which are typically 60-90% cheaper than on-demand. If spot capacity isn't available, the pool will fail rather than fall back. Use <code>allocation=\"spot-if-available\"</code> (the default) to automatically fall back to on-demand pricing.</p>"},{"location":"getting-started/#parallel-execution","title":"Parallel execution","text":"<p>A single <code>&gt;&gt;</code> sends one computation to one node. When you have multiple independent tasks, <code>gather()</code> dispatches them all concurrently:</p> <pre><code>import skyward as sky\n\n@sky.compute\ndef square(x: int) -&gt; int:\n    return x * x\n\nwith sky.ComputePool(provider=sky.AWS()) as pool:\n    results = sky.gather(square(1), square(2), square(3)) &gt;&gt; pool\n    print(results)  # (1, 4, 9)\n</code></pre> <p>The <code>&amp;</code> operator does the same thing with a fixed set of computations and full type inference:</p> <pre><code>    a, b, c = (square(4) &amp; square(5) &amp; square(6)) &gt;&gt; pool\n    print(a, b, c)  # 16 25 36\n</code></pre> <p>Both approaches dispatch tasks to the pool's nodes via round-robin scheduling. For a deeper look at parallel execution patterns, see the Parallel Execution guide.</p>"},{"location":"getting-started/#multi-node-clusters","title":"Multi-node clusters","text":"<p>To scale beyond a single instance, set <code>nodes</code> on the pool. The <code>@</code> operator broadcasts a function to every node:</p> <pre><code>import skyward as sky\n\n@sky.compute\ndef worker_info() -&gt; dict:\n    info = sky.instance_info()\n    return {\n        \"node\": info.node,\n        \"total\": info.total_nodes,\n        \"is_head\": info.is_head,\n    }\n\nwith sky.ComputePool(provider=sky.AWS(), nodes=4) as pool:\n    results = worker_info() @ pool\n    for r in results:\n        print(f\"Node {r['node']}/{r['total']} (head={r['is_head']})\")\n</code></pre> <p>Where <code>&gt;&gt;</code> sends work to one node, <code>@</code> sends it to all of them. Each node runs the same function independently, but <code>sky.instance_info()</code> returns different values on each one \u2014 node index, total count, head status \u2014 so the function can adapt its behavior based on where it's running. This is the foundation for distributed training, data-parallel processing, and any workload that benefits from multiple machines. See Broadcast for more.</p>"},{"location":"getting-started/#local-testing","title":"Local testing","text":"<p>During development, you'll want to test your functions without provisioning any cloud infrastructure. Every <code>@sky.compute</code> function exposes the original, unwrapped version via <code>.local</code>:</p> <pre><code>result = my_function.local(test_data)  # executes immediately, no cloud\n</code></pre> <p>This bypasses the lazy computation entirely \u2014 no <code>PendingCompute</code>, no serialization, no pool required. It's the fastest way to iterate on function logic before sending it to the cloud.</p> <p>For integration testing with the full Skyward lifecycle (serialization, bootstrap, worker execution) but without cloud costs, use the <code>Container</code> provider:</p> <pre><code>import skyward as sky\n\nwith sky.ComputePool(provider=sky.Container(), nodes=2) as pool:\n    result = hello() &gt;&gt; pool  # runs in local Docker containers\n</code></pre>"},{"location":"getting-started/#verbose-logging","title":"Verbose logging","text":"<p>Skyward logs lifecycle events through Python's standard <code>logging</code> module under the <code>\"skyward\"</code> logger. To see detailed debug output:</p> <pre><code>import logging\n\nlogging.getLogger(\"skyward\").setLevel(logging.DEBUG)\n</code></pre> <p>This will show SSH connection details, bootstrap script output, serialization sizes, and actor message traces \u2014 useful for diagnosing connectivity or performance issues.</p>"},{"location":"getting-started/#troubleshooting","title":"Troubleshooting","text":"<p>If the pool fails with \"No instances available\", the provider couldn't find capacity for the requested hardware. Try a different region, a different accelerator, or use <code>allocation=\"spot-if-available\"</code> to fall back to on-demand pricing.</p> <p>\"Permission denied\" errors typically mean your IAM or API credentials don't have the required permissions. For AWS, verify that your role or user has <code>ec2:*</code> and <code>iam:PassRole</code>.</p> <p>If bootstrap takes too long or times out, the most common cause is a large <code>pip</code> list \u2014 installing PyTorch from scratch on a fresh instance takes time. You can increase the timeout with <code>provision_timeout=7200</code> on the pool, or reduce the number of pip dependencies.</p> <p>Connection issues on AWS usually mean the instance doesn't have outbound internet access (needed for package installation) or the security group doesn't allow the SSH connection. Skyward creates a temporary security group during <code>prepare()</code>, but VPC configurations can override this. Enabling SSM access provides a fallback connectivity path that doesn't require open inbound ports.</p>"},{"location":"getting-started/#next-steps","title":"Next steps","text":"<ul> <li>Core Concepts \u2014 The programming model: lazy computation, operators, ephemeral pools</li> <li>Providers \u2014 Detailed configuration for AWS, RunPod, VastAI, Verda, and Container</li> <li>Distributed Training \u2014 Multi-node training with PyTorch, Keras, JAX</li> <li>Plugins \u2014 Framework plugins for PyTorch, JAX, Keras, joblib, and scikit-learn</li> </ul>"},{"location":"hpc/","title":"For HPC users","text":"<p>If you're coming from traditional HPC \u2014 SLURM, MPI, shared filesystems, queue-based scheduling \u2014 Skyward solves the same fundamental problem through a different model. Instead of submitting jobs to a scheduler and waiting for allocation on a shared cluster, you provision ephemeral clusters on demand from commercial cloud providers. They exist for the duration of your workload and shut down when done.</p> <p>The concepts map directly. What SLURM calls a \"rank\" is <code>instance_info().node</code>. What MPI calls <code>MPI_COMM_WORLD.Get_size()</code> is <code>instance_info().total_nodes</code>. Rank 0 is <code>is_head</code>. Data decomposition across ranks becomes <code>shard()</code>. Module loads and environment setup become the <code>Image()</code> specification. The distributed training patterns \u2014 head node coordinating workers, data sharded across ranks, gradients synchronized \u2014 are identical. The difference is how you get there.</p>"},{"location":"hpc/#environment-setup","title":"Environment setup","text":"<p>In a traditional HPC environment, you'd configure the compute environment through module loads, environment variables in your batch script, and MPI launch commands. Skyward replaces all of this with the <code>Image</code> and plugins.</p> <pre><code>import skyward as sky\n\nwith sky.ComputePool(provider=sky.VastAI(), accelerator=sky.accelerators.A100(), nodes=4) as pool:\n    train() @ pool  # runs on all 4 nodes\n</code></pre> <p>This is roughly equivalent to:</p> <pre><code>#SBATCH --nodes=4\n#SBATCH --gres=gpu:a100:1\nsrun python train.py\n</code></pre> <p>Skyward sets up the environment variables that frameworks expect \u2014 <code>MASTER_ADDR</code>, <code>RANK</code>, <code>WORLD_SIZE</code> for PyTorch, <code>JAX_COORDINATOR_ADDRESS</code> for JAX, <code>TF_CONFIG</code> for TensorFlow. The framework handles communication via NCCL or Gloo, same as on a traditional cluster. If you've written DDP training code for SLURM, the training loop is identical \u2014 only the launch mechanism changes.</p>"},{"location":"hpc/#what-carries-over","title":"What carries over","text":"<p>Your distributed training patterns work unchanged. You still have a head node coordinating workers, you still shard data across ranks, you still synchronize gradients. The framework-level code \u2014 <code>DistributedDataParallel</code>, <code>all_reduce</code>, <code>DistributedSampler</code>, <code>jax.pmap</code> \u2014 is the same whether it runs on a SLURM cluster or a Skyward pool. If you've spent time learning these APIs, that knowledge transfers directly.</p> <p>Data sharding patterns also carry over. <code>sky.shard()</code> does modulo striding across ranks, which is the same approach as <code>DistributedSampler</code> or manual MPI-based decomposition. If you're doing domain decomposition or custom data splitting, <code>instance_info()</code> gives you the same rank/world-size information you'd get from <code>MPI_Comm_rank()</code> and <code>MPI_Comm_size()</code>.</p>"},{"location":"hpc/#what-doesnt-carry-over","title":"What doesn't carry over","text":"<p>Skyward doesn't wrap MPI. If your code uses <code>mpi4py</code> or raw MPI calls for inter-process communication, you'll need to either configure MPI yourself on the remote instances or refactor to use framework-native collectives (NCCL for PyTorch, XLA for JAX). For most ML workloads this isn't a limitation \u2014 DDP, JAX distributed, and Keras distribution strategies handle the collectives you need \u2014 but tightly-coupled numerical simulations that depend on MPI's point-to-point messaging or custom communicators won't work out of the box.</p> <p>There's no job queue. SLURM manages a backlog of jobs, prioritizes them, and allocates resources as they become available. Skyward provisions immediately or fails \u2014 there's no waiting, but also no backlog management. If the cloud provider can't give you the instances you need right now, you get an error rather than a queue position.</p> <p>Interconnect depends on the provider. Some cloud instances offer InfiniBand or equivalent \u2014 AWS EFA on p5 instances provides 3200 Gbps networking, and some providers offer bare-metal InfiniBand. Others use standard cloud networking. For most ML workloads the difference is negligible: gradient synchronization is bursty (short spikes during all-reduce between training steps), not sustained, so even moderate bandwidth handles it well. Workloads that require tight coupling \u2014 large all-to-all operations, frequent small messages, lattice QCD, molecular dynamics \u2014 should target providers and instance types with high-bandwidth interconnect.</p> <p>Checkpoint/restart isn't built in. SLURM environments typically have a shared filesystem where checkpoints persist across job submissions. Skyward clusters are ephemeral \u2014 when the pool exits, the instances are gone. Your training code needs to save checkpoints to persistent storage (S3, GCS, NFS, or a mounted volume) and resume from them if interrupted. Spot instances get replaced automatically on preemption, but your code is responsible for saving and loading state.</p>"},{"location":"hpc/#the-trade-off","title":"The trade-off","text":"<p>The core trade-off is queue wait vs. cost. On a shared HPC cluster, you don't pay per-hour, but you wait for allocation \u2014 sometimes minutes, sometimes days, depending on the cluster's utilization and your priority in the scheduler. With Skyward, you get immediate access to the latest GPUs (H100, H200, MI300X) from multiple providers, but you pay for the time you use. For teams where GPU access is the bottleneck \u2014 waiting in SLURM queues, competing for GPU time on shared clusters \u2014 eliminating the queue can dramatically accelerate iteration cycles. For teams with free access to well-provisioned clusters, the cost model may not be justified.</p> <p>The other significant trade-off is persistence vs. ephemerality. HPC clusters have shared filesystems that persist between jobs \u2014 your data, code, and checkpoints are always there. Skyward clusters are ephemeral by design: provision, compute, tear down. This means you need to explicitly manage data movement (syncing code via <code>includes</code>, loading data from cloud storage, saving checkpoints externally), but it also means there are no environments to maintain, no software versions to keep in sync across nodes, and no idle costs.</p>"},{"location":"hpc/#when-this-matters","title":"When this matters","text":"<p>If you're training models with PyTorch, JAX, or TensorFlow and your bottleneck is getting access to accelerators \u2014 waiting in SLURM queues, competing for GPU time on shared clusters \u2014 Skyward removes the queue. Your training code stays almost identical; only the launch mechanism changes.</p> <p>If your code is MPI-native and tightly coupled, or if you have free access to a well-provisioned cluster with InfiniBand, traditional HPC may still be the right tool. But cloud instances with EFA, InfiniBand, and the latest GPU generations are closing the gap fast.</p>"},{"location":"providers/","title":"Cloud providers","text":"<p>Skyward supports six providers. Five are cloud services \u2014 AWS, GCP, RunPod, Verda, VastAI \u2014 and one is local containers for development and CI. All implement the same <code>Provider</code> protocol, so the orchestration layer (actor system, SSH tunnels, bootstrap, task dispatch) works identically regardless of which provider you choose. The difference is in how instances are provisioned, what hardware is available, and how authentication works.</p> <p>Provider configs are lightweight frozen dataclasses. They hold configuration \u2014 region, API keys, disk sizes \u2014 but don't import any cloud SDK at module level. The SDK is loaded lazily when the pool starts, so <code>import skyward</code> stays fast regardless of which providers are installed.</p>"},{"location":"providers/#provider-comparison","title":"Provider comparison","text":"Feature AWS GCP RunPod Verda VastAI Container GPUs H100, A100, T4, L4, Trainium, Inferentia H100, A100, T4, L4, V100, H200 H100, A100, A40, RTX series H100, A100, H200, GB200 Marketplace (varies) None (CPU) Spot Instances Yes (60-90% savings) Yes (preemptible/spot) Yes Yes Yes (bid-based) N/A Regions 20+ 40+ zones Global (Secure + Community) FIN, ICL, ISR Global marketplace Local Auth AWS credentials Application Default Credentials API key Client ID + Secret API key None"},{"location":"providers/#aws","title":"AWS","text":"<p>AWS uses EC2 Fleet for provisioning, with automatic spot-to-on-demand fallback. Instances are launched in a VPC with security groups managed by Skyward (or you can provide your own). SSH keys are created per cluster and cleaned up on teardown.</p> <p>AMI resolution happens automatically via SSM Parameter Store \u2014 Skyward looks up the latest Ubuntu AMI for your chosen version and architecture. You can override this with a custom AMI.</p>"},{"location":"providers/#setup","title":"Setup","text":"<pre><code>export AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_key\nexport AWS_DEFAULT_REGION=us-east-1\n</code></pre> <p>Or use the AWS CLI:</p> <pre><code>aws configure\n</code></pre>"},{"location":"providers/#usage","title":"Usage","text":"<pre><code>import skyward as sky\n\nwith sky.ComputePool(\n    provider=sky.AWS(region=\"us-east-1\"),\n    accelerator=sky.accelerators.A100(),\n    nodes=2,\n) as pool:\n    result = train(data) &gt;&gt; pool\n</code></pre>"},{"location":"providers/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>region</code> <code>str</code> <code>\"us-east-1\"</code> AWS region <code>ami</code> <code>str or None</code> <code>None</code> Custom AMI ID. Auto-resolved via SSM if not set. <code>ubuntu_version</code> <code>str</code> <code>\"24.04\"</code> Ubuntu LTS version for auto-resolved AMIs <code>subnet_id</code> <code>str or None</code> <code>None</code> VPC subnet. Uses default VPC if not set. <code>security_group_id</code> <code>str or None</code> <code>None</code> Security group. Auto-created if not set. <code>instance_profile_arn</code> <code>str or None</code> <code>None</code> IAM instance profile. Auto-created if not set. <code>username</code> <code>str or None</code> <code>None</code> SSH user. Auto-detected from AMI if not set. <code>allocation_strategy</code> <code>str</code> <code>\"price-capacity-optimized\"</code> EC2 Fleet spot allocation strategy <code>exclude_burstable</code> <code>bool</code> <code>False</code> Exclude burstable instances (t3, t4g)"},{"location":"providers/#required-iam-permissions","title":"Required IAM permissions","text":"<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ec2:RunInstances\",\n                \"ec2:TerminateInstances\",\n                \"ec2:DescribeInstances\",\n                \"ec2:DescribeInstanceTypes\",\n                \"ec2:DescribeImages\",\n                \"ec2:CreateSecurityGroup\",\n                \"ec2:AuthorizeSecurityGroupIngress\",\n                \"ec2:DescribeSecurityGroups\",\n                \"ec2:CreateKeyPair\",\n                \"ec2:DescribeKeyPairs\",\n                \"ec2:CreateFleet\",\n                \"ec2:DescribeFleets\",\n                \"ssm:GetParameter\"\n            ],\n            \"Resource\": \"*\"\n        },\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": \"iam:PassRole\",\n            \"Resource\": \"arn:aws:iam::*:role/*\"\n        }\n    ]\n}\n</code></pre>"},{"location":"providers/#gcp","title":"GCP","text":"<p>GCP uses Compute Engine with instance templates and <code>bulk_insert</code> for fleet-style provisioning. Skyward resolves the best machine type dynamically \u2014 for GPUs like T4 and V100, it uses N1 machines with guest accelerators; for A100 and H100, it picks the matching A2/A3 machine family with built-in GPUs. Spot instances use the <code>SPOT</code> provisioning model with automatic deletion on preemption.</p> <p>SSH keys are injected via instance metadata. The project is auto-detected from Application Default Credentials or <code>GOOGLE_CLOUD_PROJECT</code>. GCP API calls use sync clients dispatched to a dedicated thread pool (configurable via <code>thread_pool_size</code>).</p> <p>Skyward creates an instance template and a firewall rule per cluster, both cleaned up on teardown. Instances use Google's Deep Learning VM images (CUDA 12.x, NVIDIA 570 drivers) for GPU workloads.</p>"},{"location":"providers/#setup_1","title":"Setup","text":"<pre><code>gcloud auth application-default login\n</code></pre> <p>Or set the project explicitly:</p> <pre><code>export GOOGLE_CLOUD_PROJECT=your_project_id\n</code></pre> <p>GPU Quotas</p> <p>Listing available accelerator types does not mean you have quota. Check your quotas with: <pre><code>gcloud compute regions describe &lt;region&gt; --format=\"table(quotas.metric,quotas.limit,quotas.usage)\" | grep GPU\n</code></pre> Request quota increases in the Cloud Console.</p>"},{"location":"providers/#usage_1","title":"Usage","text":"<pre><code>import skyward as sky\n\nwith sky.ComputePool(\n    provider=sky.GCP(zone=\"us-central1-a\"),\n    accelerator=sky.accelerators.T4(),\n    nodes=2,\n) as pool:\n    result = train(data) &gt;&gt; pool\n</code></pre>"},{"location":"providers/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>project</code> <code>str or None</code> <code>None</code> GCP project ID. Auto-detected from ADC or <code>GOOGLE_CLOUD_PROJECT</code>. <code>zone</code> <code>str</code> <code>\"us-central1-a\"</code> Compute Engine zone <code>network</code> <code>str</code> <code>\"default\"</code> VPC network name <code>subnet</code> <code>str or None</code> <code>None</code> Specific subnet. Uses auto-mode subnet if not set. <code>disk_size_gb</code> <code>int</code> <code>200</code> Boot disk size in GB <code>disk_type</code> <code>str</code> <code>\"pd-balanced\"</code> Boot disk type <code>instance_timeout</code> <code>int</code> <code>300</code> Safety timeout in seconds (self-destruction timer) <code>service_account</code> <code>str or None</code> <code>None</code> GCE service account email <code>thread_pool_size</code> <code>int</code> <code>8</code> Thread pool size for blocking GCP API calls"},{"location":"providers/#required-permissions","title":"Required permissions","text":"<p>The authenticated principal needs the following roles (or equivalent permissions):</p> <ul> <li><code>compute.instances.create</code>, <code>compute.instances.delete</code>, <code>compute.instances.list</code>, <code>compute.instances.get</code></li> <li><code>compute.instanceTemplates.create</code>, <code>compute.instanceTemplates.delete</code></li> <li><code>compute.firewalls.create</code>, <code>compute.firewalls.delete</code>, <code>compute.firewalls.get</code></li> <li><code>compute.machineTypes.list</code>, <code>compute.acceleratorTypes.list</code></li> <li><code>compute.images.getFromFamily</code></li> </ul> <p>The simplest approach is the Compute Admin role (<code>roles/compute.admin</code>).</p>"},{"location":"providers/#install","title":"Install","text":"<pre><code>uv add \"skyward[gcp]\"\n</code></pre>"},{"location":"providers/#runpod","title":"RunPod","text":"<p>RunPod offers GPU pods in two tiers: Secure Cloud (enterprise-grade, dedicated hardware) and Community Cloud (lower-cost, peer-hosted). Skyward provisions pods via RunPod's GraphQL API, configures SSH access, and manages the full lifecycle.</p> <p>SSH keys are auto-detected from <code>~/.ssh/id_ed25519.pub</code> or <code>~/.ssh/id_rsa.pub</code> and registered on your RunPod account.</p>"},{"location":"providers/#setup_2","title":"Setup","text":"<pre><code>export RUNPOD_API_KEY=your_api_key\n</code></pre>"},{"location":"providers/#usage_2","title":"Usage","text":"<pre><code>import skyward as sky\n\nwith sky.ComputePool(\n    provider=sky.RunPod(),\n    accelerator=sky.accelerators.A100(),\n    nodes=2,\n) as pool:\n    result = train(data) &gt;&gt; pool\n</code></pre>"},{"location":"providers/#parameters_2","title":"Parameters","text":"Parameter Type Default Description <code>api_key</code> <code>str or None</code> <code>None</code> API key (falls back to <code>RUNPOD_API_KEY</code> env var) <code>cloud_type</code> <code>CloudType</code> <code>SECURE</code> <code>CloudType.SECURE</code> or <code>CloudType.COMMUNITY</code> <code>container_disk_gb</code> <code>int</code> <code>50</code> Container disk size in GB <code>volume_gb</code> <code>int</code> <code>20</code> Persistent volume size in GB <code>volume_mount_path</code> <code>str</code> <code>\"/workspace\"</code> Volume mount path <code>data_center_ids</code> <code>tuple or \"global\"</code> <code>\"global\"</code> Preferred data centers or <code>\"global\"</code> for auto-selection <code>ports</code> <code>tuple[str, ...]</code> <code>(\"22/tcp\",)</code> Port mappings"},{"location":"providers/#verda","title":"Verda","text":"<p>Verda is a GPU cloud with data centers in Europe and the Middle East. It uses OAuth2 authentication with a client ID and secret \u2014 not a single API key.</p> <p>SSH keys are auto-detected and registered on Verda if needed. If <code>region</code> is not specified (the default is <code>\"FIN-01\"</code>), Verda uses its default region. The provider also supports auto-region discovery: if the requested GPU isn't available in the configured region, Skyward finds another region with availability.</p>"},{"location":"providers/#setup_3","title":"Setup","text":"<pre><code>export VERDA_CLIENT_ID=your_client_id\nexport VERDA_CLIENT_SECRET=your_client_secret\n</code></pre>"},{"location":"providers/#usage_3","title":"Usage","text":"<pre><code>import skyward as sky\n\nwith sky.ComputePool(\n    provider=sky.Verda(),\n    accelerator=sky.accelerators.H100(),\n    nodes=4,\n) as pool:\n    results = train() @ pool\n</code></pre>"},{"location":"providers/#parameters_3","title":"Parameters","text":"Parameter Type Default Description <code>region</code> <code>str</code> <code>\"FIN-01\"</code> Preferred region <code>client_id</code> <code>str or None</code> <code>None</code> OAuth2 client ID (falls back to <code>VERDA_CLIENT_ID</code>) <code>client_secret</code> <code>str or None</code> <code>None</code> OAuth2 client secret (falls back to <code>VERDA_CLIENT_SECRET</code>) <code>ssh_key_id</code> <code>str or None</code> <code>None</code> Specific SSH key ID to use"},{"location":"providers/#available-regions","title":"Available regions","text":"Region Location GPUs <code>FIN-01</code> Finland H100, A100, H200, GB200 <code>ICL-01</code> Iceland H100, A100 <code>ISR-01</code> Israel H100, A100"},{"location":"providers/#vastai","title":"VastAI","text":"<p>VastAI is a GPU marketplace \u2014 instances are Docker containers running on hosts from independent providers worldwide. Pricing is dynamic, and reliability varies by host. Skyward filters offers by reliability score, CUDA version, and optional geolocation, then provisions containers via the VastAI API.</p> <p>SSH keys are auto-detected from <code>~/.ssh/id_ed25519.pub</code> or <code>~/.ssh/id_rsa.pub</code> and registered on VastAI if needed. For multi-node clusters, VastAI supports overlay networks for NCCL communication between instances.</p>"},{"location":"providers/#setup_4","title":"Setup","text":"<pre><code>export VAST_API_KEY=your_api_key\n</code></pre> <p>Get your API key at: https://cloud.vast.ai/account/</p>"},{"location":"providers/#usage_4","title":"Usage","text":"<pre><code>import skyward as sky\n\nwith sky.ComputePool(\n    provider=sky.VastAI(geolocation=\"US\"),\n    accelerator=sky.accelerators.RTX_4090(),\n    nodes=2,\n) as pool:\n    result = train(data) &gt;&gt; pool\n</code></pre>"},{"location":"providers/#parameters_4","title":"Parameters","text":"Parameter Type Default Description <code>api_key</code> <code>str or None</code> <code>None</code> API key (falls back to <code>VAST_API_KEY</code>) <code>min_reliability</code> <code>float</code> <code>0.95</code> Minimum host reliability score (0.0-1.0) <code>min_cuda</code> <code>float</code> <code>12.0</code> Minimum CUDA version <code>geolocation</code> <code>str or None</code> <code>None</code> Filter by region/country (e.g., <code>\"US\"</code>, <code>\"EU\"</code>) <code>bid_multiplier</code> <code>float</code> <code>1.2</code> Multiplier for spot bid price <code>docker_image</code> <code>str or None</code> <code>None</code> Base Docker image for containers <code>disk_gb</code> <code>int</code> <code>100</code> Disk space in GB <code>use_overlay</code> <code>bool</code> <code>True</code> Enable overlay networking for multi-node clusters <code>require_direct_port</code> <code>bool</code> <code>False</code> Only select offers with direct port access <p>VastAI also provides a helper for building NVIDIA CUDA base images:</p> <pre><code>image_name = sky.VastAI.ubuntu(version=\"24.04\", cuda=\"12.9.1\")\n# \u2192 \"nvcr.io/nvidia/cuda:12.9.1-runtime-ubuntu24.04\"\n</code></pre>"},{"location":"providers/#container","title":"Container","text":"<p>The Container provider runs compute nodes as local containers \u2014 Docker, podman, nerdctl, or Apple's container CLI. No cloud credentials, no costs. Useful for development, CI testing, and validating your code before deploying to real hardware.</p> <p>Containers are launched with SSH access, joined to a shared network, and bootstrapped the same way cloud instances are. From the pool's perspective, they look like any other nodes.</p>"},{"location":"providers/#usage_5","title":"Usage","text":"<pre><code>import skyward as sky\n\nwith sky.ComputePool(\n    provider=sky.Container(),\n    nodes=2,\n    image=sky.Image(pip=[\"numpy\"]),\n) as pool:\n    result = train(data) &gt;&gt; pool\n</code></pre>"},{"location":"providers/#parameters_5","title":"Parameters","text":"Parameter Type Default Description <code>image</code> <code>str</code> <code>\"ubuntu:24.04\"</code> Docker image <code>ssh_user</code> <code>str</code> <code>\"root\"</code> SSH user inside the container <code>binary</code> <code>str</code> <code>\"docker\"</code> Container runtime (<code>\"docker\"</code>, <code>\"podman\"</code>, <code>\"nerdctl\"</code>) <code>container_prefix</code> <code>str or None</code> <code>None</code> Prefix for container names <code>network</code> <code>str or None</code> <code>None</code> Docker network name. Auto-created if not set."},{"location":"providers/#choosing-a-provider","title":"Choosing a provider","text":"<p>AWS \u2014 When you need specific hardware (H100, Trainium, Inferentia), spot instance savings, or enterprise reliability. Best if you're already in the AWS ecosystem.</p> <p>GCP \u2014 Deep integration with Google Cloud. Deep Learning VM images with pre-installed CUDA drivers, dynamic machine type resolution, fleet-style provisioning via <code>bulk_insert</code>. Supports T4, L4, V100, A100, H100, H200.</p> <p>RunPod \u2014 Fast provisioning, competitive pricing, minimal setup. Both Secure Cloud (dedicated) and Community Cloud (cheaper) tiers. Good for A100/H100/RTX workloads.</p> <p>Verda \u2014 European data residency (Finland, Iceland, Israel). H100/A100/H200/GB200 availability with automatic region selection.</p> <p>VastAI \u2014 Maximum cost savings through marketplace pricing. Consumer GPUs (RTX 4090, 3090) available alongside datacenter hardware. Overlay networks for multi-node training.</p> <p>Container \u2014 Local development and CI. Zero cost, instant provisioning. Validates your code end-to-end before deploying to a real provider.</p>"},{"location":"providers/#common-issues","title":"Common issues","text":""},{"location":"providers/#gcp-no-gcp-accelerator-matches","title":"GCP: \"No GCP accelerator matches\"","text":"<ol> <li>Check available accelerators in your zone: <code>gcloud compute accelerator-types list --filter=\"zone:us-central1-a\"</code></li> <li>Try a different zone \u2014 GPU availability varies by zone</li> <li>Request GPU quota increases in the Cloud Console</li> </ol>"},{"location":"providers/#gcp-quota-exceeded","title":"GCP: \"Quota exceeded\"","text":"<ol> <li>Check current quotas: <code>gcloud compute regions describe &lt;region&gt; | grep -A2 GPU</code></li> <li>Request increases for the specific GPU type (e.g., <code>NVIDIA_T4_GPUS</code>, <code>NVIDIA_L4_GPUS</code>)</li> <li>Both on-demand and preemptible quotas are separate \u2014 check both</li> </ol>"},{"location":"providers/#aws-no-instances-available","title":"AWS: \"No instances available\"","text":"<ol> <li>Try a different region</li> <li>Use <code>allocation=\"spot-if-available\"</code> (the default) to fall back to on-demand</li> <li>Request a service quota increase in the AWS console</li> </ol>"},{"location":"providers/#verda-region-not-available","title":"Verda: \"Region not available\"","text":"<ol> <li>The default region is <code>\"FIN-01\"</code> \u2014 try a different one or let auto-discovery find capacity</li> <li>Check your account's region access</li> </ol>"},{"location":"providers/#vastai-no-offers-available","title":"VastAI: \"No offers available\"","text":"<ol> <li>Lower <code>min_reliability</code> (e.g., 0.8)</li> <li>Expand or remove the <code>geolocation</code> filter</li> <li>Try a different accelerator type</li> <li>Check marketplace availability at https://cloud.vast.ai/</li> </ol>"},{"location":"providers/#related-topics","title":"Related topics","text":"<ul> <li>Getting Started \u2014 Installation and credentials setup</li> <li>Accelerators \u2014 Accelerator selection guide</li> <li>API Reference \u2014 Complete API documentation</li> </ul>"},{"location":"provision-controllers/","title":"Provision controllers","text":"<p>A fixed-size pool works well when you know in advance how much compute you need. But many workloads don't have that property. A hyperparameter sweep might start with a burst of 200 trials and taper off as early stopping kills the losers. An inference service might see ten requests per second at noon and zero at midnight. A data pipeline might have phases \u2014 heavy preprocessing, then light aggregation \u2014 where the ideal cluster size changes mid-job.</p> <p>Skyward handles this with elastic pools: pools whose node count adjusts automatically based on workload pressure. Two actors make this work \u2014 the autoscaler and the reconciler \u2014 and they are designed around a clean separation: the autoscaler is a pure policy engine that decides how many nodes are needed, while the reconciler is the actuator that provisions and terminates cloud instances to match that decision. Neither knows about the other's internals. They communicate through a single message: <code>DesiredCountChanged</code>.</p>"},{"location":"provision-controllers/#enabling-elastic-pools","title":"Enabling elastic pools","text":"<p>Pass a tuple instead of an integer for the <code>nodes</code> parameter:</p> <pre><code>import skyward as sky\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"A100\",\n    nodes=(2, 16),  # min 2, max 16\n    image=sky.Image(pip=[\"torch\"]),\n) as pool:\n    results = sky.gather(*tasks) &gt;&gt; pool\n</code></pre> <p>The pool starts with the minimum number of nodes (2 in this case) and can grow up to the maximum (16) based on demand. When the workload decreases, it shrinks back down \u2014 but never below the minimum.</p> <p>The two tuning knobs are <code>autoscale_cooldown</code> and <code>autoscale_idle_timeout</code>:</p> <pre><code>sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=(2, 16),\n    autoscale_cooldown=30.0,         # minimum seconds between scaling decisions\n    autoscale_idle_timeout=60.0,     # seconds of idleness before scaling down\n    reconcile_tick_interval=15.0,    # seconds between drift-check ticks\n)\n</code></pre> <p><code>autoscale_cooldown</code> prevents rapid oscillation \u2014 the autoscaler won't change its mind more than once every 30 seconds. <code>autoscale_idle_timeout</code> controls how patient the system is before releasing idle capacity \u2014 a cluster with no inflight tasks must stay idle for 60 seconds before the autoscaler decides to shrink it. <code>reconcile_tick_interval</code> sets how often the reconciler checks for drift between the desired and actual cluster size \u2014 lower values detect failures faster but add more overhead.</p>"},{"location":"provision-controllers/#the-autoscaler","title":"The autoscaler","text":"<p>The autoscaler receives pressure reports from the task manager and translates them into scaling decisions. A pressure report is a snapshot of the task manager's state: how many tasks are queued, how many are currently running, what the total capacity is, and how many nodes exist. The task manager emits a report on every state change \u2014 task submitted, task completed, node added, node removed \u2014 so the autoscaler always has a current picture.</p> <p>The scaling logic is a pure function. It takes the current pressure, the current desired count, and the configuration parameters, and returns a new desired count. No cloud API calls, no side effects, no internal state beyond what's passed in. This makes the algorithm trivially testable \u2014 and trivially replaceable, if you ever need a different policy.</p>"},{"location":"provision-controllers/#the-decision-tree","title":"The decision tree","text":"<p>The autoscaler evaluates these conditions in priority order:</p> <p>Scale up \u2014 queued tasks. If tasks are waiting in the queue with no available slots, the cluster needs to grow. The autoscaler computes how many additional nodes would be needed to drain the queue (<code>ceil(queued / slots_per_node)</code>) and adds that many to the current count, capped at <code>max_nodes</code>. This is the only condition that scales aggressively \u2014 it reacts immediately to demand rather than waiting for a cooldown cycle.</p> <p>Scale down \u2014 fully idle. If no tasks are running (<code>inflight == 0</code>) and the cluster has been idle for longer than <code>autoscale_idle_timeout</code>, collapse to <code>min_nodes</code>. This is the steady-state response: when there's genuinely no work, release everything that isn't in the minimum set.</p> <p>Scale down \u2014 low utilization. If there's no queue but utilization is below 30%, the cluster is over-provisioned for the current workload. The autoscaler computes the minimum number of nodes that would handle the current inflight tasks (<code>ceil(inflight / slots_per_node) + 1</code>, with a +1 buffer) and scales down to that. This catches the case where a burst of work has finished but a trickle remains \u2014 you don't need 16 nodes for 3 running tasks.</p> <p>Steady state. If none of the above conditions are met, the desired count stays the same. The cluster is appropriately sized for its current workload.</p>"},{"location":"provision-controllers/#cooldown-and-the-timer-tick","title":"Cooldown and the timer tick","text":"<p>Two mechanisms prevent the autoscaler from thrashing:</p> <p>The cooldown window (default 30 seconds) suppresses decisions. If the last scaling action was less than <code>cooldown</code> seconds ago, incoming pressure reports are stored but not acted upon. This gives the cluster time to absorb the effects of a previous scaling decision \u2014 new nodes need time to boot, and tasks need time to migrate \u2014 before the autoscaler re-evaluates.</p> <p>The timer tick fires every <code>cooldown</code> seconds and replays the most recent pressure report. This is essential for scale-down: if the cluster becomes idle and no new tasks arrive, there are no pressure reports to trigger a re-evaluation. The tick ensures that the \"fully idle\" condition is eventually detected even in the absence of new activity. Without it, an idle cluster would stay at its current size forever.</p>"},{"location":"provision-controllers/#the-reconciler","title":"The reconciler","text":"<p>The reconciler translates the autoscaler's decisions into infrastructure changes. It tracks three sets of nodes \u2014 <code>current</code> (active and healthy), <code>pending</code> (provisioning in flight), and <code>draining</code> (being removed) \u2014 and progresses through three states to bring the actual cluster size in line with the desired count.</p> <pre><code>graph LR\n    watching[\"&lt;b&gt;watching&lt;/b&gt;&lt;br/&gt;monitoring for changes\"]\n    scaling_up[\"&lt;b&gt;scaling_up&lt;/b&gt;&lt;br/&gt;provisioning in flight\"]\n    draining[\"&lt;b&gt;draining&lt;/b&gt;&lt;br/&gt;removing excess nodes\"]\n\n    watching -- \"desired &gt; effective\" --&gt; scaling_up\n    watching -- \"desired &lt; current\" --&gt; draining\n    scaling_up -- \"provision complete\" --&gt; watching\n    scaling_up -- \"still need more\" --&gt; scaling_up\n    draining -- \"all nodes drained\" --&gt; watching\n    draining -- \"desired increased\" --&gt; watching\n    draining -- \"desired increased\" --&gt; scaling_up</code></pre> <p>The key metric is effective count: <code>len(current) + len(pending)</code>. Nodes that are still booting count toward the total, which prevents the reconciler from over-provisioning while instances are starting up.</p>"},{"location":"provision-controllers/#watching","title":"Watching","text":"<p>The default state. The reconciler monitors for three kinds of events:</p> <ul> <li><code>DesiredCountChanged</code> \u2014 The autoscaler decided the cluster should be a different size. If the desired count exceeds the effective count, transition to <code>scaling_up</code>. If it's less than the current count, transition to <code>draining</code>. Otherwise, nothing to do.</li> <li><code>ReconcilerNodeLost</code> \u2014 A node died (spot preemption, network failure, crash). Remove it from the active set and, if the cluster is now below the desired count, transition to <code>scaling_up</code>.</li> <li><code>NodeJoined</code> \u2014 A previously pending node finished booting and is now active. Move it from <code>pending</code> to <code>current</code>.</li> </ul>"},{"location":"provision-controllers/#scaling-up","title":"Scaling up","text":"<p>The reconciler calls <code>provider.provision(cluster, count)</code> to launch new instances. When the call returns, it tells the pool actor to spawn node actors for the new instances (<code>SpawnNodes</code>), moves their IDs into the <code>pending</code> set, and checks whether the effective count now meets the desired count. If not \u2014 because the desired count changed mid-flight, or because the provider returned fewer instances than requested \u2014 it issues another provision call. Once the effective count meets or exceeds the desired count, it returns to <code>watching</code>.</p> <p>If provisioning fails (cloud quota exceeded, API error, transient failure), the reconciler logs the error and returns to <code>watching</code>. It doesn't retry immediately \u2014 the reconcile tick (described below) will catch the drift and try again on the next cycle. This avoids hammering a failing API.</p>"},{"location":"provision-controllers/#draining","title":"Draining","text":"<p>When the cluster needs to shrink, the reconciler selects victim nodes \u2014 highest ID first \u2014 and asks the pool actor to drain them. Node 0 (the head node) is explicitly excluded from victim selection, because distributed training frameworks depend on the head node's address for coordination.</p> <p>Draining is cooperative. The reconciler sends <code>DrainNode</code> to the pool actor, which removes the node from the task manager's rotation (so no new tasks are dispatched to it) and replies with <code>DrainComplete</code>. For each drained node, the reconciler issues <code>provider.terminate</code> to destroy the cloud instance. Termination is fire-and-forget \u2014 the reconciler doesn't wait for the cloud API to confirm the instance is gone.</p> <p>If the desired count increases while draining is in progress (new tasks arrived, the autoscaler changed its mind), the reconciler can abort the drain. It clears the draining set and, if the new desired count exceeds the effective count, transitions directly to <code>scaling_up</code>. This means the system responds to new demand even in the middle of scaling down.</p>"},{"location":"provision-controllers/#the-reconcile-tick","title":"The reconcile tick","text":"<p>Periodically (every <code>reconcile_tick_interval</code> seconds \u2014 15 by default), the reconciler checks for drift: is the effective count below the desired count? If so, it initiates a scale-up. This is the self-healing mechanism \u2014 if a provisioning call failed, if a node crashed between ticks, if the cloud provider returned fewer instances than requested, the tick catches the discrepancy and corrects it. The reconciler doesn't need explicit retry logic because the tick provides implicit, periodic retries.</p>"},{"location":"provision-controllers/#always-on-reconciliation","title":"Always-on reconciliation","text":"<p>The reconciler is spawned for every pool \u2014 not just elastic ones. In a fixed-size pool (<code>nodes=4</code>), the reconciler starts with <code>min_nodes = max_nodes = 4</code> and the autoscaler is never created. No <code>DesiredCountChanged</code> messages arrive, so the reconciler stays in the <code>watching</code> state permanently.</p> <p>But the reconcile tick still fires, and <code>ReconcilerNodeLost</code> messages still arrive. If a node dies in a fixed pool, the reconciler detects that the effective count (3) is below the desired count (4) and provisions a replacement. The pool self-heals without any elastic configuration \u2014 the reconciliation loop handles node replacement as a natural consequence of keeping <code>effective == desired</code>.</p>"},{"location":"provision-controllers/#how-they-interact","title":"How they interact","text":"<p>The full interaction involves four actors: the task manager observes workload pressure, the autoscaler decides the right cluster size, the reconciler makes it happen, and the pool actor manages node lifecycles.</p> <pre><code>sequenceDiagram\n    participant TM as Task Manager\n    participant AS as Autoscaler\n    participant RC as Reconciler\n    participant Pool as Pool Actor\n    participant Cloud as Cloud Provider\n\n    Note over TM: tasks accumulate in queue\n\n    TM-&gt;&gt;AS: PressureReport(queued=12, inflight=4, capacity=8)\n    AS-&gt;&gt;AS: compute_desired() \u2192 6 nodes\n    AS-&gt;&gt;RC: DesiredCountChanged(desired=6)\n    RC-&gt;&gt;Cloud: provision(cluster, 2)\n    Cloud--&gt;&gt;RC: 2 new instances\n    RC-&gt;&gt;Pool: SpawnNodes(instances, start_id=4)\n    Pool-&gt;&gt;Pool: spawn node actors 4, 5\n\n    Note over Pool: nodes boot, bootstrap, join\n\n    Pool-&gt;&gt;RC: NodeJoined(node_id=4)\n    Pool-&gt;&gt;RC: NodeJoined(node_id=5)\n    Pool-&gt;&gt;TM: NodeAvailable(node_id=4)\n    Pool-&gt;&gt;TM: NodeAvailable(node_id=5)\n\n    Note over TM: queue drains with more capacity\n\n    TM-&gt;&gt;AS: PressureReport(queued=0, inflight=2, capacity=12)\n\n    Note over AS: idle for 60s, tick fires\n\n    AS-&gt;&gt;RC: DesiredCountChanged(desired=2)\n    RC-&gt;&gt;Pool: DrainNode(node_id=5)\n    RC-&gt;&gt;Pool: DrainNode(node_id=4)\n    RC-&gt;&gt;Pool: DrainNode(node_id=3)\n    RC-&gt;&gt;Pool: DrainNode(node_id=2)\n    Pool--&gt;&gt;RC: DrainComplete(5), DrainComplete(4), DrainComplete(3), DrainComplete(2)\n    RC-&gt;&gt;Cloud: terminate instances</code></pre> <p>The flow is fully event-driven. The task manager doesn't know about the autoscaler \u2014 it just emits pressure reports to whoever registers as an observer. The autoscaler doesn't know about the cloud provider \u2014 it just tells the reconciler what the desired count should be. The reconciler doesn't know about task scheduling \u2014 it just brings the infrastructure in line with the desired count. Each actor has a single, well-defined responsibility, and communication happens through typed messages.</p>"},{"location":"provision-controllers/#design-decisions","title":"Design decisions","text":"<p>Policy and mechanism are separate. The autoscaler is a pure function wrapped in a thin actor. It has no cloud SDK dependency, no async I/O, no state beyond a few numbers. The reconciler does all the infrastructure work. This means you can reason about scaling policy by reading a single function (<code>_compute_desired</code>), and you can reason about infrastructure orchestration by reading the reconciler's state machine \u2014 without either concern polluting the other.</p> <p>Pending nodes count toward effective. When the reconciler calls <code>provision()</code>, the new node IDs are added to the <code>pending</code> set and counted in the effective total. This prevents the autoscaler from seeing a gap (desired 6, effective 4) and triggering another provision call while the first batch is still booting. Over-provisioning wastes money; counting pending nodes prevents it.</p> <p>The head node is protected. Drain victim selection sorts nodes by ID in descending order and explicitly excludes node 0. This guarantees that the head node \u2014 whose address is used as <code>MASTER_ADDR</code> for distributed training frameworks \u2014 survives any scale-down event. Removing the head mid-training would break every framework's coordination protocol.</p> <p>Drains are abortable. If the autoscaler raises the desired count while a drain is in progress, the reconciler clears the draining set and scales up immediately. This avoids the pathological case where the system scales down and immediately scales back up \u2014 the in-progress drain is simply cancelled, and the surviving nodes absorb the new work.</p> <p>The reconcile tick provides implicit retries. Rather than implementing explicit retry logic with exponential backoff, the reconciler checks for drift every <code>reconcile_tick_interval</code> seconds (default 15). If a provision call fails, the next tick detects that <code>effective &lt; desired</code> and tries again. This is simpler, more robust, and naturally rate-limited.</p>"},{"location":"provision-controllers/#further-reading","title":"Further reading","text":"<ul> <li>Core Concepts \u2014 Lazy computation, operators, and the pool lifecycle</li> <li>Architecture \u2014 The actor hierarchy and cluster formation</li> <li>Providers \u2014 Cloud provider configuration and protocols</li> </ul>"},{"location":"sky-computing/","title":"Sky computing","text":"<p>\"We don't ask that readers accept Sky Computing as inevitable, merely as not impossible.\"</p> <p>\u2014 The Sky Above The Clouds, Berkeley 2022</p> <p>Technology ecosystems follow a predictable pattern. Telephony started with AT&amp;T's monopoly and evolved into carrier interoperability. The Internet began with proprietary networks \u2014 CompuServe, AOL, Prodigy \u2014 and converged on universal TCP/IP. Personal computers started with IBM dominance and opened into the x86 ecosystem. In each case, single providers or closed systems gave way to competitive markets with compatibility standards.</p> <p>Cloud computing, barely 15 years old, is still in its pre-standards phase. Each provider is a silo with proprietary APIs: AWS, GCP, and Azure each have their own compute, storage, and networking interfaces. The paper The Sky Above The Clouds (Berkeley, 2022) argues that this is the natural starting point of a technology ecosystem, not its final state. The proposed answer is Sky Computing \u2014 a compatibility layer above the clouds that routes workloads to the best provider based on cost, availability, or performance, without locking users to a single vendor.</p>"},{"location":"sky-computing/#the-vision","title":"The vision","text":"<p>Sky Computing proposes an intercloud broker that abstracts provider differences behind a unified interface. You describe what you need \u2014 compute, storage, accelerators \u2014 and the broker finds the best option across all available clouds. If one provider is cheaper, the workload goes there. If another has better availability for the GPU you need, it goes there instead. If a provider has an outage, the broker fails over transparently.</p> <p>This is the same idea behind carrier portability in telephony (keep your phone number when you switch carriers) or TCP/IP in networking (your application doesn't know whether the packet travels over fiber, copper, or wireless). The abstraction boundary moves up: applications talk to the compatibility layer, the compatibility layer talks to providers.</p>"},{"location":"sky-computing/#how-skyward-uses-these-ideas","title":"How Skyward uses these ideas","text":"<p>Skyward focuses on a specific problem within this vision: running Python functions on remote accelerators without worrying about where. The full Sky Computing vision includes storage interoperability, network optimization, and global job scheduling across clouds. Skyward narrows the scope to compute orchestration for ML workloads \u2014 but within that scope, it implements the key ideas from the paper.</p>"},{"location":"sky-computing/#provider-portability","title":"Provider portability","text":"<p>All providers implement the same <code>Provider</code> protocol \u2014 five methods (<code>prepare</code>, <code>provision</code>, <code>get_instance</code>, <code>terminate</code>, <code>teardown</code>) that map to the pool lifecycle. Switching between providers is a one-line change:</p> <pre><code># Development: local containers, zero cost\nwith sky.ComputePool(provider=sky.Container(), nodes=2) as pool:\n    result = train(data) &gt;&gt; pool\n\n# Production: real GPUs on AWS\nwith sky.ComputePool(provider=sky.AWS(), accelerator=\"H100\", nodes=4) as pool:\n    result = train(data) &gt;&gt; pool\n\n# Same code, different cloud\nwith sky.ComputePool(provider=sky.RunPod(), accelerator=\"H100\", nodes=4) as pool:\n    result = train(data) &gt;&gt; pool\n</code></pre> <p>The <code>@sky.compute</code> functions, the operators, the <code>Image</code> specification \u2014 everything stays identical across providers. Your code doesn't know which cloud it's running on, and it doesn't need to. This is the practical realization of the Sky Computing idea: you're not locked to a single cloud, and moving between them doesn't require rewriting anything.</p>"},{"location":"sky-computing/#unified-resource-specification","title":"Unified resource specification","text":"<p>You describe hardware needs in logical terms \u2014 <code>\"A100\"</code>, <code>sky.accelerators.H100(count=4)</code> \u2014 and the provider translates that into whatever its API requires. An \"A100\" on AWS is a <code>p4d.24xlarge</code> instance. On RunPod, it's a GPU pod with a specific <code>gpuTypeId</code>. On VastAI, it's a marketplace offer filtered by GPU model. The translation from a logical accelerator name to a provider-specific resource involves resolving instance types, memory variants, multi-GPU configurations, and availability constraints. The accelerator catalog centralizes this complexity so that <code>sky.accelerators.A100(count=4)</code> resolves correctly on any provider that supports it.</p>"},{"location":"sky-computing/#economic-arbitrage","title":"Economic arbitrage","text":"<p>The <code>allocation</code> parameter is a simple form of economic optimization. <code>\"spot-if-available\"</code> (the default) requests discounted spot capacity first and falls back to on-demand if none is available. <code>\"spot\"</code> always uses spot instances for maximum savings. <code>\"cheapest\"</code> compares all options and picks the lowest-cost one. These aren't as sophisticated as a full intercloud broker optimizing across providers simultaneously, but they capture the core idea: let the system find the best price for the hardware you need, rather than manually comparing instance types and pricing pages.</p> <p>With multi-spec pools, Skyward takes this further. You can describe the same hardware need across multiple providers \u2014 <code>sky.Spec(provider=sky.VastAI(), accelerator=\"A100\")</code> alongside <code>sky.Spec(provider=sky.AWS(), accelerator=\"A100\")</code> \u2014 and Skyward queries all of them, compares the offers, and provisions from the cheapest. This is cross-provider price comparison at the API level, applied automatically at pool start. See Resource Selection for details.</p>"},{"location":"sky-computing/#what-skyward-adds-ephemeral-compute","title":"What Skyward adds: ephemeral compute","text":"<p>While the Berkeley paper focuses on interoperability, Skyward adds a specific philosophy: ephemeral compute. Accelerator infrastructure should exist only during your job \u2014 not before, not after. The <code>ComputePool</code> context manager guarantees this: provision on enter, destroy on exit, cleanup guaranteed even if your code throws an exception.</p> <pre><code>with sky.ComputePool(provider=sky.AWS(), accelerator=\"H100\", nodes=4) as pool:\n    metrics = train(dataset) @ pool\n# all instances terminated \u2014 no idle costs, no forgotten machines\n</code></pre> <p>This model fits ML workloads naturally. Training runs, fine-tuning jobs, hyperparameter sweeps, batch inference \u2014 these are all tasks with a beginning and an end. There are no machines to forget about, no environments that drift over time, no idle costs accumulating overnight. The pool's lifetime is the job's lifetime.</p>"},{"location":"sky-computing/#skyward-vs-skypilot","title":"Skyward vs SkyPilot","text":"<p>Both projects draw from the Sky Computing vision. The difference is in programming model.</p> <p>SkyPilot is job-oriented. You define a task in YAML \u2014 a script, resource requirements, and storage mounts \u2014 and run <code>sky launch</code> to submit it. SkyPilot provisions a cluster, runs your script, and optionally tears it down. The cluster can persist between jobs (for iterative development) or auto-shutdown after idle time. Results are written to storage (S3, GCS, etc.) rather than returned to your code. SkyPilot excels at batch pipelines where jobs are self-contained units \u2014 you submit a script, it runs on the best available cloud, and the output lands in a storage bucket.</p> <p>Skyward is function-oriented. You decorate a Python function with <code>@sky.compute</code> and dispatch it with <code>&gt;&gt;</code> or <code>@</code>. Your local Python process orchestrates everything \u2014 functions are transparently executed remotely and return results directly to your code. There's no YAML, no job submission, no separate storage layer for results. The programming model is a function call that happens to execute on a remote GPU. Skyward excels at interactive development where you iterate quickly and want remote compute to feel like a local operation.</p> <p>The choice depends on your workflow. If your workload is \"run this script end-to-end and store the results,\" SkyPilot is the right tool. If you want to call a function on a remote GPU and get the result back in a variable \u2014 composing remote computation with local logic, running experiments interactively, iterating on training code \u2014 Skyward is the right tool.</p>"},{"location":"sky-computing/#further-reading","title":"Further reading","text":"<ul> <li>The Sky Above The Clouds \u2014 The original Berkeley paper (2022)</li> <li>SkyPilot \u2014 Berkeley's reference implementation</li> <li>Sky Computing Lab \u2014 Research lab at UC Berkeley</li> <li>Core Concepts \u2014 Skyward's programming model and ephemeral compute</li> <li>Getting Started \u2014 Installation and first steps</li> <li>Providers \u2014 AWS, RunPod, VastAI, Verda, and Container configuration</li> </ul>"},{"location":"volumes/","title":"Volumes","text":"<p>Cloud instances are ephemeral \u2014 when the pool exits, the machines are gone. But data isn't ephemeral. Training datasets, model checkpoints, experiment artifacts \u2014 these need to outlive the compute that produced or consumed them. Volumes bridge this gap: they mount cloud storage (S3, GCS, or any S3-compatible API) as a local filesystem on every worker, so your <code>@sky.compute</code> functions read and write to familiar paths like <code>/data</code> or <code>/checkpoints</code> while the actual bytes live in a durable object store.</p> <p>The key idea is that storage and compute have different lifecycles. A dataset in S3 exists before the pool starts and persists after it's torn down. Checkpoints written during training survive instance preemption. Multiple pools \u2014 even across different providers \u2014 can mount the same bucket. Volumes make this separation explicit: you declare what storage you need, and Skyward mounts it before your code runs.</p>"},{"location":"volumes/#the-volume-dataclass","title":"The Volume dataclass","text":"<p>A <code>Volume</code> is a frozen dataclass with four fields:</p> <pre><code>sky.Volume(\n    bucket=\"my-datasets\",       # S3 bucket (or GCS bucket, or RunPod volume ID)\n    mount=\"/data\",              # where it appears on the worker\n    prefix=\"imagenet/train/\",   # subfolder within the bucket (optional)\n    read_only=True,             # default \u2014 prevents accidental writes\n)\n</code></pre> <p><code>bucket</code> is the storage identifier \u2014 an S3 bucket name on AWS, a GCS bucket name on GCP, or a network volume ID on RunPod. <code>mount</code> is the absolute path where the volume appears on workers. <code>prefix</code> scopes the mount to a subfolder within the bucket, so you don't expose the entire bucket when you only need one directory. <code>read_only</code> defaults to <code>True</code> because most volumes are input data \u2014 you opt into writes explicitly.</p> <p>Validation is immediate: mount paths must be absolute, and system paths (<code>/</code>, <code>/root</code>, <code>/tmp</code>, <code>/opt</code>) are rejected at construction time.</p>"},{"location":"volumes/#using-volumes","title":"Using volumes","text":"<p>Pass volumes to <code>ComputePool</code> as a list. Inside <code>@sky.compute</code> functions, the mount paths are regular directories:</p> <pre><code>import skyward as sky\n\n@sky.compute\ndef train(data_dir: str, checkpoint_dir: str) -&gt; float:\n    dataset = load(data_dir)\n    model = fit(dataset)\n    torch.save(model, f\"{checkpoint_dir}/model.pt\")\n    return model.accuracy\n\nwith sky.ComputePool(\n    provider=sky.AWS(instance_profile_arn=\"auto\"),\n    nodes=4,\n    volumes=[\n        sky.Volume(bucket=\"my-datasets\", mount=\"/data\", read_only=True),\n        sky.Volume(bucket=\"my-experiments\", mount=\"/checkpoints\", read_only=False),\n    ],\n) as pool:\n    accuracy = train(\"/data\", \"/checkpoints\") &gt;&gt; pool\n</code></pre> <p>The function doesn't know it's reading from S3 or writing to S3. It sees <code>/data</code> and <code>/checkpoints</code> as local directories. This means existing code \u2014 scripts that read from disk, libraries that expect file paths, frameworks that save checkpoints to a directory \u2014 works without modification.</p>"},{"location":"volumes/#how-it-works","title":"How it works","text":"<p>Under the hood, Skyward uses s3fs-fuse to mount S3-compatible buckets as FUSE filesystems. The mounting happens during the bootstrap phase, after system packages and Python dependencies are installed but before the worker starts accepting tasks.</p> <p>The process has three steps:</p> <ol> <li> <p>Endpoint resolution. The pool actor asks the provider for its S3-compatible endpoint and credentials. AWS returns the regional S3 endpoint with IAM role authentication (no explicit credentials). GCP returns the GCS S3-compatible endpoint with HMAC keys. RunPod returns its datacenter-specific S3 API endpoint with the API key as credentials.</p> </li> <li> <p>Bucket mounting. Each unique bucket is mounted once at <code>/mnt/s3fs/&lt;bucket&gt;</code> via s3fs. If multiple volumes reference the same bucket, Skyward deduplicates \u2014 one FUSE mount serves all of them. If any volume on a bucket needs writes, the entire bucket is mounted read-write.</p> </li> <li> <p>Symlink creation. Each volume's <code>mount</code> path is created as a symlink pointing to the appropriate location inside <code>/mnt/s3fs/&lt;bucket&gt;/&lt;prefix&gt;</code>. This is why you see <code>/data</code> instead of <code>/mnt/s3fs/my-datasets/imagenet/train/</code> \u2014 the implementation detail is hidden behind a clean path.</p> </li> </ol> <pre><code>graph LR\n    A[\"/data\"] --&gt;|symlink| B[\"/mnt/s3fs/my-datasets/imagenet/\"]\n    C[\"/checkpoints\"] --&gt;|symlink| D[\"/mnt/s3fs/my-experiments/run-042/\"]\n    B --&gt;|s3fs-fuse| E[\"s3://my-datasets\"]\n    D --&gt;|s3fs-fuse| F[\"s3://my-experiments\"]</code></pre>"},{"location":"volumes/#provider-support","title":"Provider support","text":"<p>Volumes work with any provider that implements the <code>Mountable</code> protocol \u2014 a single method that returns an S3-compatible endpoint with optional credentials.</p> Provider Endpoint Authentication AWS <code>s3.{region}.amazonaws.com</code> IAM role (no explicit credentials) GCP <code>storage.googleapis.com</code> HMAC keys (generated during <code>prepare()</code>) RunPod <code>s3api-{datacenter}.runpod.io</code> API key <p>On AWS, the cleanest setup is an instance profile with S3 permissions \u2014 pass <code>instance_profile_arn=\"auto\"</code> to <code>sky.AWS()</code> and no credentials are written to disk. On GCP, Skyward generates HMAC keys automatically from your service account during cluster preparation. On RunPod, the existing API key doubles as S3 credentials.</p> <p>Providers that don't implement <code>Mountable</code> (VastAI, Verda, Container) will fail fast with a clear error if you pass volumes.</p>"},{"location":"volumes/#deduplication","title":"Deduplication","text":"<p>When multiple volumes share a bucket, Skyward mounts it once:</p> <pre><code>volumes=[\n    sky.Volume(bucket=\"my-data\", mount=\"/train\", prefix=\"train/\", read_only=True),\n    sky.Volume(bucket=\"my-data\", mount=\"/val\", prefix=\"val/\", read_only=True),\n    sky.Volume(bucket=\"my-data\", mount=\"/output\", prefix=\"results/\", read_only=False),\n]\n</code></pre> <p>This creates one FUSE mount at <code>/mnt/s3fs/my-data</code> (read-write, because <code>/output</code> needs writes) and three symlinks: <code>/train \u2192 /mnt/s3fs/my-data/train/</code>, <code>/val \u2192 /mnt/s3fs/my-data/val/</code>, <code>/output \u2192 /mnt/s3fs/my-data/results/</code>. The mode is coerced upward: if any volume on a bucket is writable, the bucket mounts as read-write.</p>"},{"location":"volumes/#toml-configuration","title":"TOML configuration","text":"<p>Volumes can also be declared in <code>skyward.toml</code> or <code>~/.skyward/defaults.toml</code>:</p> <pre><code>[pool]\nprovider = \"aws\"\nnodes = 2\n\n[[pool.volumes]]\nbucket = \"my-datasets\"\nmount = \"/data\"\nprefix = \"imagenet/\"\nread_only = true\n\n[[pool.volumes]]\nbucket = \"my-experiments\"\nmount = \"/checkpoints\"\nread_only = false\n</code></pre> <p>This is equivalent to passing the same <code>Volume</code> objects in Python. TOML configuration is useful for separating infrastructure concerns from code \u2014 the same script can mount different buckets in different environments by swapping the config file.</p>"},{"location":"volumes/#next-steps","title":"Next steps","text":"<ul> <li>S3 Volumes Guide \u2014 Step-by-step walkthrough with a working example</li> <li>Providers \u2014 Provider-specific configuration and authentication</li> <li>Core Concepts \u2014 Image, bootstrap, and the pool lifecycle</li> </ul>"},{"location":"why-asyncio/","title":"Why asyncio","text":"<p>Skyward orchestrates cloud machines \u2014 it provisions instances, installs dependencies, opens SSH tunnels, starts workers, routes tasks, and tears everything down when the job is done. But what does this orchestration actually do on your laptop?</p> <p>The answer is networking. HTTP requests to cloud APIs. SSH connections to remote machines. TCP tunnels carrying actor messages. Polling loops waiting for instances to boot. The client never trains a model, never processes a dataset, never runs a computation. It coordinates. And the nature of that coordination \u2014 many concurrent I/O operations, zero CPU-bound work \u2014 is what makes asyncio the right foundation.</p>"},{"location":"why-asyncio/#blocking-vs-non-blocking","title":"Blocking vs non-blocking","text":"<p>When your code makes a network call \u2014 say, asking AWS to launch an instance \u2014 the response takes time. The machine needs to be allocated, the hypervisor needs to start it, the OS needs to boot. That might take 30 seconds. With a blocking call, the thread that made the request sits idle for those 30 seconds, doing nothing, unable to handle anything else.</p> <pre><code># Blocking \u2014 this thread is frozen until AWS responds\ninstance = ec2.run_instances(...)  # 30 seconds of waiting\n</code></pre> <p>A non-blocking call lets the thread move on to other work while the response is in transit. When the response arrives, the thread picks it back up. The distinction is irrelevant if you only have one thing to do \u2014 but when you're launching 8 instances, opening 8 SSH connections, and polling 8 cloud APIs simultaneously, the difference is fundamental.</p>"},{"location":"why-asyncio/#threads-vs-async","title":"Threads vs async","text":"<p>Both threads and asyncio solve the same problem: doing multiple things concurrently. They differ in how.</p> <p>Threads are managed by the operating system. Each thread has its own stack (typically ~8MB of memory), and the OS decides when to switch between them. This means threads work automatically \u2014 you don't need to think about yielding control \u2014 but the OS context switch is expensive (microseconds per switch, kernel involvement), and each thread consumes real memory. A hundred threads might use 800MB of stack space alone. Python's GIL adds another constraint: only one thread can execute Python bytecode at a time, so threads don't help with CPU-bound work anyway \u2014 they only help when threads are waiting, not computing.</p> <p>Asyncio uses a single thread running an event loop. Concurrent tasks are coroutines \u2014 functions that explicitly yield control at <code>await</code> points. There's no OS involvement in switching between tasks; the event loop simply runs the next ready coroutine when the current one awaits. A coroutine's overhead is a few hundred bytes (no dedicated stack), and switching between coroutines costs nanoseconds, not microseconds. Ten thousand concurrent coroutines are cheap. The trade-off is that you must write <code>async/await</code> code \u2014 the concurrency is cooperative, not preemptive.</p> <p>For CPU-bound work \u2014 number crunching, data transformation, model training \u2014 neither threads nor asyncio help much in Python (that's what processes are for). But for I/O-bound work with many concurrent operations \u2014 which is exactly what an orchestration client does \u2014 asyncio is strictly more efficient: lower memory, faster switching, no OS overhead.</p>"},{"location":"why-asyncio/#what-the-client-actually-does","title":"What the client actually does","text":"<p>Here is every category of work the Skyward client performs on your machine. All of it is I/O.</p>"},{"location":"why-asyncio/#cloud-api-calls","title":"Cloud API calls","text":"<p>Every provider interaction is an HTTP request. Querying available instance types, checking spot pricing, launching instances, polling their status, terminating them, tearing down infrastructure \u2014 these are all REST or GraphQL calls over HTTPS:</p> <ul> <li>AWS \u2014 EC2 API via <code>aioboto3</code> (async). Fleet creation, instance polling, spot capacity queries.</li> <li>GCP \u2014 Compute Engine API. Instance templates, bulk insert, firewall rules, machine type lookups.</li> <li>RunPod \u2014 GraphQL API for pod deployment, GPU type listing, SSH key management.</li> <li>VastAI \u2014 HTTP marketplace API for offer search, instance creation, status polling.</li> <li>Verda \u2014 OAuth2-authenticated REST API with automatic token refresh.</li> </ul> <p>During the offer selection phase alone, Skyward may query multiple providers in parallel to compare pricing and availability \u2014 all concurrent HTTP requests.</p>"},{"location":"why-asyncio/#ssh","title":"SSH","text":"<p>Once instances are running, the client opens SSH connections via <code>asyncssh</code>:</p> <ul> <li>Connection establishment with automatic retry (instances take time to accept SSH after boot).</li> <li>Remote command execution \u2014 transferring and running the bootstrap script, monitoring its output line by line.</li> <li>File transfer \u2014 syncing local code directories (the <code>includes</code> in your <code>Image</code>) to the remote machine.</li> </ul> <p>Each node in the pool gets its own persistent SSH connection. With 8 nodes, that's 8 concurrent SSH sessions, each streaming bootstrap output in real time.</p>"},{"location":"why-asyncio/#tcp-tunnels","title":"TCP tunnels","text":"<p>Each SSH connection also establishes a local port forward \u2014 a TCP tunnel from a random local port to the remote machine's port 25520, where the Casty worker actor system listens. This tunnel stays open for the lifetime of the pool, carrying all task payloads and results as actor messages.</p>"},{"location":"why-asyncio/#actor-messaging","title":"Actor messaging","text":"<p>Task dispatch \u2014 the <code>&gt;&gt;</code>, <code>@</code>, <code>&amp;</code> operators \u2014 translates to actor messages sent over these TCP tunnels. When you write <code>train(10) &gt;&gt; pool</code>, the function and arguments are serialized (cloudpickle + lz4), sent as a Casty message through the SSH tunnel to the remote worker, executed there, and the result flows back through the same path. The client's role is purely routing: serialize, send, wait for response, deserialize.</p>"},{"location":"why-asyncio/#whats-not-here","title":"What's not here","text":"<p>Computation. The client never executes your <code>@sky.compute</code> functions \u2014 that happens on the remote workers. The only CPU work on your laptop is serialization (cloudpickle + lz4 compression), which takes microseconds per task. Everything else is waiting for network responses.</p>"},{"location":"why-asyncio/#why-this-matters-at-scale","title":"Why this matters at scale","text":"<p>Consider a pool with 100 nodes. The client simultaneously:</p> <ul> <li>Maintains 100 SSH connections (each with keepalive heartbeats)</li> <li>Runs 100 TCP tunnels (port forwards carrying actor messages)</li> <li>Polls 100 cloud API endpoints during provisioning</li> <li>Routes tasks to 100 workers through the actor hierarchy</li> <li>Streams bootstrap output from 100 instances in parallel</li> </ul> <p>With threads, each of these concurrent activities needs its own thread \u2014 at minimum 100 threads just for SSH, plus more for API polling and task routing. That's several hundred threads, each consuming ~8MB of stack space, each requiring OS context switches. With asyncio, all of it runs on a single event loop in a single thread. A hundred concurrent coroutines use a fraction of the memory of a hundred threads, and switching between them costs nanoseconds instead of microseconds \u2014 no kernel involvement, no context save/restore overhead.</p> <p>The Casty actor framework maps naturally onto this: each actor is a coroutine, message passing is <code>await</code>-based, and the <code>pipe_to_self</code> pattern bridges async operations (like SSH commands or HTTP calls) into actor messages without blocking the event loop.</p> <p>This is also why the actor model fits the orchestration layer. Each node progresses through its own state machine \u2014 <code>idle \u2192 waiting \u2192 active</code> \u2014 at its own pace. Node 0 might finish bootstrapping while node 73 is still booting. With asyncio + actors, each node's lifecycle is an independent coroutine responding to messages, and the event loop interleaves them automatically. No thread coordination, no locks, no shared mutable state.</p>"},{"location":"why-asyncio/#the-synchronous-api","title":"The synchronous API","text":"<p>Despite being fully asynchronous internally, Skyward exposes a synchronous API. You write normal, blocking Python:</p> <pre><code>with sky.ComputePool(provider=sky.AWS(), nodes=4) as pool:\n    result = train(10) &gt;&gt; pool  # blocks until result is ready\n</code></pre> <p>The bridge is simple. When you enter the <code>ComputePool</code> context manager, Skyward starts a background daemon thread running an asyncio event loop. Every public method \u2014 <code>&gt;&gt;</code>, <code>@</code>, <code>&gt;</code>, <code>gather</code> \u2014 calls <code>asyncio.run_coroutine_threadsafe()</code> to submit work to that event loop and blocks the calling thread until the result is ready.</p> <p>This gives you the best of both worlds: the efficiency of async orchestration underneath, with the simplicity of synchronous code on top. You don't need to write <code>async/await</code> in your application code. You don't need to manage an event loop. The concurrency is an implementation detail of the runtime \u2014 invisible unless you want to understand it.</p>"},{"location":"why-asyncio/#further-reading","title":"Further reading","text":"<ul> <li>Architecture \u2014 The actor hierarchy and how the cluster forms</li> <li>Core Concepts \u2014 Pool lifecycle, operators, and the orchestration model</li> <li>Casty Documentation \u2014 The actor framework powering the runtime</li> </ul>"},{"location":"guides/broadcast/","title":"Broadcast","text":"<p>Some operations need to run on every node in the pool \u2014 not just one. Distributed training setups, data-parallel processing, cache warming, model loading. The <code>@</code> operator handles this: it sends the same computation to all nodes and returns a list with one result per node. Combined with <code>shard()</code>, each node can operate on its own partition of the data while receiving the same function and arguments.</p>"},{"location":"guides/broadcast/#processing-data-on-every-node","title":"Processing data on every node","text":"<p>Define a compute function that uses <code>shard()</code> to get its portion of the data:</p> <pre><code>@sky.compute\ndef process_partition(data: list[int]) -&gt; dict:\n    \"\"\"Process this node's shard of the data.\"\"\"\n    info = sky.instance_info()\n    assert info is not None\n    local_data = sky.shard(data)\n\n    return {\n        \"node\": info.node,\n        \"size\": len(local_data),\n        \"sum\": sum(local_data),\n    }\n</code></pre> <p>The function receives the full dataset as its argument, but <code>shard()</code> returns only the portion assigned to the current node. This means the serialization cost is paid once (the full dataset is sent to every node), but each node processes a different slice. The sharding is automatic \u2014 <code>shard()</code> reads the node's position from <code>instance_info()</code> and uses modulo striding (<code>indices[node::total_nodes]</code>) to divide the data evenly.</p> <p><code>instance_info()</code> returns an <code>InstanceInfo</code> with the node's index, the total cluster size, whether it's the head node, and the addresses of all peers. This is how the function knows where it sits in the cluster without any explicit configuration.</p>"},{"location":"guides/broadcast/#broadcasting-with","title":"Broadcasting with <code>@</code>","text":"<p>Use <code>@</code> instead of <code>&gt;&gt;</code> to run the function on every node:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=4,\n) as pool:\n    data = list(range(1000))\n    results = process_partition(data) @ pool\n</code></pre> <p>Where <code>&gt;&gt;</code> sends work to a single node (round-robin), <code>@</code> sends it to all nodes. The pool serializes the function and arguments once, dispatches a copy to each worker, waits for every node to complete, and collects the results. The return type is <code>list[T]</code> \u2014 one entry per node, in node order.</p> <p>This is the foundation for distributed patterns in Skyward. When every node runs the same function but <code>shard()</code> gives each one different data, you get data parallelism without any explicit coordination. The function body is identical across nodes \u2014 the differentiation happens at runtime based on each node's position in the cluster.</p>"},{"location":"guides/broadcast/#aggregating-results","title":"Aggregating results","text":"<p>Each node returns a partial result. Since broadcast returns a list, you combine them locally on the client side:</p> <pre><code>total = 0\nfor r in results:\n    print(f\"  Node {r['node']}: {r['size']} items, sum={r['sum']}\")\n    total += r[\"sum\"]\n\nprint(f\"\\nTotal: {total} (expected: {sum(data)})\")\n</code></pre> <p>This map-reduce pattern \u2014 broadcast a function, shard the data inside, aggregate the results \u2014 is the simplest form of distributed computation in Skyward. More complex patterns (distributed training with gradient synchronization, for example) build on the same foundation but use framework plugins to handle the inter-node communication.</p>"},{"location":"guides/broadcast/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/03_broadcast.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>@</code> operator broadcasts a function to every node in the pool, returning <code>list[T]</code>.</li> <li><code>shard()</code> divides data for the current node \u2014 each node processes its own slice of the full dataset.</li> <li><code>instance_info()</code> provides the node's identity: index, total count, head status, peer addresses.</li> <li>Map-reduce pattern \u2014 broadcast + shard inside + aggregate locally \u2014 is the foundation for distributed computation.</li> </ul>"},{"location":"guides/cuml-acceleration/","title":"GPU-accelerated scikit-learn with cuML","text":"<p>scikit-learn runs on CPU. For large datasets, training algorithms like RandomForest or KNN becomes a bottleneck \u2014 minutes or hours spent on cross-validation and hyperparameter search. NVIDIA cuML provides GPU-backed implementations of popular sklearn estimators with the same API. Swap the import, and the same code runs on GPU with speedups of 50x to 175x.</p> <p>Skyward makes this practical even if you don't have a local GPU. Provision a GPU instance on the cloud, send your code there with <code>@sky.compute</code>, and cuML handles the rest. The <code>cuml</code> plugin installs cuML and configures the RAPIDS package indexes automatically.</p>"},{"location":"guides/cuml-acceleration/#the-dataset","title":"The dataset","text":"<p>A 20,000-sample subset of MNIST, downloaded directly on the remote worker \u2014 no need to serialize and ship 784-dimensional arrays over the wire:</p> <pre><code>def load_mnist(n_samples: int):  # noqa: N806\n    \"\"\"Load a subset of MNIST on the remote worker.\"\"\"\n    import numpy as np\n    from sklearn.datasets import fetch_openml\n\n    X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)  # noqa: N806\n    X = (X[:n_samples] / 255.0).astype(np.float32)  # noqa: N806\n    y = y[:n_samples].astype(np.int32)\n    return X, y\n</code></pre> <p><code>load_mnist</code> is a plain function (not <code>@sky.compute</code>) that the GPU task calls. Since it's defined at module level, cloudpickle captures it alongside the decorated functions. The data is fetched from OpenML on the remote machine.</p>"},{"location":"guides/cuml-acceleration/#gpu-version-with-cuml","title":"GPU version with cuML","text":"<p>The GPU version uses standard scikit-learn imports \u2014 cuML's zero-code-change acceleration intercepts sklearn calls and routes them to the GPU:</p> <pre><code>@sky.compute\ndef train_on_gpu(n_samples: int):\n    \"\"\"Train the same RandomForest, but with cuML GPU acceleration.\"\"\"\n    from time import perf_counter\n\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import cross_val_score\n\n    X, y = load_mnist(n_samples)  # noqa: N806\n\n    clf = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n    start = perf_counter()\n    scores = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\n    elapsed = perf_counter() - start\n\n    return {\"accuracy\": scores.mean(), \"time\": elapsed}\n</code></pre> <p>All sklearn imports are inside the function because <code>@sky.compute</code> serializes the function and sends it to a remote worker. The worker needs to resolve imports in its own environment. The function returns both accuracy and wall-clock time so we can compare against a CPU baseline.</p>"},{"location":"guides/cuml-acceleration/#running-with-plugins","title":"Running with plugins","text":"<p>The <code>cuml</code> plugin handles installing <code>cuml-cu12</code> and configuring the RAPIDS pip indexes. Combined with the <code>sklearn</code> plugin, which adds <code>scikit-learn</code> and <code>joblib</code>:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=sky.accelerators.L4(),\n    nodes=1,\n    plugins=[\n        sky.plugins.cuml(),\n        sky.plugins.sklearn()\n    ],\n) as pool:\n</code></pre> <p>The plugins handle all dependency management \u2014 no need to manually specify pip packages or index URLs in the Image. The <code>cuml</code> plugin knows which RAPIDS indexes to configure for the CUDA 12 packages.</p>"},{"location":"guides/cuml-acceleration/#results","title":"Results","text":"<pre><code>    result = train_on_gpu(N_SAMPLES) &gt;&gt; pool\n\nprint(f\"accuracy: {result['accuracy']:.2%}, time: {result['time']:.1f}s\")\n</code></pre> <p>Expect accuracy to be roughly equivalent between CPU and GPU \u2014 cuML implements the same algorithms, not approximations. The wall-clock time is where the difference shows: cuML on an L4 can be significantly faster depending on the algorithm and dataset size.</p>"},{"location":"guides/cuml-acceleration/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/16_cuml_acceleration.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>plugins=[sky.plugins.cuml(), sky.plugins.sklearn()]</code> \u2014 cuML plugin installs RAPIDS packages and configures indexes; sklearn plugin adds scikit-learn.</li> <li>cuML estimators work with sklearn utilities \u2014 <code>cross_val_score</code>, <code>GridSearchCV</code>, and <code>Pipeline</code> all accept cuML estimators.</li> <li>Zero-code-change acceleration \u2014 cuML intercepts sklearn calls and routes them to the GPU transparently.</li> <li>Plugins handle dependencies \u2014 no manual pip packages or index URLs needed in the Image.</li> <li>Load data on the worker \u2014 avoid serializing large arrays; download datasets directly on the remote machine.</li> </ul>"},{"location":"guides/data-sharding/","title":"Data sharding","text":"<p>The most common pattern in distributed computing is sending the same function to every node but having each node operate on a different slice of the data. <code>sky.shard()</code> automates this: it reads the current node's position from <code>instance_info()</code> and returns only the portion of the data that belongs to this node. No manual index math, no configuration \u2014 just pass the full dataset and get back your shard.</p>"},{"location":"guides/data-sharding/#automatic-sharding","title":"Automatic sharding","text":"<p>Pass the full dataset to the compute function. Inside, call <code>shard()</code> to get this node's portion:</p> <pre><code>@sky.compute\ndef train_on_shard(full_x: list, full_y: list) -&gt; dict:\n    \"\"\"Train on this node's shard of the data.\"\"\"\n    import numpy as np\n\n    x, y = sky.shard(full_x, full_y, shuffle=True, seed=42)\n    info = sky.instance_info()\n    assert info is not None\n\n    x_arr = np.array(x)\n    return {\n        \"node\": info.node,\n        \"shard_size\": len(x),\n        \"mean\": float(x_arr.mean()),\n    }\n</code></pre> <p>The function receives the full dataset as arguments \u2014 <code>full_x</code> and <code>full_y</code> are the complete arrays. <code>shard()</code> divides them using modulo striding: with 4 nodes and 1000 samples, node 0 gets indices <code>[0, 4, 8, ...]</code>, node 1 gets <code>[1, 5, 9, ...]</code>, and so on. Each node ends up with ~250 samples, evenly distributed regardless of whether the total is divisible by the node count.</p> <p>The <code>shuffle=True</code> parameter randomizes the order before sharding, with a fixed <code>seed</code> ensuring all nodes agree on the same permutation. This is important for training: without shuffling, each node would get a contiguous block of the original data order, which can introduce bias if the data is sorted.</p>"},{"location":"guides/data-sharding/#sharding-multiple-arrays","title":"Sharding multiple arrays","text":"<p>When you pass multiple arrays to <code>shard()</code>, the same indices are selected from each one \u2014 so paired data stays consistent:</p> <pre><code>x, y = sky.shard(full_x, full_y, shuffle=True, seed=42)\n</code></pre> <p>This is critical for supervised learning: features and labels, inputs and targets, questions and answers. After sharding, <code>x[i]</code> still corresponds to <code>y[i]</code> because the same positions were selected from both arrays. You can pass any number of arrays to a single <code>shard()</code> call, and they'll all be split at the same indices.</p>"},{"location":"guides/data-sharding/#type-preservation","title":"Type preservation","text":"<p><code>shard()</code> returns the same type it receives. Lists produce lists, tuples produce tuples, NumPy arrays produce arrays, PyTorch tensors produce tensors:</p> <pre><code>@sky.compute\ndef show_shard_types() -&gt; dict:\n    \"\"\"Demonstrate that shard() preserves types.\"\"\"\n    import numpy as np\n\n    info = sky.instance_info()\n    assert info is not None\n\n    sharded_list = sky.shard(list(range(100)))\n    sharded_tuple = sky.shard(tuple(range(100)))\n    sharded_array = sky.shard(np.arange(100))\n\n    return {\n        \"node\": info.node,\n        \"list\": type(sharded_list).__name__,\n        \"tuple\": type(sharded_tuple).__name__,\n        \"array\": type(sharded_array).__name__,\n    }\n</code></pre> <p>This means you can shard a tensor and immediately pass it to a model without type conversions or wrapping. The sharding operation is transparent to downstream code \u2014 it doesn't know (or care) that it's working with a subset.</p>"},{"location":"guides/data-sharding/#equal-size-shards-with-drop_last","title":"Equal-size shards with <code>drop_last</code>","text":"<p>By default, striding can produce shards of slightly different sizes when the total isn't evenly divisible. If your training loop requires fixed batch dimensions (common with compiled models or certain padding strategies), use <code>drop_last=True</code>:</p> <pre><code>x, y = sky.shard(x_full, y_full, drop_last=True)\n</code></pre> <p>This switches from striding to contiguous blocks and discards leftover elements, guaranteeing every node gets exactly the same number of samples.</p>"},{"location":"guides/data-sharding/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/05_data_sharding.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>shard()</code> automatically partitions data for the current node using modulo striding.</li> <li>Multiple arrays sharded in a single call stay aligned \u2014 same indices selected from each.</li> <li><code>shuffle=True</code> + <code>seed</code> randomize the split deterministically, avoiding bias from data ordering.</li> <li>Type preservation \u2014 lists, tuples, arrays, and tensors all stay their original type after sharding.</li> <li><code>drop_last=True</code> guarantees equal-size shards by discarding leftover elements.</li> </ul>"},{"location":"guides/hello-skyward/","title":"Hello, Skyward!","text":"<p>This guide walks you through running your first function on a remote cloud instance. By the end, you'll understand the three core ideas in Skyward: compute functions, pools, and the <code>&gt;&gt;</code> operator \u2014 and what happens behind the scenes when you combine them.</p>"},{"location":"guides/hello-skyward/#the-compute-function","title":"The compute function","text":"<p>Any Python function can run on the cloud. The only change is adding the <code>@sky.compute</code> decorator:</p> <pre><code>@sky.compute\ndef add(a: int, b: int) -&gt; int:\n    \"\"\"Add two numbers on a remote instance.\"\"\"\n    return a + b\n\n\n@sky.compute\ndef process(data: list[int]) -&gt; dict:\n    \"\"\"Process data remotely and return statistics.\"\"\"\n    return {\n        \"count\": len(data),\n        \"sum\": sum(data),\n        \"mean\": sum(data) / len(data),\n    }\n</code></pre> <p>This decorator doesn't execute anything. Calling <code>add(2, 3)</code> no longer returns <code>5</code> \u2014 it returns a <code>PendingCompute[int]</code>, a frozen description of the computation. The arguments are captured, the function is recorded, but nothing runs. This is lazy computation: you're building a description of work, not performing it. The actual execution \u2014 serializing the function, sending it to a remote machine, running it there \u2014 doesn't happen until you dispatch the computation with an operator like <code>&gt;&gt;</code>.</p> <p>This design means <code>PendingCompute</code> is a value you can pass around, compose with other computations, or store for later. It's also what makes remote execution possible: because the computation is a data structure rather than a running process, it can be serialized with cloudpickle, sent over the network, and executed on a different machine.</p>"},{"location":"guides/hello-skyward/#the-pool","title":"The pool","text":"<p>A <code>ComputePool</code> is a context manager that provisions cloud infrastructure for the duration of your work:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n) as pool:\n    result = add(2, 3) &gt;&gt; pool\n    print(f\"2 + 3 = {result}\")\n\n    stats = process([1, 2, 3, 4, 5]) &gt;&gt; pool\n    print(f\"Stats: {stats}\")\n</code></pre> <p>When you enter the <code>with</code> block, Skyward asks the provider (AWS, in this case) to launch an instance, waits for it to boot, opens an SSH tunnel, installs dependencies via an idempotent bootstrap script, and starts a worker process. When you exit the block \u2014 whether normally or through an exception \u2014 the instance is terminated and all infrastructure is torn down.</p> <p>This is ephemeral compute: the machine exists only for the duration of your work. There are no instances to forget about, no environments that drift, no idle costs accumulating overnight. The pool's lifetime is the job's lifetime.</p>"},{"location":"guides/hello-skyward/#dispatching-with","title":"Dispatching with <code>&gt;&gt;</code>","text":"<p>The <code>&gt;&gt;</code> operator is what connects a lazy computation to a pool:</p> <pre><code>result = add(2, 3) &gt;&gt; pool\n</code></pre> <p>This single expression triggers the full execution pipeline. The pool serializes the function and its arguments using cloudpickle (compressed with zlib), sends the payload to the remote worker over the SSH tunnel, the worker deserializes and executes <code>add(2, 3)</code>, and the result \u2014 <code>5</code> \u2014 is serialized back and returned to your local process. From your perspective, it looks like a normal function call that happens to return from a remote machine.</p> <p>The generic type flows through the entire chain: <code>add(2, 3)</code> produces <code>PendingCompute[int]</code>, and <code>&gt;&gt; pool</code> returns <code>int</code>. Your type checker sees the correct types whether the function runs locally or on a cloud GPU.</p>"},{"location":"guides/hello-skyward/#local-execution","title":"Local execution","text":"<p>During development, you often want to test a compute function without provisioning any infrastructure. Every <code>@sky.compute</code> function exposes the original, unwrapped version via <code>.local</code>:</p> <pre><code>result = add.local(2, 3)  # executes immediately, returns 5\n</code></pre> <p>This bypasses the lazy computation entirely \u2014 no <code>PendingCompute</code>, no serialization, no pool required. It's useful for unit testing, debugging, and local profiling.</p>"},{"location":"guides/hello-skyward/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/01_hello_skyward.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>@sky.compute</code> transforms a function into a lazy <code>PendingCompute</code> \u2014 calling it captures the computation without executing it.</li> <li><code>ComputePool</code> provisions cloud instances on enter and tears them down on exit \u2014 ephemeral, scoped infrastructure.</li> <li><code>&gt;&gt;</code> dispatches a computation to the pool: serialize, send, execute remotely, return the result.</li> <li><code>.local</code> bypasses remote execution for testing and debugging.</li> </ul>"},{"location":"guides/huggingface-finetuning/","title":"HuggingFace fine-tuning","text":"<p>Fine-tuning a pre-trained transformer is one of the most common ML workflows: take a model from the HuggingFace Hub, adapt it to your task with a small labeled dataset, and evaluate the results. The bottleneck is usually hardware \u2014 fine-tuning even a small model like DistilBERT benefits significantly from a GPU, and larger models require one. Skyward lets you wrap the entire pipeline in a single <code>@sky.compute</code> function, provision a GPU instance, and run it remotely. Everything \u2014 model download, tokenization, training, evaluation \u2014 happens on the cloud instance.</p>"},{"location":"guides/huggingface-finetuning/#loading-model-and-tokenizer","title":"Loading model and tokenizer","text":"<p>Load a pre-trained model inside the compute function:</p> <pre><code>tokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=2,\n    id2label={0: \"negative\", 1: \"positive\"},\n    label2id={\"negative\": 0, \"positive\": 1},\n)\n</code></pre> <p><code>AutoModelForSequenceClassification.from_pretrained()</code> downloads the base model and adds a classification head. The download happens on the remote instance, which typically has faster internet than a laptop and avoids transferring multi-GB model weights over the SSH tunnel. The <code>id2label</code> and <code>label2id</code> mappings configure the model for binary sentiment classification.</p>"},{"location":"guides/huggingface-finetuning/#preparing-the-dataset","title":"Preparing the dataset","text":"<p>Load IMDB, tokenize, and prepare for training \u2014 all on the remote instance:</p> <pre><code>dataset = load_dataset(\"imdb\")\ntrain_ds = dataset[\"train\"].select(range(max_samples))\ntest_ds = dataset[\"test\"].select(range(max_samples // 4))\n\ndef tokenize(examples):\n    return tokenizer(examples[\"text\"], truncation=True, max_length=256)\n\ntrain_ds = train_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\ntest_ds = test_ds.map(tokenize, batched=True, remove_columns=[\"text\"])\n</code></pre> <p><code>load_dataset(\"imdb\")</code> downloads the dataset on the worker. The <code>select(range(max_samples))</code> call limits the dataset size for faster iteration during development \u2014 remove it for a full fine-tuning run. Tokenization runs remotely too, so you don't need <code>transformers</code> or <code>datasets</code> installed locally.</p> <p>This is one of the key advantages of remote execution: heavy data processing and model operations happen on a machine with the right hardware and fast network, while your local machine just dispatches the work and collects results.</p>"},{"location":"guides/huggingface-finetuning/#training-with-the-trainer-api","title":"Training with the Trainer API","text":"<p>Configure training arguments and launch the Trainer:</p> <pre><code>trainer = Trainer(\n    model=model,\n    args=TrainingArguments(\n        output_dir=\"/tmp/finetuned\",\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        eval_strategy=\"epoch\",\n        save_strategy=\"no\",\n        fp16=torch.cuda.is_available(),\n        report_to=\"none\",\n    ),\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    data_collator=DataCollatorWithPadding(tokenizer),\n    compute_metrics=compute_metrics,\n)\n\ntrain_result = trainer.train()\neval_result = trainer.evaluate()\n</code></pre> <p>The <code>Trainer</code> manages the training loop, evaluation, gradient accumulation, and mixed-precision (fp16) when a GPU is available. <code>eval_strategy=\"epoch\"</code> runs evaluation after each epoch. <code>save_strategy=\"no\"</code> disables checkpointing \u2014 since the instance is ephemeral, saved checkpoints would be lost on teardown. For production fine-tuning, you'd save checkpoints to a persistent location (S3, HuggingFace Hub, or a mounted volume).</p> <p>The function returns a summary dict with training loss, evaluation accuracy, and runtime. This is the result that comes back through the SSH tunnel to your local process.</p>"},{"location":"guides/huggingface-finetuning/#dispatching-to-the-cloud","title":"Dispatching to the cloud","text":"<p>The full example dispatches the fine-tuning job to an A100 instance:</p> <pre><code>result = finetune(\n    model_name=\"distilbert-base-uncased\",\n    epochs=2,\n    batch_size=16,\n) &gt;&gt; pool\n</code></pre> <p>The HuggingFace Trainer handles device placement and mixed-precision internally. Skyward provisions the GPU instance, runs the function, and returns the result. The <code>Image(pip=[...])</code> in the pool configuration installs the required dependencies on the worker.</p>"},{"location":"guides/huggingface-finetuning/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/08_huggingface_finetuning.py\n</code></pre> <p>What you learned:</p> <ul> <li>Everything runs remotely \u2014 model download, tokenization, training, evaluation all happen on the cloud GPU.</li> <li>No Skyward-specific APIs inside the function \u2014 standard HuggingFace <code>Trainer</code>, <code>AutoModel</code>, <code>load_dataset</code>.</li> <li>Remote imports \u2014 <code>transformers</code> and <code>datasets</code> only need to be installed on the worker (via the Image's <code>pip</code> field), not locally.</li> <li>Ephemeral instances \u2014 checkpoints are lost on teardown; save to persistent storage for production runs.</li> <li>Single-node fine-tuning \u2014 the HuggingFace Trainer manages device placement internally; add <code>sky.plugins.torch()</code> for multi-node.</li> </ul>"},{"location":"guides/joblib-concurrency/","title":"Joblib concurrency","text":"<p>joblib's <code>Parallel</code> is the standard way to parallelize work in Python \u2014 scikit-learn, NLTK, and many other libraries use it internally. By default, it runs tasks across local threads or processes. Skyward's <code>joblib</code> plugin replaces the backend with a distributed one: <code>n_jobs=-1</code> sends tasks to cloud instances instead of local cores. No code changes needed beyond the pool configuration \u2014 existing <code>Parallel(n_jobs=-1)(delayed(fn)(x) for x in data)</code> patterns work as-is.</p>"},{"location":"guides/joblib-concurrency/#defining-tasks","title":"Defining tasks","text":"<p>Any regular Python function works with joblib \u2014 the plugin handles serialization and dispatch internally:</p> <pre><code>def slow_task(x):\n    \"\"\"A slow task that takes 5 seconds.\"\"\"\n    sleep(5)\n    return x * 2\n</code></pre> <p>joblib handles its own serialization. The <code>joblib</code> plugin intercepts joblib's task batches, wraps them internally, and dispatches them to the cluster.</p>"},{"location":"guides/joblib-concurrency/#distributed-execution-with-the-joblib-plugin","title":"Distributed execution with the Joblib plugin","text":"<p>Wrap your <code>Parallel</code> call inside a <code>ComputePool</code> with the <code>joblib</code> plugin:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=10,\n    worker=sky.Worker(concurrency=10),\n    plugins=[sky.plugins.joblib()],\n) as pool:\n    t0 = perf_counter()\n\n    results = Parallel(n_jobs=-1)(\n        delayed(slow_task)(i) for i in range(2000)\n    )\n</code></pre> <p>When you enter the pool block, Skyward provisions the instances and the <code>joblib</code> plugin registers a custom joblib backend. Every <code>Parallel(n_jobs=-1)</code> call inside the block distributes tasks across the cluster. The <code>worker</code> parameter accepts a <code>Worker</code> dataclass that controls per-node execution \u2014 <code>Worker(concurrency=10)</code> means each node runs 10 tasks simultaneously. With 10 nodes and <code>concurrency=10</code>, you get 100 effective workers.</p> <p>When you exit the block, the instances are terminated and the default joblib backend is restored.</p>"},{"location":"guides/joblib-concurrency/#measuring-throughput","title":"Measuring throughput","text":"<p>Compare actual time against the theoretical ideal:</p> <pre><code>elapsed = perf_counter() - t0\n\nprint(\"Tasks: 2000 | Nodes: 10 | Concurrency: 10\")\nprint(\"Effective workers: 100\")\nprint(f\"Total time: {elapsed:.2f}s\")\nprint(f\"Throughput: {2000 / elapsed:.2f} tasks/s\")\nprint(f\"Ideal time: {2000 / 100 * 5:.0f}s\")\nprint(f\"Efficiency: {(2000 / 100 * 5) / elapsed * 100:.1f}%\")\n</code></pre> <p>With 2000 tasks, 100 effective workers, and 5 seconds per task, the ideal time is <code>2000 / 100 * 5 = 100s</code>. Efficiency measures how close you get to that ideal \u2014 the ratio of ideal time to actual time.</p>"},{"location":"guides/joblib-concurrency/#real-world-results","title":"Real-world results","text":"<p>Running with 10 <code>t4g.micro</code> instances (1GB RAM, 2 vCPUs) on AWS:</p> <pre><code>Tasks: 2000 | Nodes: 10 | Concurrency: 10\nEffective workers: 100\nTotal time: 102.57s\nThroughput: 19.50 tasks/s\nIdeal time (2000 tasks / 100 workers * 5s): 100s\nEfficiency: 97.5%\n</code></pre> <p>97.5% efficiency \u2014 nearly perfect linear scaling. The overhead comes from serialization, network round-trips, and scheduling. Skyward communicates with each worker via an SSH tunnel to a lightweight Casty actor system running on the node, using raw TCP over asyncio. The minimal protocol overhead \u2014 no HTTP, no REST, no message broker \u2014 is what makes near-ideal throughput possible even on the smallest instances.</p> <p>This also illustrates the cost model: 10 <code>t4g.micro</code> instances at ~$0.008/hour each costs $0.08/hour total. The same 2000 tasks running locally at 1 task/second would take ~2.8 hours. The cluster finishes in under 2 minutes for a fraction of a cent.</p>"},{"location":"guides/joblib-concurrency/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/10_joblib_concurrency.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>plugins=[sky.plugins.joblib()]</code> replaces joblib's backend with a distributed one \u2014 <code>n_jobs=-1</code> uses all cloud workers.</li> <li>Plain functions \u2014 joblib handles serialization; the plugin wraps batches internally.</li> <li>Effective workers = nodes x worker concurrency \u2014 both parameters multiply throughput.</li> <li>Near-linear scaling \u2014 97.5% efficiency with minimal protocol overhead (SSH + Casty actors, raw TCP).</li> <li>Standard joblib API \u2014 <code>Parallel</code>, <code>delayed</code> work unchanged inside the context manager.</li> </ul>"},{"location":"guides/keras-training/","title":"Keras training","text":"<p>Keras 3 is backend-agnostic \u2014 the same model code runs on JAX, TensorFlow, or PyTorch. Skyward's <code>keras</code> plugin configures the backend on the remote worker before your function runs, and <code>shard()</code> handles data partitioning for multi-node training. This guide walks through training an MLP on MNIST across multiple cloud GPUs using Keras with JAX as the backend.</p>"},{"location":"guides/keras-training/#the-keras-plugin","title":"The <code>keras</code> plugin","text":"<p>Add <code>sky.plugins.keras(backend=\"jax\")</code> to your pool's plugins. When using the JAX backend, also include <code>sky.plugins.jax()</code> for distributed initialization:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"T4\",\n    nodes=2,\n    plugins=[sky.plugins.jax(), sky.plugins.keras(backend=\"jax\")],\n) as pool:\n</code></pre> <p>The <code>backend</code> parameter sets <code>KERAS_BACKEND</code> on the remote worker before Keras is imported. This is critical \u2014 Keras reads the backend at import time, so the environment variable must be set first.</p> <p>The function itself just uses <code>@sky.compute</code> \u2014 the backend and distributed setup are handled by the plugins:</p> <pre><code>@sky.compute\ndef train_mnist(epochs: int = 5, batch_size: int = 128) -&gt; dict:\n    \"\"\"Train an MLP on this node's shard of MNIST.\"\"\"\n</code></pre>"},{"location":"guides/keras-training/#loading-and-sharding-data","title":"Loading and sharding data","text":"<p>Load the full dataset inside the function, then use <code>shard()</code> to get this node's portion:</p> <pre><code>(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\nx_train = x_train.reshape(-1, 784).astype(\"float32\") / 255.0\nx_test = x_test.reshape(-1, 784).astype(\"float32\") / 255.0\n\nx_train, y_train = sky.shard(x_train, y_train, shuffle=True, seed=42)\n</code></pre> <p><code>keras.datasets.mnist.load_data()</code> downloads the dataset on the remote worker. <code>shard()</code> then splits the training data so each node trains on a different subset \u2014 with 2 nodes, each gets half. The <code>shuffle=True</code> and <code>seed=42</code> parameters ensure a deterministic, randomized split so both nodes agree on who gets which samples.</p> <p>Note that sharding happens inside the function, after the data is loaded. The full dataset exists on every node (each one downloads it independently), and sharding selects each node's portion based on <code>instance_info()</code>. This is simpler than pre-splitting and distributing data from the client.</p>"},{"location":"guides/keras-training/#model-definition","title":"Model definition","text":"<p>Define a standard Keras model \u2014 nothing Skyward-specific here:</p> <pre><code>model = keras.Sequential([\n    layers.Dense(256, activation=\"relu\", input_shape=(784,)),\n    layers.Dropout(0.2),\n    layers.Dense(128, activation=\"relu\"),\n    layers.Dropout(0.2),\n    layers.Dense(10, activation=\"softmax\"),\n])\n</code></pre> <p>This is the same Keras <code>Sequential</code> API you'd use locally. The model runs on whatever backend the plugin configured \u2014 JAX in this case. If you switch to <code>backend=\"torch\"</code>, the same model definition produces a PyTorch-backed model.</p>"},{"location":"guides/keras-training/#training","title":"Training","text":"<p>Compile and fit as usual:</p> <pre><code>model.compile(\n    optimizer=\"adam\",\n    loss=\"sparse_categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\n\nhistory = model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1)\n_, test_acc = model.evaluate(x_test, y_test, verbose=0)\n</code></pre> <p><code>model.fit()</code> runs on the remote GPU. Each node trains independently on its shard of the data, so training time scales inversely with the number of nodes (minus overhead). The evaluation runs on the full test set \u2014 each node evaluates independently and reports its own accuracy.</p> <p>For synchronized multi-node training with gradient averaging (similar to PyTorch DDP), Keras provides distribution strategies. The <code>keras</code> plugin can configure these automatically when running with JAX on multiple nodes. For data-parallel training where each node trains independently on its shard (as in this example), no extra configuration is needed.</p>"},{"location":"guides/keras-training/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/07_keras_training.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>plugins=[sky.plugins.jax(), sky.plugins.keras(backend=\"jax\")]</code> sets the Keras backend and configures JAX distributed on the remote worker.</li> <li><code>shard()</code> splits training data across nodes \u2014 each node trains on its own subset.</li> <li>Standard Keras API \u2014 <code>Sequential</code>, <code>model.compile()</code>, <code>model.fit()</code> work unchanged.</li> <li>Backend-agnostic \u2014 switch between JAX, TensorFlow, and PyTorch with one parameter.</li> <li>Data loads on the worker \u2014 no need to transfer datasets from your local machine.</li> </ul>"},{"location":"guides/local-containers/","title":"Local containers","text":"<p>When developing with Skyward, the normal feedback loop involves provisioning cloud instances, waiting for bootstrap, running your function, and tearing everything down. This works, but it's slow and costs money \u2014 not ideal when you're iterating on a function's logic, debugging serialization issues, or validating that your <code>Image</code> installs the right dependencies.</p> <p>The <code>Container</code> provider solves this by running the exact same orchestration locally. Instead of launching EC2 instances or GPU pods, it starts Docker containers on your machine. The pool lifecycle is identical: SSH tunnels, bootstrap scripts, worker processes, cluster formation. The only difference is that \"the cloud\" is your laptop.</p> <p>This means you can develop and test your <code>@sky.compute</code> functions locally, then switch to <code>sky.AWS()</code> or <code>sky.RunPod()</code> when you're ready for real hardware \u2014 with confidence that the behavior will be the same.</p>"},{"location":"guides/local-containers/#prerequisites","title":"Prerequisites","text":"<p>You need a container runtime installed and running:</p> <ul> <li>Docker (default): <code>docker</code> CLI available in PATH</li> <li>Podman: pass <code>binary=\"podman\"</code> to <code>sky.Container()</code></li> <li>nerdctl: pass <code>binary=\"nerdctl\"</code></li> </ul>"},{"location":"guides/local-containers/#basic-usage","title":"Basic usage","text":"<p>The <code>Container</code> provider is a drop-in replacement for any cloud provider. The only change is the <code>provider</code> parameter:</p> <pre><code>\"\"\"Local Containers \u2014 test your compute functions without cloud costs.\"\"\"\n\nimport skyward as sky\n\n\n@sky.compute\ndef hello() -&gt; str:\n    \"\"\"A simple function that reports where it's running.\"\"\"\n    import socket\n\n    return f\"Hello from {socket.gethostname()}\"\n</code></pre> <pre><code># 1. Basic: single container, no dependencies\nwith sky.ComputePool(provider=sky.Container(), nodes=1) as pool:\n    print(hello() &gt;&gt; pool)\n</code></pre> <p>Behind the scenes, Skyward builds a lightweight Docker image with SSH access, starts a container, opens an SSH tunnel, bootstraps the environment, and runs the worker \u2014 the same pipeline that runs on a real cloud instance. The function is serialized with cloudpickle, sent over the tunnel, executed inside the container, and the result comes back.</p>"},{"location":"guides/local-containers/#testing-your-image","title":"Testing your image","text":"<p>One of the most common sources of failures in cloud runs is a misconfigured <code>Image</code>: a missing pip package, a wrong environment variable, a Python version mismatch. The Container provider lets you validate all of this locally before spending time and money on cloud provisioning.</p> <pre><code>def check_env() -&gt; str:\n    \"\"\"Read an environment variable set via Image.\"\"\"\n    import os\n\n    return os.environ.get(\"MY_VAR\", \"not set\")\n</code></pre> <pre><code>with sky.ComputePool(\n    provider=sky.Container(),\n    nodes=1,\n    image=sky.Image(env={\"MY_VAR\": \"it works\"}),\n) as pool:\n    print(check_env() &gt;&gt; pool)\n</code></pre> <p>If the function returns <code>\"it works\"</code>, you know the <code>env</code> field in your Image is being injected correctly. The same applies to <code>pip</code> (install packages and import them inside the function), <code>apt</code> (install system tools and shell out to them), and <code>includes</code> (sync local modules and import them).</p>"},{"location":"guides/local-containers/#multi-node-locally","title":"Multi-node locally","text":"<p>The Container provider supports <code>nodes &gt; 1</code>. Each node becomes a separate container, and they form a real Casty cluster \u2014 with a head node, peer discovery, and inter-node networking via a Docker bridge network. This lets you test broadcast, data sharding, and distributed collections without any cloud infrastructure.</p> <pre><code>def node_info() -&gt; dict:\n    \"\"\"Report this node's position in the cluster.\"\"\"\n    info = sky.instance_info()\n    assert info is not None\n    return {\n        \"node\": info.node,\n        \"total_nodes\": info.total_nodes,\n        \"is_head\": info.is_head,\n    }\n\n\n@sky.compute\ndef shard_sum(data: list[int]) -&gt; int:\n    \"\"\"Sum only this node's shard of the data.\"\"\"\n    local = sky.shard(data)\n    return sum(local)\n</code></pre> <pre><code>with sky.ComputePool(provider=sky.Container(), nodes=3) as pool:\n    results = node_info() @ pool\n    for r in results:\n        print(f\"  Node {r['node']}/{r['total_nodes']} (head={r['is_head']})\")\n\n    partial_sums = shard_sum(list(range(100))) @ pool\n    print(f\"  Total: {sum(partial_sums)}\")\n</code></pre> <p>Each container gets its own <code>instance_info()</code> with the correct <code>node</code>, <code>total_nodes</code>, and <code>is_head</code> values. <code>sky.shard()</code> works as expected \u2014 each node processes its portion of the data. This is the same behavior you'd see on a 3-node AWS cluster, just running on <code>localhost</code>.</p>"},{"location":"guides/local-containers/#configuration","title":"Configuration","text":"<p>The <code>Container</code> dataclass accepts a few parameters beyond the default:</p> <pre><code>sky.Container(\n    image=\"ubuntu:24.04\",   # base Docker image (default)\n    binary=\"docker\",        # container runtime CLI\n    network=\"my-network\",   # shared Docker network name (optional)\n)\n</code></pre> <ul> <li><code>image</code> \u2014 The base Docker image. Skyward builds an SSH-enabled layer on top of it. Use a different base if your functions need system libraries that take long to install via <code>apt</code>.</li> <li><code>binary</code> \u2014 The container runtime. Useful for environments where Docker isn't available (Podman in rootless mode, nerdctl with containerd).</li> <li><code>network</code> \u2014 By default, each pool creates its own isolated Docker network and tears it down on exit. If you set a shared network name, multiple concurrent pools reuse the same network \u2014 useful when running parallel test suites.</li> </ul> <p>Resource limits (<code>vcpus</code> and <code>memory_gb</code>) are set on the pool, not the provider:</p> <pre><code>sky.ComputePool(\n    provider=sky.Container(),\n    nodes=2,\n    vcpus=1,\n    memory_gb=1,\n)\n</code></pre> <p>These map directly to Docker's <code>--cpus</code> and <code>--memory</code> flags.</p>"},{"location":"guides/local-containers/#cicd","title":"CI/CD","text":"<p>The Container provider works anywhere Docker runs \u2014 including CI environments. Since GitHub Actions runners have Docker pre-installed, you can run integration tests against real Skyward pools without cloud credentials.</p> <p>A typical pytest setup uses a session-scoped fixture so that the pool is provisioned once and reused across tests:</p> <pre><code>import pytest\nimport skyward as sky\n\n@pytest.fixture(scope=\"session\")\ndef pool():\n    with sky.ComputePool(\n        provider=sky.Container(network=\"skyward-ci\"),\n        nodes=2,\n        vcpus=1,\n        memory_gb=1,\n    ) as p:\n        yield p\n</code></pre> <p>Then test your compute functions against the fixture:</p> <pre><code>@sky.compute\ndef double(x: int) -&gt; int:\n    return x * 2\n\ndef test_single_execution(pool):\n    assert double(5) &gt;&gt; pool == 10\n\ndef test_broadcast(pool):\n    results = double(5) @ pool\n    assert all(r == 10 for r in results)\n</code></pre> <p>This runs real containers, real SSH tunnels, real serialization \u2014 the full Skyward stack \u2014 in your CI pipeline.</p>"},{"location":"guides/local-containers/#what-you-can-and-cant-test-locally","title":"What you can and can't test locally","text":"<p>The Container provider replicates the full orchestration pipeline: provisioning, SSH, bootstrap, worker startup, cluster formation, task dispatch, and teardown. Most of what matters for correctness \u2014 serialization, Image configuration, data sharding, distributed collections, broadcast \u2014 works identically.</p> <p>What it doesn't replicate:</p> <ul> <li>GPU execution \u2014 containers don't have accelerators (unless your machine has GPUs and you use <code>--gpus</code> via a custom setup). Code that calls <code>torch.cuda</code> will see no devices.</li> <li>Spot preemption \u2014 there's no concept of spot interruption in local containers. Preemption handling can't be tested this way.</li> <li>Cloud-specific networking \u2014 VPCs, security groups, and cross-AZ latency don't exist locally. Multi-node communication is over a Docker bridge, which is effectively zero-latency.</li> <li>Real provisioning time \u2014 containers start in seconds. Cloud instances take minutes. Timeout tuning needs real cloud testing.</li> </ul> <p>The general pattern: test logic and integration locally with <code>Container</code>, test performance and infrastructure with a real cloud provider.</p>"},{"location":"guides/local-containers/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/11_local_containers.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>sky.Container()</code> runs the full Skyward stack locally using Docker containers.</li> <li>Same orchestration \u2014 SSH, bootstrap, workers, cluster formation \u2014 just on localhost.</li> <li>Image validation \u2014 test pip, apt, env, and includes before deploying to cloud.</li> <li>Multi-node locally \u2014 broadcast, shard, and distributed collections work with <code>nodes &gt; 1</code>.</li> <li>CI-friendly \u2014 session-scoped pytest fixtures give you real integration tests without cloud costs.</li> </ul>"},{"location":"guides/microgpt-distributed/","title":"Distributed MicroGPT with JAX","text":"<p>Andrej Karpathy's MicroGPT trains a character-level GPT in ~200 lines of pure Python \u2014 no frameworks, no dependencies. It's the complete algorithm; everything else is just efficiency. This guide takes that same algorithm, rewrites it in idiomatic JAX, and distributes it across multiple GPUs with Skyward. By the end, you'll have a model that trains on sharded data across a cluster, with automatic gradient synchronization, and generates hallucinated names from what it learned.</p>"},{"location":"guides/microgpt-distributed/#the-compute-function","title":"The compute function","text":"<p>The entire training pipeline lives inside a single <code>@sky.compute</code> function:</p> <pre><code>@sky.compute\n@sky.stdout(only=\"head\")\ndef train_microgpt(\n    n_layer: int = 6,\n    n_embd: int = 256,\n    n_head: int = 8,\n    block_size: int = 512,\n    batch_size: int = 64,\n    num_epochs: int = 2,\n    lr: float = 3e-4,\n    dropout_rate: float = 0.1,\n    temperature: float = 0.8,\n    num_samples: int = 10,\n) -&gt; dict:\n</code></pre> <ul> <li><code>@sky.compute</code> wraps the function into a <code>PendingCompute</code> \u2014 nothing executes until dispatched to a pool via an operator.</li> <li><code>@sky.stdout(only=\"head\")</code> silences stdout on all nodes except node 0. Without this, every node would print interleaved training logs.</li> </ul> <p>JAX's distributed runtime is initialized by <code>sky.plugins.jax()</code> on the pool (shown in the Running It section below). All hyperparameters are function arguments with defaults \u2014 you can override any of them when calling the function without touching the code.</p>"},{"location":"guides/microgpt-distributed/#data-sharding","title":"Data sharding","text":"<p>Each node downloads the dataset independently, but only trains on its own slice:</p> <pre><code>docs: list[str] = (\n    recipes[\"title\"] + \"\\n\" + recipes[\"ingredients\"] + \"\\n\" + recipes[\"directions\"]\n).tolist()\nlocal_docs = sky.shard(docs)\n</code></pre> <p><code>sky.shard(docs)</code> splits the list of documents across nodes using modulo striding \u2014 node 0 gets indices <code>[0, N, 2N, ...]</code>, node 1 gets <code>[1, N+1, 2N+1, ...]</code>, and so on. Each node then tokenizes only its local shard and concatenates the tokens into a single flat array on GPU:</p> <pre><code>encoded = enc.encode_batch(local_docs)\nflat_tokens = [t for doc_toks in encoded for t in (eot, *doc_toks)]\nflat_tokens.append(eot)\n\ntokens_gpu = jax.device_put(\n    jnp.array(flat_tokens, dtype=jnp.int32), replicated,\n)\nn_tokens = tokens_gpu.shape[0]\n</code></pre> <p>This is data parallelism at the dataset level \u2014 the model is identical on every node, but each node sees different training examples.</p>"},{"location":"guides/microgpt-distributed/#mesh-sharding","title":"Mesh sharding","text":"<p>This is where Skyward's JAX plugin and JAX's SPMD model come together. After initializing parameters, the code sets up a sharding mesh:</p> <pre><code>mesh = jax.make_mesh((jax.device_count(),), (\"batch\",))\ndata_sharding = jax.sharding.NamedSharding(\n    mesh, jax.sharding.PartitionSpec(\"batch\"),\n)\nreplicated = jax.sharding.NamedSharding(\n    mesh, jax.sharding.PartitionSpec(),\n)\n</code></pre> <p><code>jax.make_mesh</code> creates a logical device mesh spanning all devices across all nodes. <code>NamedSharding</code> with <code>PartitionSpec(\"batch\")</code> means data tensors are split along the batch dimension across devices. <code>PartitionSpec()</code> (empty) means parameters are replicated on every device.</p> <p>Because <code>sky.plugins.jax()</code> already initialized the distributed runtime, <code>jax.device_count()</code> returns the total number of devices across the entire cluster \u2014 not just local ones. The mesh spans the full cluster transparently.</p>"},{"location":"guides/microgpt-distributed/#training","title":"Training","text":"<p>The training step uses Optax for optimization with warmup + cosine decay + gradient clipping:</p> <pre><code>@nnx.jit\ndef train_step(model, optimizer, inputs, targets):\n    def loss_fn(model):\n        logits = model(inputs, deterministic=False)\n        return optax.softmax_cross_entropy_with_integer_labels(\n            logits=logits, labels=targets,\n        ).mean()\n\n    loss, grads = nnx.value_and_grad(loss_fn)(model)\n    optimizer.update(model, grads)\n    return loss\n</code></pre> <p>The <code>@nnx.jit</code> decorator compiles this into an XLA computation. Because the inputs use <code>NamedSharding</code>, JAX automatically inserts all-reduce operations for the gradients \u2014 each node computes gradients on its local data shard, and XLA averages them across the mesh before the optimizer step. No explicit <code>all_reduce</code> call needed.</p> <p>The training loop samples mini-batches from the local token array and feeds them through the sharded pipeline:</p> <pre><code>for step in range(num_steps):\n    sample_key, subkey = random.split(sample_key)\n    starts = random.randint(subkey, (batch_size,), 0, max_start)\n    batch = jax.vmap(get_window)(starts)\n    inputs_local = batch[:, :-1]\n    targets_local = batch[:, 1:]\n\n    inputs = jax.make_array_from_process_local_data(data_sharding, inputs_local)\n    targets = jax.make_array_from_process_local_data(data_sharding, targets_local)\n\n    loss = train_step(model, optimizer, inputs, targets)\n\n    final_loss = float(loss)\n    if step % log_interval == 0 or step == num_steps - 1:\n        print(f\"step {step+1:5d}/{num_steps} | loss {final_loss:.4f}\")\n</code></pre> <p><code>jax.make_array_from_process_local_data</code> takes each node's local batch and constructs a globally-sharded array that JAX's runtime distributes according to the mesh. From JAX's perspective, it sees a single large batch split across devices.</p>"},{"location":"guides/microgpt-distributed/#running-it","title":"Running it","text":"<p>The main block provisions a cluster and broadcasts the training function to all nodes:</p> <pre><code>with sky.ComputePool(\n    sky.Spec(\n        provider=sky.RunPod(),\n        accelerator=sky.accelerators.RTX_4090(),\n        ttl=2400\n    ),\n    image=sky.Image(pip=[\"flax\", \"pandas==2.3.3\", \"tiktoken\"]),\n    plugins=[sky.plugins.jax()],\n) as pool:\n    results = train_microgpt().with_timeout(2400) @ pool\n</code></pre> <p><code>@ pool</code> (the matmul operator) broadcasts <code>train_microgpt()</code> to every node in the pool. Each node executes the same function with the same hyperparameters, but trains on different data shards. The results \u2014 a dict per node with loss and generated samples \u2014 are collected back to the caller.</p> <p>The <code>jax</code> plugin is specified on the pool via <code>plugins=[sky.plugins.jax()]</code> \u2014 it installs JAX with CUDA support and initializes the distributed runtime on each worker before the function runs.</p>"},{"location":"guides/microgpt-distributed/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/30_flax_minigpt.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>plugins=[sky.plugins.jax()]</code> initializes JAX's distributed runtime automatically \u2014 coordinator address, process count, and process id are derived from the cluster topology.</li> <li><code>sky.shard()</code> splits data across nodes \u2014 each node tokenizes and trains on its own partition.</li> <li>Mesh sharding with <code>NamedSharding</code> gives you automatic gradient all-reduce \u2014 params replicated, data sharded, XLA handles the communication.</li> <li><code>@sky.stdout(only=\"head\")</code> keeps training logs clean by silencing non-head nodes.</li> <li><code>@ pool</code> (broadcast) sends the compute function to all nodes \u2014 results are collected as a list.</li> </ul>"},{"location":"guides/multi-provider/","title":"Multi-provider selection","text":"<p>Accelerator prices fluctuate. Availability varies by region, time of day, and provider. An A100 on VastAI might cost $1.50/hr right now but have no capacity, while the same accelerator on AWS is $3.00/hr with instant availability. Instead of checking pricing pages manually, you can describe your hardware need across multiple providers and let Skyward query all of them, compare offers, and provision from the best option.</p>"},{"location":"guides/multi-provider/#the-spec","title":"The spec","text":"<p>A <code>sky.Spec</code> bundles a provider with hardware preferences into a single, composable unit:</p> <pre><code>sky.Spec(\n    provider=sky.VastAI(),\n    accelerator=\"A100\",\n    max_hourly_cost=2.50,\n    allocation=\"spot\",\n)\n</code></pre> <p>It carries the same fields you'd normally pass to <code>ComputePool</code> \u2014 <code>accelerator</code>, <code>nodes</code>, <code>vcpus</code>, <code>memory_gb</code>, <code>architecture</code>, <code>allocation</code>, <code>region</code>, <code>max_hourly_cost</code>, <code>ttl</code> \u2014 but scoped to a specific provider. This separation is what makes cross-provider comparison possible: each <code>Spec</code> is a self-contained description of \"what I want, from whom.\"</p>"},{"location":"guides/multi-provider/#cheapest-across-providers","title":"Cheapest across providers","text":"<p>Pass multiple <code>Spec</code> objects to <code>ComputePool</code> and Skyward queries each provider's available offers, compares prices, and provisions from the cheapest:</p> <pre><code>with sky.ComputePool(\n    sky.Spec(provider=sky.VastAI(), accelerator=\"A100\"),\n    sky.Spec(provider=sky.AWS(), accelerator=\"A100\"),\n    selection=\"cheapest\",\n    image=sky.Image(pip=[\"torch\"]),\n) as pool:\n    result = train(10) &gt;&gt; pool\n    print(f\"Cheapest: {result}\")\n</code></pre> <p>With <code>selection=\"cheapest\"</code> (the default), Skyward calls <code>offers()</code> on every provider in the list, collects all available machine types with their pricing, and picks the one with the lowest cost. The pool then provisions from that provider \u2014 the rest of the lifecycle (SSH, bootstrap, task dispatch) is unchanged.</p> <p>This is useful when you don't have a strong provider preference and want cost optimization. The same A100 workload might end up on VastAI one day and AWS the next, depending on current market prices.</p>"},{"location":"guides/multi-provider/#first-available","title":"First available","text":"<p>When you have a preferred provider but want a fallback, use <code>selection=\"first\"</code>:</p> <pre><code>with sky.ComputePool(\n    sky.Spec(provider=sky.RunPod(), accelerator=\"H100\", nodes=4),\n    sky.Spec(provider=sky.AWS(), accelerator=\"H100\", nodes=4),\n    selection=\"first\",\n    image=sky.Image(pip=[\"torch\"]),\n) as pool:\n    results = train(10) @ pool\n    print(f\"First available: {results}\")\n</code></pre> <p>Specs are tried in order. Skyward queries RunPod first \u2014 if it has H100 offers available, that's where the pool provisions. If RunPod has no capacity, Skyward moves to AWS. This gives you deterministic priority ordering while still avoiding manual retries when your preferred provider is out of stock.</p>"},{"location":"guides/multi-provider/#per-spec-constraints","title":"Per-spec constraints","text":"<p>Each <code>Spec</code> can have its own allocation strategy, cost cap, and region. This enables escalating fallback patterns \u2014 start aggressive, fall back to safer options:</p> <pre><code>with sky.ComputePool(\n    sky.Spec(\n        provider=sky.VastAI(),\n        accelerator=\"A100\",\n        max_hourly_cost=2.50,\n        allocation=\"spot\",\n    ),\n    sky.Spec(\n        provider=sky.Verda(),\n        accelerator=\"A100\",\n        allocation=\"spot-if-available\",\n    ),\n    sky.Spec(\n        provider=sky.AWS(),\n        accelerator=\"A100\",\n        allocation=\"on-demand\",\n    ),\n    selection=\"cheapest\",\n    image=sky.Image(pip=[\"torch\"]),\n) as pool:\n    result = train(10) &gt;&gt; pool\n</code></pre> <p>The first spec tries spot instances on VastAI with a $2.50/hr cap \u2014 the cheapest option if available. If that fails (no capacity, or prices exceed the cap), the second spec tries Verda with spot-if-available. The third spec is the safety net: on-demand AWS, which is more expensive but virtually always available. Skyward evaluates all three and picks the cheapest viable option.</p>"},{"location":"guides/multi-provider/#how-it-works","title":"How it works","text":"<p>When <code>ComputePool.__enter__</code> runs, Skyward iterates through your specs before provisioning anything:</p> <ol> <li>For each <code>Spec</code>, it creates a provider instance and calls <code>provider.offers(spec)</code> \u2014 an async generator that yields available machine types with pricing.</li> <li>Based on the <code>selection</code> strategy, it either takes the first available offer or collects all offers and picks the cheapest.</li> <li>The winning offer \u2014 which carries the selected provider, machine type, and pricing \u2014 is passed to <code>provider.prepare(spec, offer)</code> to set up infrastructure.</li> <li>From there, the lifecycle proceeds normally: provision instances, bootstrap, start workers.</li> </ol> <p>The key insight is that offer querying is fast (API calls to check availability and pricing) while provisioning is slow (launching machines). By querying all providers before committing to one, Skyward makes an informed decision without wasting time on failed provisioning attempts.</p>"},{"location":"guides/multi-provider/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/12_multi_provider.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>sky.Spec</code> bundles a provider with hardware preferences into a composable unit \u2014 accelerator, nodes, allocation, cost cap, all scoped to one provider.</li> <li>Multi-spec <code>ComputePool</code> accepts multiple Specs as positional arguments and selects the best offer before provisioning.</li> <li><code>selection=\"cheapest\"</code> (default) queries all providers and picks the lowest price across all of them.</li> <li><code>selection=\"first\"</code> respects your priority ordering \u2014 tries specs in sequence, stops at the first with available offers.</li> <li>Per-spec constraints let each Spec have its own allocation strategy and cost cap, enabling escalating fallback patterns.</li> <li>Single-provider mode still works \u2014 <code>ComputePool(provider=sky.AWS(), ...)</code> is unchanged and internally wraps a single <code>Spec</code>.</li> </ul>"},{"location":"guides/nvidia-mig/","title":"NVIDIA MIG","text":"<p>A single A100 or H100 GPU has enormous compute capacity \u2014 often more than a single workload needs. NVIDIA Multi-Instance GPU (MIG) solves this by partitioning one physical GPU into multiple isolated instances, each with its own compute units, memory, and L2 cache. Two inference servers, two training runs, or two independent benchmarks can share the same card without any interference. No time-slicing, no memory contention \u2014 each partition behaves like a smaller, dedicated GPU.</p> <p>The catch is that setting up MIG is manual and finicky. You need to enable MIG mode via <code>nvidia-smi</code>, create GPU instances with the right profile, create compute instances inside them, and then assign each process to its partition through <code>CUDA_VISIBLE_DEVICES</code> with MIG-specific UUIDs. Skyward's <code>mig</code> plugin handles all of this: it enables MIG during bootstrap, creates the partitions, and assigns each subprocess to its own device automatically.</p>"},{"location":"guides/nvidia-mig/#how-mig-partitioning-works","title":"How MIG partitioning works","text":"<p>A MIG-capable GPU (A100, A30, H100) can be sliced into profiles like <code>1g.10gb</code>, <code>3g.40gb</code>, or <code>7g.80gb</code>. The first number indicates how many compute slices the partition gets; the second is its dedicated memory. An A100 80GB can be split into two <code>3g.40gb</code> partitions (42 SMs and ~40GB each) or seven <code>1g.10gb</code> partitions (14 SMs and ~10GB each). The profiles are fixed \u2014 you choose one when creating instances, and all partitions on a GPU must use the same profile.</p> <p>Each partition gets its own MIG UUID, visible through <code>nvidia-smi -L</code>. When a process sets <code>CUDA_VISIBLE_DEVICES</code> to a MIG UUID, CUDA presents that partition as the only available device. The process sees a GPU with fewer SMs and less memory, but the isolation is hardware-enforced \u2014 there's no way for one partition to access another's memory or steal its compute cycles.</p>"},{"location":"guides/nvidia-mig/#the-mig-plugin","title":"The <code>mig</code> plugin","text":"<p>Add <code>sky.plugins.mig()</code> to your pool and set the worker concurrency to match the number of partitions:</p> <pre><code>PARTITIONS = 2\nPROFILE = \"3g.40gb\"\n</code></pre> <pre><code>with sky.ComputePool(\n    provider=sky.Verda(),\n    nodes=1,\n    accelerator=sky.accelerators.A100(),\n    worker=sky.Worker(concurrency=PARTITIONS, executor=\"process\"),\n    image=sky.Image(pip=[\"torch\"]),\n    plugins=[sky.plugins.mig(profile=PROFILE)],\n) as pool:\n</code></pre> <p>Three things happen here. First, the <code>mig</code> plugin's <code>profile</code> parameter tells Skyward which MIG profile to create \u2014 in this case, <code>3g.40gb</code>, which splits the GPU into two partitions. Second, <code>Worker(concurrency=2, executor=\"process\")</code> creates two loky subprocesses on the node, one per partition. Third, <code>Image(pip=[\"torch\"])</code> installs PyTorch on the worker \u2014 we're not using <code>sky.plugins.torch()</code> here because the torch plugin is designed for multi-node distributed training (DDP), and MIG partitions on a single GPU are independent workloads, not a distributed cluster.</p> <p>The concurrency and the profile must agree: if a profile creates two partitions, concurrency should be two. If you set concurrency to three but the GPU only supports two instances of your profile, the bootstrap will fail.</p>"},{"location":"guides/nvidia-mig/#what-the-plugin-does","title":"What the plugin does","text":"<p>The plugin handles the full MIG lifecycle so your <code>@sky.compute</code> function doesn't need to know about partitioning at all. When the node boots, the plugin enables MIG mode and creates the requested partitions. When a subprocess picks up its first task, the plugin detects which partition belongs to it and sets <code>CUDA_VISIBLE_DEVICES</code> accordingly. From that point on, CUDA sees only that partition \u2014 your code calls <code>torch.device(\"cuda\")</code> and lands on the right slice of the GPU automatically.</p> <p>For the technical details on how each hook works, see the MIG plugin reference.</p>"},{"location":"guides/nvidia-mig/#training-on-a-partition","title":"Training on a partition","text":"<p>The function itself doesn't know about MIG \u2014 it just sees a CUDA device:</p> <pre><code>@sky.compute\ndef train_on_partition(epochs: int, lr: float) -&gt; dict:\n    \"\"\"Train a small CNN on a MIG partition.\"\"\"\n    import os\n\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, TensorDataset\n\n    info = sky.instance_info()\n    device = torch.device(\"cuda\")\n\n    model = nn.Sequential(\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        nn.Linear(256, 128),\n        nn.ReLU(),\n        nn.Linear(128, 10),\n    ).to(device)\n\n    x = torch.randn(5000, 784, device=device)\n    y = torch.randint(0, 10, (5000,), device=device)\n    loader = DataLoader(TensorDataset(x, y), batch_size=128, shuffle=True)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(epochs):\n        epoch_loss = 0.0\n        correct = 0\n        total = 0\n        for batch_x, batch_y in loader:\n            optimizer.zero_grad()\n            output = model(batch_x)\n            loss = criterion(output, batch_y)\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            correct += (output.argmax(1) == batch_y).sum().item()\n            total += batch_y.size(0)\n\n    accuracy = 100.0 * correct / total\n    return {\n        \"worker\": info.worker,\n        \"partition\": os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"unset\"),\n        \"final_loss\": round(epoch_loss / len(loader), 4),\n        \"accuracy\": round(accuracy, 1),\n    }\n</code></pre> <p>There are no <code>if torch.cuda.is_available()</code> guards \u2014 this code runs on a GPU node with MIG partitions, so CUDA is always present. <code>torch.device(\"cuda\")</code> resolves to the MIG partition assigned by the plugin. The function trains a small feedforward network on synthetic data and returns the final metrics along with the worker index and partition UUID.</p> <p><code>instance_info().worker</code> returns the subprocess index (0 or 1), which the plugin already used to assign the correct MIG partition. Each subprocess runs independently in its own loky process with its own CUDA context, so there's no shared state between the two training runs.</p>"},{"location":"guides/nvidia-mig/#running-both-partitions-in-parallel","title":"Running both partitions in parallel","text":"<p>Dispatch one task per partition using <code>gather()</code>:</p> <pre><code>tasks = [train_on_partition(epochs=10, lr=1e-3) for _ in range(PARTITIONS)]\nresults = list(sky.gather(*tasks, stream=True) &gt;&gt; pool)\n</code></pre> <p>Both tasks execute simultaneously \u2014 worker 0 on partition 0, worker 1 on partition 1. Since MIG provides hardware-level isolation, neither task impacts the other's performance. The wall time should be close to the time of a single task, not the sum.</p>"},{"location":"guides/nvidia-mig/#choosing-the-right-profile","title":"Choosing the right profile","text":"<p>The profile determines the trade-off between partition count and per-partition resources. On an A100 80GB, a <code>3g.40gb</code> profile splits the GPU into two partitions with ~40 GB and 42 SMs each, while a <code>1g.10gb</code> profile fits seven partitions with ~10 GB and 14 SMs each. Other GPUs have different profile sets \u2014 an RTX PRO 6000 supports four profiles instead of seven. Throughput scales roughly linearly with profile size. The first number in the profile name indicates compute slices, not a fraction of the GPU: <code>3g.40gb</code> gets about 3/7 of the SMs, not half.</p> <p>See NVIDIA's supported GPUs page for the full list of profiles per GPU, or run <code>nvidia-smi mig -lgip</code> on a MIG-capable node to see what's available.</p>"},{"location":"guides/nvidia-mig/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/18_nvidia_mig.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>sky.plugins.mig(profile=\"3g.40gb\")</code> \u2014 enables MIG mode, creates partitions during bootstrap, and assigns each subprocess to its own MIG device.</li> <li><code>Worker(concurrency=N, executor=\"process\")</code> \u2014 one loky subprocess per partition; concurrency must match the number of partitions the profile supports.</li> <li>Hardware isolation \u2014 each partition has dedicated compute units, memory, and L2 cache; no time-slicing or contention.</li> <li>Transparent to <code>@sky.compute</code> \u2014 functions see a normal CUDA device; the plugin sets <code>CUDA_VISIBLE_DEVICES</code> per subprocess via the <code>around_process</code> hook.</li> <li>Profile choice \u2014 more partitions means less compute per partition; pick based on your workload's memory and throughput needs.</li> </ul>"},{"location":"guides/parallel-execution/","title":"Parallel execution","text":"<p>A single <code>&gt;&gt;</code> sends one computation to one node. But many workloads consist of multiple independent tasks \u2014 processing chunks of data, running different model configurations, evaluating several inputs. Skyward provides two ways to parallelize: <code>gather()</code> for dynamic collections and <code>&amp;</code> for type-safe composition of a fixed set. Both dispatch all tasks concurrently and block until the results are ready.</p>"},{"location":"guides/parallel-execution/#compute-functions","title":"Compute functions","text":"<p>Define the functions you want to run remotely. Each one is independent \u2014 they can execute in any order, on the same or different nodes:</p> <pre><code>@sky.compute\ndef process_chunk(data: list[int]) -&gt; int:\n    \"\"\"Sum all numbers in a chunk.\"\"\"\n    return sum(data)\n\n\n@sky.compute\ndef multiply(x: int, y: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return x * y\n\n\n@sky.compute\ndef factorial(n: int) -&gt; int:\n    \"\"\"Calculate factorial.\"\"\"\n    result = 1\n    for i in range(1, n + 1):\n        result *= i\n    return result\n</code></pre>"},{"location":"guides/parallel-execution/#parallel-with-gather","title":"Parallel with <code>gather()</code>","text":"<p>When you have a dynamic number of tasks \u2014 iterating over a list of chunks, a set of configurations, or any collection whose size isn't known at write time \u2014 use <code>gather()</code>:</p> <pre><code># gather() runs all calls in parallel\nresults = sky.gather(*[process_chunk(c) for c in chunks]) &gt;&gt; pool\nprint(f\"Chunk sums: {results}\")  # (6, 15, 24)\n</code></pre> <p><code>gather()</code> collects multiple <code>PendingCompute</code> values into a <code>PendingComputeGroup</code>. When dispatched with <code>&gt;&gt;</code>, all tasks execute concurrently on the pool's nodes (distributed via round-robin) and the results come back as a tuple. The pool handles serialization, dispatch, and collection \u2014 you just express which tasks should run in parallel.</p>"},{"location":"guides/parallel-execution/#type-safe-parallel-with","title":"Type-safe parallel with <code>&amp;</code>","text":"<p>When the number of parallel tasks is fixed and you want full type inference, use the <code>&amp;</code> operator:</p> <pre><code># &amp; operator for type-safe parallel execution\na, b = (multiply(2, 3) &amp; multiply(4, 5)) &gt;&gt; pool\nprint(f\"Products: {a}, {b}\")  # 6, 20\n</code></pre> <p>The <code>&amp;</code> operator creates the same <code>PendingComputeGroup</code> that <code>gather()</code> produces, but with a key difference: the types are preserved individually. Here, <code>a</code> and <code>b</code> are both <code>int</code> because <code>multiply</code> returns <code>int</code>. If you chain three different functions \u2014 <code>preprocess() &amp; train() &amp; evaluate()</code> \u2014 the result type is <code>tuple[DataFrame, Model, float]</code>, not <code>tuple[Any, ...]</code>.</p>"},{"location":"guides/parallel-execution/#mixing-different-computations","title":"Mixing different computations","text":"<p>Since <code>&amp;</code> preserves types per-position, you can compose completely different functions in a single parallel batch:</p> <pre><code># Mix different computations\ns, p, f = (process_chunk([1, 2, 3]) &amp; multiply(10, 20) &amp; factorial(5)) &gt;&gt; pool\nprint(f\"Mixed: sum={s}, product={p}, factorial={f}\")  # 6, 200, 120\n</code></pre> <p>Each computation may go to a different node (round-robin scheduling), and the group blocks until all of them complete. The destructured variables <code>s</code>, <code>p</code>, <code>f</code> each carry their correct type.</p> <p>The distinction from broadcast (<code>@</code>) is important: <code>@</code> runs the same function on all nodes, while <code>&amp;</code> runs different functions concurrently. Use <code>@</code> when every node should do the same work; use <code>&amp;</code> when you have distinct, independent tasks.</p>"},{"location":"guides/parallel-execution/#streaming-results","title":"Streaming results","text":"<p>By default, <code>gather()</code> waits for all tasks to finish before returning. With <code>stream=True</code>, results are yielded as they complete \u2014 useful when tasks have variable duration and you want to start processing early:</p> <pre><code># gather(stream=True) yields results as they complete\ntasks = [process_chunk([i] * 1000) for i in range(5)]\nstart = time.monotonic()\nfor result in sky.gather(*tasks, stream=True) &gt;&gt; pool:\n    elapsed = time.monotonic() - start\n    print(f\"  [{elapsed:.1f}s] Got: {result}\")\n</code></pre> <p>Streaming changes the return type from a tuple to a generator. Results arrive in completion order, not submission order \u2014 the fastest tasks come first. This is ideal for displaying progress, feeding partial results into a downstream pipeline, or reducing time-to-first-result when tasks have uneven durations.</p> <p>If you need results in the original submission order even while streaming, pass <code>ordered=True</code> (the default). Skyward will buffer internally and yield in order, though this means you won't see a result until all preceding tasks have also completed.</p>"},{"location":"guides/parallel-execution/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/02_parallel_execution.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>gather()</code> collects a dynamic number of computations into a parallel batch \u2014 dispatch with <code>&gt;&gt;</code>, results as a tuple.</li> <li><code>&amp;</code> operator composes a fixed set of computations with full type inference per position.</li> <li><code>stream=True</code> yields results as they complete instead of waiting for all \u2014 useful for variable-duration tasks.</li> <li><code>@</code> vs <code>&amp;</code> \u2014 broadcast runs the same function on all nodes; <code>&amp;</code> runs different functions concurrently.</li> </ul>"},{"location":"guides/pytorch-distributed/","title":"PyTorch distributed","text":"<p>Training a neural network across multiple nodes requires coordinating processes that don't share memory. Each node needs to know the cluster topology \u2014 who the master is, how many peers exist, what rank it holds \u2014 and the processes need to synchronize gradients during backpropagation. PyTorch's DistributedDataParallel (DDP) handles the gradient synchronization, but the environment setup is notoriously manual: setting <code>MASTER_ADDR</code>, <code>MASTER_PORT</code>, <code>WORLD_SIZE</code>, <code>RANK</code>, and calling <code>init_process_group()</code> correctly on every node.</p> <p>Skyward's <code>torch</code> plugin does all of this automatically. It reads the cluster topology from <code>instance_info()</code>, configures the environment variables, and initializes the process group before your function runs. You write a standard DDP training loop \u2014 Skyward handles the distributed plumbing.</p>"},{"location":"guides/pytorch-distributed/#the-torch-plugin","title":"The <code>torch</code> plugin","text":"<p>Add <code>sky.plugins.torch()</code> to your pool's plugins:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=2,\n    accelerator=\"T4\",\n    plugins=[sky.plugins.torch()],\n) as pool:\n</code></pre> <p>The plugin is specified on the pool, not on the function \u2014 it configures the cluster, not the task. It reads <code>instance_info()</code> and sets <code>MASTER_ADDR</code> to the head node's private IP, <code>MASTER_PORT</code> to the coordination port, <code>WORLD_SIZE</code> to the total number of nodes, and <code>RANK</code> to this node's index. It then calls <code>torch.distributed.init_process_group()</code> with the configured backend (defaulting to <code>nccl</code> for GPU, <code>gloo</code> for CPU). By the time your function body runs, the distributed environment is fully initialized.</p> <p>The function itself just uses <code>@sky.compute</code> \u2014 the distributed setup is handled by the plugin:</p> <pre><code>@sky.compute\ndef train(epochs: int, batch_size: int, lr: float) -&gt; dict:\n    \"\"\"Train a neural network with DistributedDataParallel.\"\"\"\n</code></pre>"},{"location":"guides/pytorch-distributed/#model-with-ddp","title":"Model with DDP","text":"<p>Define a standard model and wrap it with <code>DistributedDataParallel</code>:</p> <pre><code>model = nn.Sequential(\n    nn.Linear(100, 256),\n    nn.ReLU(),\n    nn.Linear(256, 10),\n).to(device)\n\nif dist.is_initialized():\n    model = DDP(model)\n</code></pre> <p>DDP replicates the model on each node and synchronizes gradients during <code>backward()</code>. Each node trains on its own shard of the data, but the model parameters stay in sync because gradients are averaged across all nodes before each optimizer step. The <code>if dist.is_initialized()</code> guard lets the same code work in both single-node and multi-node contexts.</p>"},{"location":"guides/pytorch-distributed/#distributed-data-loading","title":"Distributed data loading","text":"<p>Use <code>DistributedSampler</code> to ensure each node gets a unique subset of the data:</p> <pre><code>x = torch.randn(10000, 100)\ny = torch.randint(0, 10, (10000,))\ndataset = TensorDataset(x, y)\nsampler = DistributedSampler(dataset, shuffle=True)\nloader = DataLoader(dataset, batch_size=batch_size, sampler=sampler)\n</code></pre> <p>The sampler reads the rank and world size from the process group and partitions the dataset indices accordingly. Unlike <code>sky.shard()</code>, which operates on raw data, <code>DistributedSampler</code> integrates with PyTorch's <code>DataLoader</code> and handles shuffling per-epoch. Call <code>sampler.set_epoch(epoch)</code> before each epoch so the shuffling pattern changes \u2014 without this, every epoch sees the same order.</p> <p>Both approaches \u2014 <code>sky.shard()</code> and <code>DistributedSampler</code> \u2014 achieve the same goal (each node processes different data), but <code>DistributedSampler</code> is the PyTorch-native way and handles edge cases like uneven dataset sizes and drop-last semantics within the DataLoader pipeline.</p>"},{"location":"guides/pytorch-distributed/#aggregating-metrics","title":"Aggregating metrics","text":"<p>During training, each node computes local metrics (loss, accuracy). To get global metrics \u2014 averaged across all nodes \u2014 use <code>all_reduce</code>:</p> <pre><code>stats = torch.tensor([epoch_loss, correct, total], dtype=torch.float64, device=device)\ndist.all_reduce(stats, op=dist.ReduceOp.SUM)\navg_loss = stats[0].item() / (len(loader) * info.total_nodes)\naccuracy = 100.0 * stats[1].item() / stats[2].item()\n</code></pre> <p><code>all_reduce</code> with <code>ReduceOp.SUM</code> sums the tensor across all nodes in-place. After the operation, every node holds the same aggregated values. Dividing by the number of nodes (or total samples) gives you the global average. This is how the head node can log consistent, cluster-wide metrics.</p>"},{"location":"guides/pytorch-distributed/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/06_pytorch_distributed.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>plugins=[sky.plugins.torch()]</code> configures <code>MASTER_ADDR</code>, <code>WORLD_SIZE</code>, <code>RANK</code>, and calls <code>init_process_group()</code> automatically.</li> <li>DDP synchronizes gradients across nodes \u2014 each node trains on different data, but model parameters stay in sync.</li> <li><code>DistributedSampler</code> partitions data per node \u2014 call <code>set_epoch()</code> each epoch for proper shuffling.</li> <li><code>all_reduce</code> aggregates metrics across all nodes \u2014 essential for consistent logging.</li> <li>Plugins configure the cluster \u2014 distributed setup lives on the pool, not on individual functions.</li> </ul>"},{"location":"guides/s3-volumes/","title":"S3 volumes","text":"<p>This guide walks through mounting S3 buckets as local filesystems on remote workers. You'll mount a read-only volume for input data and a writable volume for saving results \u2014 both accessible as ordinary directories inside your <code>@sky.compute</code> functions.</p>"},{"location":"guides/s3-volumes/#declaring-volumes","title":"Declaring volumes","text":"<p>A <code>Volume</code> maps an S3 bucket (or a prefix within it) to a local path on every worker:</p> <pre><code>volumes=[\n    sky.Volume(\n        bucket=\"my-datasets\",\n        mount=\"/data\",\n        prefix=\"imagenet/\",\n        read_only=True,\n    ),\n    sky.Volume(\n        bucket=\"my-experiments\",\n        mount=\"/checkpoints\",\n        prefix=\"run-042/\",\n        read_only=False,\n    ),\n],\n</code></pre> <p>The first volume mounts the <code>imagenet/</code> prefix of <code>my-datasets</code> at <code>/data</code> \u2014 read-only, so your training code can read from it but can't accidentally modify the dataset. The second mounts <code>run-042/</code> of <code>my-experiments</code> at <code>/checkpoints</code> \u2014 writable, so workers can save artifacts that persist to S3 after the pool is torn down.</p>"},{"location":"guides/s3-volumes/#reading-from-a-volume","title":"Reading from a volume","text":"<p>Inside a <code>@sky.compute</code> function, the mount path is a regular directory. Standard filesystem operations \u2014 <code>Path.iterdir()</code>, <code>open()</code>, <code>glob()</code> \u2014 work as expected:</p> <pre><code>@sky.compute\ndef list_files(directory: str) -&gt; list[str]:\n    \"\"\"List files in a mounted volume.\"\"\"\n    path = Path(directory)\n    if not path.exists():\n        return []\n    return sorted(f.name for f in path.iterdir() if f.is_file())[:20]\n</code></pre> <p>Because the volume is backed by s3fs-fuse, reads are transparently fetched from S3. There's no download step, no SDK calls, no boto3 \u2014 just file paths.</p>"},{"location":"guides/s3-volumes/#writing-to-a-volume","title":"Writing to a volume","text":"<p>Writable volumes (<code>read_only=False</code>) let workers save files that persist to S3:</p> <pre><code>@sky.compute\ndef save_checkpoint(checkpoint_dir: str, epoch: int, loss: float) -&gt; str:\n    \"\"\"Save a training checkpoint to the writable volume.\"\"\"\n    import json\n\n    info = sky.instance_info()\n\n    checkpoint = {\"epoch\": epoch, \"node\": info.node, \"loss\": loss}\n    path = Path(checkpoint_dir) / f\"node{info.node}_epoch{epoch}.json\"\n    path.parent.mkdir(parents=True, exist_ok=True)\n\n    with open(path, \"w\") as f:\n        json.dump(checkpoint, f)\n\n    return f\"Node {info.node}: saved {path.name} ({path.stat().st_size} bytes)\"\n</code></pre> <p>Each node writes its own checkpoint file. Because the volume is backed by S3, these files survive instance termination \u2014 they're in the bucket, not on ephemeral local disk. Note that s3fs syncs writes asynchronously, so freshly written files may take a moment to appear on other nodes or in the S3 console.</p>"},{"location":"guides/s3-volumes/#broadcasting-across-nodes","title":"Broadcasting across nodes","text":"<p>Volumes are mounted on every node, so broadcast operations naturally see the same storage:</p> <pre><code>@sky.compute\ndef count_files(directory: str) -&gt; dict:\n    \"\"\"Count files and total size in a mounted volume.\"\"\"\n    info = sky.instance_info()\n    path = Path(directory)\n\n    if not path.exists():\n        return {\"node\": info.node, \"files\": 0, \"size_mb\": 0}\n\n    files = [f for f in path.rglob(\"*\") if f.is_file()]\n    total_size = sum(f.stat().st_size for f in files)\n\n    return {\n        \"node\": info.node,\n        \"files\": len(files),\n        \"size_mb\": round(total_size / (1024 * 1024), 2),\n    }\n</code></pre> <p>When dispatched with <code>@</code> (broadcast), each node independently counts files in <code>/data</code>. They all see the same S3 data, so the counts should match \u2014 but each reports from its own perspective via <code>instance_info()</code>.</p>"},{"location":"guides/s3-volumes/#putting-it-together","title":"Putting it together","text":"<pre><code>if __name__ == \"__main__\":\n    with sky.ComputePool(\n        provider=sky.AWS(instance_profile_arn=\"auto\"),\n        nodes=2,\n        volumes=[\n            sky.Volume(\n                bucket=\"my-datasets\",\n                mount=\"/data\",\n                prefix=\"imagenet/\",\n                read_only=True,\n            ),\n            sky.Volume(\n                bucket=\"my-experiments\",\n                mount=\"/checkpoints\",\n                prefix=\"run-042/\",\n                read_only=False,\n            ),\n        ],\n    ) as pool:\n        # Each node sees the same /data \u2014 read-only from S3\n        stats = count_files(\"/data\") @ pool\n        for s in stats:\n            print(f\"  Node {s['node']}: {s['files']} files, {s['size_mb']} MB\")\n\n        # Each node writes to /checkpoints \u2014 writable to S3\n        results = save_checkpoint(\"/checkpoints\", epoch=1, loss=0.42) @ pool\n        for r in results:\n            print(f\"  {r}\")\n\n        # Verify checkpoints are visible\n        files = list_files(\"/checkpoints\") &gt;&gt; pool\n        print(f\"  Checkpoints: {files}\")\n</code></pre> <p>The pool provisions two nodes, mounts both volumes on each, and then the three operations \u2014 count, checkpoint, verify \u2014 run against familiar filesystem paths. When the <code>with</code> block exits, the instances are destroyed, but the checkpoints remain in S3.</p>"},{"location":"guides/s3-volumes/#iam-authentication-aws","title":"IAM authentication (AWS)","text":"<p>On AWS, the cleanest approach is an instance profile with S3 permissions. Pass <code>instance_profile_arn=\"auto\"</code> and Skyward uses IAM role authentication \u2014 no credentials are written to disk, no access keys in environment variables:</p> <pre><code>sky.AWS(instance_profile_arn=\"auto\")\n</code></pre> <p>The instance profile must have <code>s3:GetObject</code> and <code>s3:ListBucket</code> permissions for read-only volumes, plus <code>s3:PutObject</code> and <code>s3:DeleteObject</code> for writable ones.</p>"},{"location":"guides/s3-volumes/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/17_s3_volumes.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>sky.Volume</code> maps an S3 bucket to a local path on every worker \u2014 read or read-write.</li> <li><code>prefix</code> scopes a volume to a subfolder within the bucket, keeping mount points clean.</li> <li>s3fs-fuse handles the mounting transparently \u2014 no SDK, no download step, just file paths.</li> <li>Writable volumes persist data to S3, surviving instance termination and pool teardown.</li> <li>IAM roles (<code>instance_profile_arn=\"auto\"</code>) are the cleanest auth strategy on AWS \u2014 no credentials on disk.</li> </ul>"},{"location":"guides/scikit-grid-search/","title":"Scikit grid search","text":"<p>Hyperparameter search is embarrassingly parallel \u2014 each candidate configuration can be evaluated independently. scikit-learn's <code>GridSearchCV</code> already supports parallelism via <code>n_jobs</code>, but it's limited to the cores on a single machine. Skyward's <code>sklearn</code> plugin extends this to a cluster: it replaces joblib's default backend with a distributed one, so <code>n_jobs=-1</code> distributes cross-validation fits across cloud instances instead of local threads.</p>"},{"location":"guides/scikit-grid-search/#the-dataset","title":"The dataset","text":"<p>Load digits and split into train/test:</p> <pre><code>X, y = load_digits(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</code></pre>"},{"location":"guides/scikit-grid-search/#defining-the-search-space","title":"Defining the search space","text":"<p>Use a Pipeline with a list-of-dicts <code>param_grid</code> to search over both estimators and their hyperparameters:</p> <pre><code>pipe = Pipeline([(\"clf\", SVC())])\n\nparam_grid = [\n    {\n        \"clf\": [RandomForestClassifier(random_state=42)],\n        \"clf__n_estimators\": [50, 100, 200],\n        \"clf__max_depth\": [None, 10, 20],\n    },\n    {\n        \"clf\": [GradientBoostingClassifier(random_state=42)],\n        \"clf__n_estimators\": [50, 100],\n        \"clf__learning_rate\": [0.01, 0.1, 0.2],\n    },\n    {\n        \"clf\": [SVC()],\n        \"clf__C\": [0.1, 1, 10],\n        \"clf__kernel\": [\"rbf\", \"poly\"],\n    },\n]\n</code></pre> <p>Each dict defines a grid for one estimator family. The <code>\"clf\"</code> key swaps the estimator itself (RandomForest, GradientBoosting, SVC), while <code>\"clf__param\"</code> tunes its hyperparameters. scikit-learn expands all combinations \u2014 this grid produces 21 candidates, each cross-validated 5 times, for 105 total fits. On a single machine, these run sequentially or across a few cores. On a cluster, they run across dozens of workers simultaneously.</p>"},{"location":"guides/scikit-grid-search/#distributed-search-with-the-sklearn-plugin","title":"Distributed search with the sklearn plugin","text":"<p>The <code>sklearn</code> plugin replaces joblib's default backend so that <code>Parallel(n_jobs=-1)</code> \u2014 which <code>GridSearchCV</code> uses internally \u2014 distributes work across cloud instances:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=3,\n    worker=sky.Worker(concurrency=4),\n    plugins=[sky.plugins.sklearn()],\n):\n    grid_search = GridSearchCV(\n        estimator=pipe,\n        param_grid=param_grid,\n        cv=5,\n        scoring=\"accuracy\",\n        n_jobs=-1,\n        verbose=3,\n    )\n    grid_search.fit(X_train, y_train)\n</code></pre> <p>Inside the pool block, every joblib <code>Parallel</code> call is intercepted and routed to the Skyward cluster. Each fit is serialized with cloudpickle, sent to a worker, executed, and the result returned. The <code>worker</code> parameter accepts a <code>Worker</code> dataclass that controls per-node execution \u2014 <code>Worker(concurrency=4)</code> means each node runs 4 fits simultaneously. With 3 nodes and <code>concurrency=4</code>, you get 12 parallel fits.</p> <p>The <code>sklearn</code> plugin registers the custom joblib backend on enter and restores the default on exit. The scikit-learn API is completely unchanged \u2014 <code>GridSearchCV</code>, <code>Pipeline</code>, <code>cross_val_score</code> all work as documented.</p>"},{"location":"guides/scikit-grid-search/#results","title":"Results","text":"<p>After the search completes, access results through the standard scikit-learn interface:</p> <pre><code>best_clf = grid_search.best_params_[\"clf\"]\nprint(f\"Best: {type(best_clf).__name__}, CV={grid_search.best_score_:.2%}\")\nprint(f\"Test: {grid_search.score(X_test, y_test):.2%}\")\n</code></pre> <p>The grid search object behaves exactly as it would in a local run \u2014 <code>best_params_</code>, <code>best_score_</code>, <code>cv_results_</code> are all populated. The only difference is that the 105 fits ran on a cluster instead of a single machine.</p>"},{"location":"guides/scikit-grid-search/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/09_scikit_grid_search.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>plugins=[sky.plugins.sklearn()]</code> replaces joblib's backend with a distributed one \u2014 <code>n_jobs=-1</code> uses all cloud workers.</li> <li>Standard scikit-learn API \u2014 <code>GridSearchCV</code>, <code>Pipeline</code>, <code>cross_val_score</code> work unchanged.</li> <li><code>worker=Worker(concurrency=N)</code> controls parallelism per node \u2014 total parallel fits = nodes x concurrency.</li> <li>Pipeline + param_grid \u2014 search over different estimators and their hyperparameters in one run.</li> </ul>"},{"location":"guides/streaming/","title":"Streaming","text":"<p>Most of the time, <code>@sky.compute</code> functions run remotely and return a single result \u2014 the entire return value is serialized, sent back over the network, and handed to the caller at once. This works well for most workloads, but some patterns don't fit: a function that produces millions of rows can't materialize them all in memory before sending; a training loop that yields metrics every epoch shouldn't wait until the last epoch to report; a pipeline that feeds data into a remote model shouldn't serialize the entire dataset upfront.</p> <p>Streaming solves this. A <code>@sky.compute</code> function that uses <code>yield</code> instead of <code>return</code> becomes a streaming computation \u2014 results flow back to the caller one at a time as they're produced, and the caller consumes them as a regular Python iterator. No special API, no callback registration, no async boilerplate. You write a generator, dispatch it with <code>&gt;&gt;</code>, and iterate.</p>"},{"location":"guides/streaming/#output-streaming","title":"Output streaming","text":"<p>The simplest form: the remote function <code>yield</code>s values instead of returning a single result. On the client side, <code>&gt;&gt;</code> returns an iterator instead of a value.</p> <pre><code>@sky.compute\ndef fibonacci(n: int):\n    \"\"\"Output streaming: yields Fibonacci numbers one at a time.\"\"\"\n    a, b = 0, 1\n    for i in range(n):\n        yield a\n        a, b = b, a + b\n</code></pre> <p>Dispatching this function returns something you can iterate over immediately \u2014 results arrive as the remote function produces them, not after it finishes:</p> <pre><code>for val in fibonacci(10) &gt;&gt; pool:\n    print(f\"  {val}\")\n</code></pre> <p>Under the hood, the worker detects that the function is a generator and creates a Casty <code>stream_producer</code> actor. As the generator yields values, they're pushed into the stream with backpressure \u2014 if the client consumes slowly, the producer pauses. On the client side, the stream is wrapped in a synchronous iterator (<code>_SyncSource</code>) that bridges the async Casty protocol to Python's <code>__iter__</code>/<code>__next__</code>. The SSH tunnel carries the stream elements as individual messages, so each value crosses the network as soon as it's produced.</p> <p>This means time-to-first-result scales with your function's first <code>yield</code>, not with the total computation time. A function that yields a progress update every epoch gives you live feedback from the first epoch onward.</p>"},{"location":"guides/streaming/#input-streaming","title":"Input streaming","text":"<p>The inverse pattern: instead of streaming results out, you stream data in. Annotate a parameter with <code>Iterator[T]</code>, and Skyward streams the argument to the worker incrementally instead of serializing it all at once.</p> <pre><code>@sky.compute\ndef running_mean(data: Iterator[float]) -&gt; list[float]:\n    \"\"\"Input streaming: consumes an iterator of floats incrementally.\"\"\"\n    total = 0.0\n    means = []\n    for i, x in enumerate(data, 1):\n        total += x\n        means.append(total / i)\n    return means\n</code></pre> <p>On the client side, pass any iterable \u2014 the elements are sent to the worker as a stream:</p> <pre><code>data = iter([1.0, 2.0, 3.0, 4.0, 5.0])\nmeans = running_mean(data) &gt;&gt; pool\nfor i, m in enumerate(means, 1):\n    print(f\"  after {i} values: {m:.2f}\")\n</code></pre> <p>The detection is based on the type annotation: Skyward inspects the function's type hints and identifies parameters annotated as <code>Iterator[T]</code>. When it finds one, it replaces the argument with a Casty stream \u2014 spawning a <code>stream_producer</code> on the client side, pumping elements from the local iterator in a background thread, and giving the worker a <code>_SyncSource</code> that consumes the stream as a regular <code>for x in data</code> loop.</p> <p>This is useful when the input data is large or lazily produced. Instead of serializing a 10GB dataset into a single cloudpickle blob, you can pass a generator that reads from disk chunk by chunk \u2014 each chunk crosses the network as a stream element, and the worker processes it as it arrives. Memory usage stays flat on both sides.</p>"},{"location":"guides/streaming/#bidirectional-streaming","title":"Bidirectional streaming","text":"<p>Combine both: a function that takes an <code>Iterator[T]</code> input and <code>yield</code>s results. Data flows in, transformed results flow out, and neither side materializes the full dataset:</p> <pre><code>@sky.compute\ndef moving_average(data: Iterator[float], window: int = 3):\n    \"\"\"Bidirectional: streams data in, yields moving averages out.\"\"\"\n    from collections import deque\n\n    buf: deque[float] = deque(maxlen=window)\n    for x in data:\n        buf.append(x)\n        yield sum(buf) / len(buf)\n</code></pre> <pre><code>data = iter([1.0, 2.0, 3.0, 4.0, 5.0, 6.0])\nfor avg in moving_average(data, window=3) &gt;&gt; pool:\n    print(f\"  {avg:.2f}\")\n</code></pre> <p>The client feeds values into the input stream, the worker consumes them one at a time, and each computed result is yielded back through the output stream. This is the streaming equivalent of a Unix pipe \u2014 data flows through the remote function without buffering the entire input or output.</p>"},{"location":"guides/streaming/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/13_streaming.py\n</code></pre> <p>What you learned:</p> <ul> <li>Output streaming \u2014 <code>@sky.compute</code> generators yield results incrementally; <code>&gt;&gt;</code> returns a synchronous iterator on the client side.</li> <li>Input streaming \u2014 Parameters annotated as <code>Iterator[T]</code> are streamed to the worker instead of serialized whole.</li> <li>Bidirectional \u2014 Combine both: stream data in with <code>Iterator[T]</code>, yield results out with <code>yield</code>. Neither side buffers the full dataset.</li> <li>Backpressure \u2014 Casty's stream protocol pauses the producer if the consumer falls behind, preventing unbounded memory growth.</li> <li>Zero API overhead \u2014 No special classes or protocols. Write a generator, annotate with <code>Iterator</code>, and the streaming machinery activates automatically.</li> </ul>"},{"location":"guides/torch-model-roundtrip/","title":"PyTorch model roundtrip","text":"<p>PyTorch's <code>nn.Module</code> is fully picklable \u2014 architecture and weights travel together through Python's serialization protocol. Skyward uses cloudpickle under the hood, which means you can send an untrained model to a remote worker, train it there, and get the trained model back. No <code>state_dict</code> files, no checkpoints, no manual save/load \u2014 the model object itself is the transport.</p> <p>This guide walks through the full cycle: build locally, train remotely, evaluate locally.</p>"},{"location":"guides/torch-model-roundtrip/#the-model","title":"The model","text":"<p>A standard <code>nn.Module</code> \u2014 nothing special required for serialization:</p> <pre><code>class MNISTClassifier(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.features = nn.Sequential(\n            nn.Linear(784, 256),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n        )\n        self.classifier = nn.Linear(128, 10)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.classifier(self.features(x))\n</code></pre> <p>When cloudpickle serializes this object, it captures both the class definition and the instance state (all parameter tensors). On the remote side, it reconstructs the exact same object with the exact same weights.</p>"},{"location":"guides/torch-model-roundtrip/#loading-data-locally","title":"Loading data locally","text":"<p>MNIST is loaded on the local machine and sent as tensors to the remote worker. The worker doesn't need <code>torchvision</code> \u2014 it only needs <code>torch</code> to work with the tensors it receives:</p> <pre><code>def load_mnist() -&gt; tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    \"\"\"Load MNIST and flatten images to 784-d vectors.\"\"\"\n    from torchvision import datasets, transforms\n\n    transform = transforms.ToTensor()\n    train_ds = datasets.MNIST(\"/tmp/mnist\", train=True, download=True, transform=transform)\n    test_ds = datasets.MNIST(\"/tmp/mnist\", train=False, download=True, transform=transform)\n\n    x_train = train_ds.data.float().view(-1, 784) / 255.0\n    y_train = train_ds.targets\n    x_test = test_ds.data.float().view(-1, 784) / 255.0\n    y_test = test_ds.targets\n    return x_train, y_train, x_test, y_test\n</code></pre> <p>The 60k training images (each 28x28) are flattened to 784-d vectors and normalized. Both <code>x_train</code> and <code>y_train</code> are regular tensors \u2014 cloudpickle handles them the same way it handles any Python object.</p>"},{"location":"guides/torch-model-roundtrip/#the-remote-training-function","title":"The remote training function","text":"<p>The <code>@sky.compute</code> function receives the model as an argument and returns it after training:</p> <pre><code>@sky.compute\ndef train(\n    model: nn.Module,\n    x: torch.Tensor,\n    y: torch.Tensor,\n    epochs: int,\n    lr: float,\n) -&gt; nn.Module:\n    \"\"\"Train the model on the remote worker and return it with learned weights.\"\"\"\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, TensorDataset\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = model.to(device)\n\n    loader = DataLoader(\n        TensorDataset(x.to(device), y.to(device)),\n        batch_size=64,\n        shuffle=True,\n    )\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    model.train()\n    for epoch in range(epochs):\n        total_loss = 0.0\n        correct = 0\n        total = 0\n\n        for batch_x, batch_y in loader:\n            optimizer.zero_grad()\n            output = model(batch_x)\n            loss = criterion(output, batch_y)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n            correct += (output.argmax(1) == batch_y).sum().item()\n            total += batch_y.size(0)\n\n        acc = 100.0 * correct / total\n        print(f\"  epoch {epoch + 1}/{epochs}: loss={total_loss / len(loader):.4f}, acc={acc:.1f}%\")\n\n    return model.cpu()\n</code></pre> <p>The type signature tells the story: <code>nn.Module</code> goes in, <code>nn.Module</code> comes out. The optimizer modifies the model's parameters in-place during training. The final <code>model.cpu()</code> ensures all tensors are on CPU before serialization \u2014 this matters when training on GPU, since CUDA tensors can't deserialize on a machine without a GPU.</p>"},{"location":"guides/torch-model-roundtrip/#pinning-the-torch-version","title":"Pinning the torch version","text":"<p>Torch tensors use pickle's <code>__reduce_ex__</code> protocol to serialize their raw storage. The binary format can change between torch versions \u2014 a tensor pickled with torch 2.10 may not deserialize correctly on torch 2.8 (or vice versa). Since the model travels both directions through cloudpickle, the local and remote torch versions must match:</p> <pre><code>TORCH_VERSION = torch.__version__.split(\"+\")[0]\n</code></pre> <pre><code>image=sky.Image(\n    pip=[f\"torch=={TORCH_VERSION}\"],\n    pip_indexes=[\n        sky.PipIndex(\n            url=\"https://download.pytorch.org/whl/cpu\",\n            packages=[\"torch\"],\n        ),\n    ],\n),\n</code></pre> <p><code>TORCH_VERSION</code> strips the build suffix (e.g. <code>+cpu</code>, <code>+cu128</code>) so the version pin works across wheel variants. The <code>PipIndex</code> scopes the PyTorch wheel index to the <code>torch</code> package only, preventing it from affecting other dependencies.</p>"},{"location":"guides/torch-model-roundtrip/#the-full-cycle","title":"The full cycle","text":"<p>Build the model locally, evaluate it (random accuracy), train remotely, evaluate again:</p> <pre><code>if __name__ == \"__main__\":\n    x_train, y_train, x_test, y_test = load_mnist()\n\n    model = MNISTClassifier()\n    print(f\"Model: {sum(p.numel() for p in model.parameters()):,} parameters\")\n    print(\"Before training:\")\n    evaluate(model, x_test, y_test)\n\n    with sky.ComputePool(\n        provider=sky.Container(),\n        vcpus=4,\n        memory_gb=2,\n        image=sky.Image(\n            pip=[f\"torch=={TORCH_VERSION}\"],\n            pip_indexes=[\n                sky.PipIndex(\n                    url=\"https://download.pytorch.org/whl/cpu\",\n                    packages=[\"torch\"],\n                ),\n            ],\n        ),\n    ) as pool:\n        print(\"\\nTraining remotely...\")\n        trained_model: nn.Module = train(model, x_train, y_train, epochs=10, lr=1e-3) &gt;&gt; pool\n\n    print(\"\\nAfter training:\")\n    evaluate(trained_model, x_test, y_test)\n</code></pre> <p>The untrained model starts at ~10% accuracy (random chance for 10 classes). After <code>&gt;&gt; pool</code> dispatches the training to the remote worker and returns the trained model, local evaluation shows the learned accuracy \u2014 proving the weights survived the roundtrip.</p>"},{"location":"guides/torch-model-roundtrip/#local-evaluation","title":"Local evaluation","text":"<p>The <code>evaluate</code> function runs on your local machine using the model that came back from the cloud:</p> <pre><code>@torch.no_grad()\ndef evaluate(model: nn.Module, x: torch.Tensor, y: torch.Tensor) -&gt; None:\n    \"\"\"Evaluate the trained model locally.\"\"\"\n    model.eval()\n    output = model(x)\n    predictions = output.argmax(1)\n    accuracy = 100.0 * (predictions == y).float().mean().item()\n\n    print(f\"  test samples : {len(y)}\")\n    print(f\"  test accuracy: {accuracy:.1f}%\")\n</code></pre> <p>No reconstruction, no <code>load_state_dict</code> \u2014 the returned object is a regular <code>MNISTClassifier</code> instance with trained weights, ready for inference.</p>"},{"location":"guides/torch-model-roundtrip/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/15_torch_model_roundtrip.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>nn.Module</code> is picklable \u2014 cloudpickle captures architecture + weights together, no manual serialization needed.</li> <li>Models as arguments and return values \u2014 send an untrained model in, get a trained model back via <code>&gt;&gt;</code>.</li> <li>Pin torch versions \u2014 the pickle format for tensors is version-sensitive; <code>TORCH_VERSION</code> keeps local and remote in sync.</li> <li><code>model.cpu()</code> before returning \u2014 ensures CUDA tensors don't break deserialization on CPU-only machines.</li> <li>Data stays local until dispatch \u2014 load datasets on your machine, send as tensors; the worker only needs <code>torch</code>.</li> </ul>"},{"location":"guides/using-accelerators/","title":"Using accelerators","text":"<p>Most ML workloads need GPUs. Skyward abstracts GPU selection across providers \u2014 you specify the hardware you need, and the provider finds the right instance type, resolves availability, and provisions it. This guide covers how to request specific accelerators, detect the hardware available at runtime, and understand the difference between string and typed accelerator specs.</p>"},{"location":"guides/using-accelerators/#requesting-a-gpu","title":"Requesting a GPU","text":"<p>Specify the accelerator when creating a pool:</p> <pre><code>with sky.ComputePool(\n    provider=sky.VastAI(),\n    accelerator=sky.accelerators.L40S(),\n    image=sky.Image(pip=[\"torch\"]),\n) as pool:\n</code></pre> <p>You can pass a plain string (<code>\"A100\"</code>, <code>\"T4\"</code>, <code>\"H100\"</code>) or use the typed factory functions under <code>sky.accelerators</code>. The factory functions carry catalog metadata \u2014 VRAM size, CUDA compatibility, form factor \u2014 and provide IDE autocomplete:</p> <pre><code># String \u2014 simple, works everywhere\nsky.ComputePool(provider=sky.AWS(), accelerator=\"A100\")\n\n# Factory function \u2014 type-safe, with catalog defaults\nsky.ComputePool(provider=sky.AWS(), accelerator=sky.accelerators.A100())\nsky.ComputePool(provider=sky.AWS(), accelerator=sky.accelerators.H100(count=4))\n</code></pre> <p>Both forms produce an <code>Accelerator</code> dataclass \u2014 a frozen specification with <code>name</code>, <code>memory</code>, <code>count</code>, and optional metadata. The factory functions look up defaults from an internal catalog, so <code>sky.accelerators.H100()</code> already knows it has 80GB of memory without you specifying it.</p> <p>The translation from a logical accelerator name to a provider-specific resource isn't a simple string match. An \"A100\" on AWS is a <code>p4d.24xlarge</code>, on RunPod it's a pod with a specific <code>gpuTypeId</code>, on VastAI it's a marketplace offer filtered by GPU model. The catalog centralizes this complexity so that the same <code>Accelerator</code> spec resolves correctly on any provider that supports it.</p>"},{"location":"guides/using-accelerators/#detecting-hardware-at-runtime","title":"Detecting hardware at runtime","text":"<p>Inside a <code>@sky.compute</code> function, <code>instance_info()</code> tells you what hardware is available:</p> <pre><code>@sky.compute\ndef gpu_info() -&gt; sky.InstanceInfo:\n    \"\"\"Return information about this instance.\"\"\"\n    return sky.instance_info()\n</code></pre> <p><code>InstanceInfo</code> includes the node index, cluster size, head status, and the number and type of accelerators. This is useful for conditional logic \u2014 running a GPU path when CUDA is available, falling back to CPU otherwise.</p>"},{"location":"guides/using-accelerators/#gpu-vs-cpu-benchmark","title":"GPU vs CPU benchmark","text":"<p>A matrix multiplication benchmark illustrates the GPU advantage. The function runs on the remote instance, where the accelerator is available:</p> <pre><code>@sky.compute\ndef matrix_benchmark(size: int) -&gt; dict:\n    \"\"\"Benchmark matrix multiplication on GPU vs CPU.\"\"\"\n    import time\n\n    import torch\n\n    a = torch.randn(size, size)\n    b = torch.randn(size, size)\n\n    start = time.perf_counter()\n    torch.matmul(a, b)\n    cpu_time = time.perf_counter() - start\n\n    if torch.cuda.is_available():\n        a_gpu = a.cuda()\n        b_gpu = b.cuda()\n        torch.matmul(a_gpu, b_gpu)\n        torch.cuda.synchronize()\n\n        start = time.perf_counter()\n        torch.matmul(a_gpu, b_gpu)\n        torch.cuda.synchronize()\n        gpu_time = time.perf_counter() - start\n\n        return {\"cpu\": cpu_time, \"gpu\": gpu_time, \"speedup\": cpu_time / gpu_time}\n\n    return {\"cpu\": cpu_time}\n</code></pre> <p>The first <code>torch.matmul</code> on GPU is a warmup call \u2014 it triggers CUDA kernel compilation, which is a one-time cost. After warmup, GPU matmul on a 4096x4096 matrix is typically 20-50x faster than CPU. The exact speedup depends on the GPU model, matrix size, and data type (fp32 vs fp16).</p> <p>Note that imports happen inside the function. This is intentional \u2014 the function runs on the remote worker, where <code>torch</code> is installed via the Image's <code>pip</code> field. Your local machine doesn't need torch installed.</p>"},{"location":"guides/using-accelerators/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/04_gpu_accelerators.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>accelerator</code> parameter requests specific GPU hardware \u2014 works as a string or typed factory function.</li> <li><code>sky.accelerators.*</code> provides catalog-backed specs with VRAM, CUDA version, and provider-specific resolution.</li> <li><code>instance_info()</code> detects hardware at runtime \u2014 node identity, accelerators, cluster metadata.</li> <li>Imports inside <code>@sky.compute</code> \u2014 remote dependencies don't need to be installed locally.</li> <li>GPU warmup \u2014 first CUDA kernel compilation is a one-time cost; benchmark after warmup for accurate numbers.</li> </ul>"},{"location":"guides/worker-executors/","title":"Worker executors","text":"<p>Every node in a Skyward pool runs a worker process that receives tasks and executes them. The <code>Worker</code> dataclass controls how that execution happens \u2014 specifically, whether tasks run as separate OS processes or as threads inside the worker. This choice determines whether CPU-bound Python code can use all available cores or is limited by the GIL.</p>"},{"location":"guides/worker-executors/#a-cpu-bound-task","title":"A CPU-bound task","text":"<p>Consider a tight numerical loop \u2014 pure Python, no C extensions, no I/O:</p> <pre><code>def cpu_burn(task_id: int) -&gt; dict:\n    \"\"\"CPU-intensive task: tight numerical loop for ~10 seconds.\"\"\"\n    start = monotonic()\n    total = 0.0\n    i = 0\n    while monotonic() - start &lt; 10:\n        total += (i ** 0.5) * ((i + 1) ** 0.5)\n        i += 1\n    elapsed = monotonic() - start\n    return {\"task_id\": task_id, \"iterations\": i, \"elapsed\": round(elapsed, 1)}\n</code></pre> <p>This is the worst case for the GIL: the Python interpreter never releases the lock, so only one thread can make progress at a time. On a 2-vCPU machine with <code>concurrency=2</code>, a thread-based executor will show ~50% CPU utilization \u2014 one core active, one idle.</p>"},{"location":"guides/worker-executors/#thread-executor-default","title":"Thread executor (default)","text":"<p>The thread executor runs tasks as threads inside the worker process. All threads share the same memory space and the same GIL:</p> <pre><code># Supports streaming, low overhead, ideal for I/O-bound and GIL-releasing workloads.\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    worker=sky.Worker(concurrency=2),  # executor=\"thread\" is the default\n    nodes=3,\n) as pool:\n    results = sky.gather(*(cpu_burn(i) for i in range(total)), stream=True)\n    for r in (results &gt;&gt; pool):\n        print(f\"[thread] Task {r['task_id']}: {r['iterations']:,} iters in {r['elapsed']}s\")\n\n# Process executor \u2014 each task runs in a separate OS process.\n</code></pre> <p>Threads are lightweight, support streaming (generator functions and iterator parameters), and work seamlessly with distributed collections. For most workloads \u2014 I/O-bound tasks, C extension heavy code (NumPy, PyTorch), and mixed workloads \u2014 the thread executor is the right choice. The <code>executor=\"thread\"</code> is the default, so <code>Worker(concurrency=2)</code> is equivalent.</p>"},{"location":"guides/worker-executors/#process-executor","title":"Process executor","text":"<p>The process executor runs each task in a separate OS process via <code>ProcessPoolExecutor</code>. Each process has its own Python interpreter and its own GIL, so CPU-bound work saturates all available cores:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    worker=sky.Worker(concurrency=2, executor=\"process\"),\n    nodes=3,\n) as pool:\n    results = sky.gather(*(cpu_burn(i) for i in range(total)), stream=True)\n    for r in (results &gt;&gt; pool):\n        print(f\"[process] Task {r['task_id']}: {r['iterations']:,} iters in {r['elapsed']}s\")\n</code></pre> <p>On a 2-vCPU instance with <code>concurrency=2</code>, this achieves ~100% CPU utilization \u2014 both cores fully active. Use this explicitly for pure-Python CPU-bound workloads where bypassing the GIL matters.</p>"},{"location":"guides/worker-executors/#trade-offs","title":"Trade-offs","text":"Process (<code>\"process\"</code>) Thread (<code>\"thread\"</code>) GIL Bypassed \u2014 each process has its own interpreter Shared \u2014 one thread runs at a time CPU-bound Full core utilization Limited to ~1 core I/O-bound Works, but heavier per-task overhead Lightweight, ideal for I/O waits Distributed collections Available (via IPC bridge) Available (shared memory with worker) Task isolation Full \u2014 crash in one task doesn't affect others Shared \u2014 exceptions propagate normally Serialization Task args and results cross process boundary (pickle) No extra serialization"},{"location":"guides/worker-executors/#when-to-use-each","title":"When to use each","text":"<p>Use thread (default) for:</p> <ul> <li>I/O-bound tasks (API calls, database queries, file processing)</li> <li>C extension heavy workloads that release the GIL (NumPy, PyTorch inference)</li> <li>Streaming tasks (generator functions, iterator parameters)</li> <li>Most ML training workloads (PyTorch, JAX, Keras release the GIL)</li> </ul> <p>Use process for:</p> <ul> <li>Pure-Python CPU-bound computation (tight loops, data transformation)</li> <li>Any workload where Python code dominates CPU time</li> <li>Tasks that benefit from crash isolation</li> </ul>"},{"location":"guides/worker-executors/#run-the-full-example","title":"Run the full example","text":"<pre><code>git clone https://github.com/gabfssilva/skyward.git\ncd skyward\nuv run python examples/guides/14_worker_executors.py\n</code></pre> <p>What you learned:</p> <ul> <li><code>Worker(executor=\"thread\")</code> (default) runs tasks as threads \u2014 lightweight, supports streaming, shares memory, but GIL-limited for pure-Python CPU-bound code.</li> <li><code>Worker(executor=\"process\")</code> runs tasks in separate OS processes \u2014 bypasses the GIL, full CPU utilization for compute-heavy work.</li> <li><code>concurrency</code> controls task slots per node \u2014 total parallelism = <code>nodes * concurrency</code>.</li> <li>Distributed collections work with both executors \u2014 the process executor uses an IPC bridge to proxy operations to the parent worker.</li> <li>Choose based on workload: process for CPU-bound, thread for I/O-bound.</li> </ul>"},{"location":"plugins/","title":"What are plugins?","text":"<p>Skyward's plugin system is the way you bring third-party frameworks into the compute pool. When you pass <code>plugins=[sky.plugins.torch()]</code> to a <code>ComputePool</code>, you are telling Skyward: install PyTorch on the remote workers, configure the distributed runtime before my function runs, and clean up when the worker stops. The plugin handles the environment setup, the lifecycle hooks, and the per-task wrapping \u2014 things you would otherwise do manually with <code>Image(pip=[...])</code>, environment variables, and boilerplate inside your <code>@sky.compute</code> functions.</p> <p>The key insight is that plugins operate at the pool level, not at the function level. A single plugin declaration on the pool affects every task dispatched to it. This is different from the decorator pattern you might be used to, where each function explicitly opts in to framework setup. With plugins, the pool is the unit of configuration: once you declare that a pool uses PyTorch with NCCL, every function dispatched to that pool gets PyTorch's distributed environment configured automatically.</p>"},{"location":"plugins/#the-plugin-dataclass","title":"The Plugin dataclass","text":"<p>A <code>Plugin</code> is a frozen dataclass with five optional hooks. Each hook corresponds to a different phase in the pool and worker lifecycle. You do not need to implement all five \u2014 most plugins use two or three.</p> <pre><code>@dataclass(frozen=True, slots=True)\nclass Plugin:\n    name: str\n    transform: ImageTransform | None = None\n    bootstrap: BootstrapFactory | None = None\n    decorate: TaskDecorator | None = None\n    around_app: AppLifecycle | None = None\n    around_client: ClientLifecycle | None = None\n</code></pre> <p>The hooks are:</p> <p><code>transform</code> modifies the <code>Image</code> before bootstrap. It receives the current <code>Image</code> and the <code>Cluster</code> metadata, and returns a new <code>Image</code> with additional pip packages, pip indexes, environment variables, or apt packages. This is how plugins install their dependencies on the remote worker. For example, the <code>torch</code> plugin appends <code>\"torch\"</code> to <code>pip</code> and adds PyTorch's CUDA wheel index. The <code>keras</code> plugin appends <code>\"keras\"</code> and sets <code>KERAS_BACKEND</code> in the environment. Since <code>Image</code> is a frozen dataclass, the transform returns a new copy via <code>replace()</code> \u2014 it never mutates the original.</p> <p><code>bootstrap</code> injects shell operations after the standard bootstrap phases (apt, pip, etc.). It receives the <code>Cluster</code> and returns a tuple of shell ops. The <code>huggingface</code> plugin uses this to run <code>huggingface-cli login</code> after pip packages are installed, so the worker is authenticated before any task runs. The <code>mps</code> plugin uses it to start the NVIDIA MPS daemon.</p> <p><code>decorate</code> wraps each <code>@sky.compute</code> function at execution time on the remote worker. It is a classic Python decorator: it takes a function and returns a function. This is for per-task logic that must run every time a function executes \u2014 things like logging, metrics collection, or framework-specific wrappers that depend on each call's arguments.</p> <p><code>around_app</code> is a context manager that runs once per worker process. It receives an <code>InstanceInfo</code> and returns a context manager. The context is entered when the first task arrives and stays active for the lifetime of the worker. This is designed for one-time, process-wide initialization \u2014 things that must happen exactly once and persist. The <code>torch</code> plugin uses this for <code>dist.init_process_group()</code>, the <code>jax</code> plugin for <code>jax.distributed.initialize()</code>, the <code>keras</code> plugin for <code>DataParallel</code> distribution setup, and the <code>cuml</code> plugin for <code>cuml.accel.install()</code>. All are irreversible, process-global operations that should not be repeated per task.</p> <p>The state module (<code>skyward.plugins.state</code>) tracks which <code>around_app</code> hooks have been entered. It stores the context managers in a module-level dictionary and checks before entering \u2014 if the key already exists, it is a no-op. This makes the hook idempotent: even if multiple tasks execute on the same worker, each <code>around_app</code> is entered exactly once.</p> <p><code>around_client</code> is a context manager that runs on the client side, not the worker. It receives the <code>ComputePool</code> and the <code>Cluster</code>, and wraps the pool's entire active lifetime. The <code>joblib</code> and <code>sklearn</code> plugins use this to register the <code>SkywardBackend</code> as joblib's parallel backend, so that any <code>Parallel(n_jobs=-1)</code> call inside the <code>with</code> block dispatches work to the cluster instead of local processes.</p>"},{"location":"plugins/#builder-api","title":"Builder API","text":"<p>You can construct plugins using the builder pattern instead of passing all hooks to the constructor:</p> <pre><code>plugin = (\n    Plugin.create(\"my-plugin\")\n    .with_image_transform(lambda img, cluster: replace(img, pip=(*img.pip, \"my-lib\")))\n    .with_decorator(my_decorator)\n    .with_around_app(my_lifecycle)\n)\n</code></pre> <p>Each <code>.with_*</code> method returns a new <code>Plugin</code> instance (immutable \u2014 uses <code>replace()</code>). This is how the built-in plugins are implemented internally: the factory function (e.g., <code>sky.plugins.torch()</code>) defines the hooks as closures and chains them together with the builder.</p>"},{"location":"plugins/#how-hooks-execute","title":"How hooks execute","text":"<p>The hooks run at different points in the pool lifecycle, and the order matters.</p> <p>When the pool starts (<code>ComputePool.__enter__</code>):</p> <ol> <li><code>transform</code> hooks run first, in plugin order. Each transform receives the image returned by the previous one. The final image is used to generate the bootstrap script.</li> <li><code>bootstrap</code> hooks run after the standard bootstrap phases complete on each worker. The ops are appended in plugin order.</li> <li><code>around_client</code> hooks are entered on the client, in plugin order.</li> </ol> <p>When a task executes on a worker:</p> <ol> <li><code>around_app</code> hooks are lazily entered on first task execution (idempotent \u2014 skipped if already active).</li> <li><code>decorate</code> hooks wrap the function. If multiple plugins have decorators, they are chained: the first plugin's decorator is outermost, the last is innermost. The chaining uses <code>functools.reduce</code> over <code>reversed(decorators)</code>, so the first plugin listed in <code>plugins=[...]</code> runs first and the last runs last.</li> </ol> <p>When the pool stops (<code>ComputePool.__exit__</code>):</p> <ol> <li><code>around_client</code> contexts are exited in reverse order.</li> <li><code>around_app</code> contexts are exited in reverse order when the worker process shuts down.</li> </ol>"},{"location":"plugins/#plugin-composition","title":"Plugin composition","text":"<p>Plugins compose naturally because each hook is independent. You can stack multiple plugins and their effects combine:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"A100\",\n    nodes=4,\n    plugins=[\n        sky.plugins.torch(backend=\"nccl\"),\n        sky.plugins.huggingface(token=\"hf_xxx\"),\n    ],\n) as pool:\n    train() &gt;&gt; pool\n</code></pre> <p>The <code>torch</code> plugin adds PyTorch to pip and initializes DDP via <code>around_app</code>. The <code>huggingface</code> plugin adds transformers, datasets, and tokenizers to pip, sets <code>HF_TOKEN</code>, and runs <code>huggingface-cli login</code>. Their image transforms compose (PyTorch packages + HuggingFace packages), and their <code>around_app</code> hooks are entered independently in plugin order.</p> <p>Order can matter. When using Keras with JAX, the JAX plugin should come first because its <code>around_app</code> initializes the distributed runtime that Keras's <code>decorate</code> depends on:</p> <pre><code>plugins=[sky.plugins.jax(), sky.plugins.keras(backend=\"jax\")]\n</code></pre> <p>The JAX plugin's <code>around_app</code> calls <code>jax.distributed.initialize()</code>, and Keras's <code>around_app</code> calls <code>keras.distribution.set_distribution(DataParallel(...))</code>. The distribution setup needs JAX's device mesh to already be visible, so JAX must initialize first. Since <code>around_app</code> hooks are entered in plugin order, listing JAX first ensures the correct sequence.</p>"},{"location":"plugins/#built-in-plugins","title":"Built-in plugins","text":"<p>Skyward ships with eight plugins:</p> Plugin Primary Hooks Purpose <code>torch</code> <code>transform</code>, <code>around_app</code> PyTorch installation and DDP initialization <code>jax</code> <code>transform</code>, <code>around_app</code> JAX installation and distributed initialization <code>keras</code> <code>transform</code>, <code>around_app</code> Keras backend configuration and DataParallel <code>huggingface</code> <code>transform</code>, <code>bootstrap</code> Transformers, datasets, tokenizers, and auth <code>joblib</code> <code>transform</code>, <code>around_client</code> Distributed joblib parallel backend <code>sklearn</code> <code>transform</code>, <code>around_client</code> Scikit-learn with distributed joblib <code>cuml</code> <code>transform</code>, <code>around_app</code> GPU-accelerated scikit-learn via RAPIDS cuML <code>mps</code> <code>transform</code>, <code>bootstrap</code> NVIDIA Multi-Process Service for GPU sharing"},{"location":"plugins/#custom-plugins","title":"Custom plugins","text":"<p>Building a custom plugin follows the same pattern as the built-in ones. Define your hooks as functions, then chain them with the builder:</p> <pre><code>from dataclasses import replace\nfrom skyward.plugins import Plugin\n\ndef my_framework() -&gt; Plugin:\n    def transform(image, cluster):\n        return replace(image, pip=(*image.pip, \"my-framework\"))\n\n    def decorate(fn):\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            setup_my_framework()\n            return fn(*args, **kwargs)\n        return wrapper\n\n    return (\n        Plugin.create(\"my-framework\")\n        .with_image_transform(transform)\n        .with_decorator(decorate)\n    )\n</code></pre> <p>Use it like any built-in plugin:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    plugins=[my_framework()],\n) as pool:\n    my_task() &gt;&gt; pool\n</code></pre>"},{"location":"plugins/#next-steps","title":"Next steps","text":"<ul> <li>PyTorch \u2014 DDP initialization and CUDA wheel management</li> <li>JAX \u2014 Distributed initialization with <code>around_app</code></li> <li>Keras \u2014 Backend-agnostic training with DataParallel</li> <li>Distributed Training \u2014 How plugins fit into multi-node training</li> <li>Getting Started \u2014 First steps with Skyward</li> </ul>"},{"location":"plugins/cuml/","title":"cuML","text":"<p>scikit-learn is CPU-only. For large datasets \u2014 tens of thousands of samples, hundreds of features \u2014 algorithms like RandomForest, KNN, DBSCAN, and PCA become bottlenecks. Cross-validation and hyperparameter search multiply the problem: a 5-fold grid search over 20 candidates means 100 fits, each one CPU-bound. On a 64-core machine, this is tolerable. On a laptop, it is hours.</p> <p>NVIDIA cuML provides GPU-backed implementations of popular scikit-learn estimators. The API is the same \u2014 <code>RandomForestClassifier</code>, <code>KMeans</code>, <code>PCA</code>, <code>cross_val_score</code> \u2014 but the computation runs on GPU with speedups of 10x to 175x depending on the algorithm and dataset size. cuML's zero-code-change acceleration mode goes further: you write standard scikit-learn code with standard scikit-learn imports, and cuML intercepts the calls at runtime, routing them to GPU transparently.</p> <p>Skyward's <code>cuml</code> plugin makes this practical without a local GPU. It installs the cuML package with the correct CUDA variant, configures the NVIDIA pip index, and activates the zero-code-change acceleration on the remote worker. Your function imports <code>sklearn</code>, calls <code>sklearn</code> APIs, and cuML handles the rest.</p>"},{"location":"plugins/cuml/#what-it-does","title":"What it does","text":"<p>Image transform \u2014 Appends <code>cuml-cu12</code> (or the CUDA variant you specify) to the worker's pip dependencies and adds the NVIDIA pip index (<code>https://pypi.nvidia.com</code>) configured for that package. The RAPIDS packages are hosted on NVIDIA's own index, not PyPI, so the plugin handles the index configuration that you would otherwise need to set up manually in the <code>Image</code>.</p> <p>Worker lifecycle (<code>around_app</code>) \u2014 When the worker starts, the plugin calls <code>cuml.accel.install()</code>. This is cuML's zero-code-change acceleration entry point. It monkey-patches the scikit-learn namespace so that <code>from sklearn.ensemble import RandomForestClassifier</code> returns cuML's GPU implementation instead of scikit-learn's CPU one. The patching happens once, at worker startup, before any task runs. Every task on that worker benefits from it.</p>"},{"location":"plugins/cuml/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>cuda</code> <code>str</code> <code>\"cu12\"</code> CUDA version suffix. Determines which cuML package variant to install (e.g. <code>cuml-cu12</code>). Must match the CUDA version on the worker's GPU. <p>The default <code>\"cu12\"</code> works with CUDA 12.x, which covers most modern NVIDIA GPUs and cloud instances. If your instances run CUDA 11.x, use <code>cuda=\"cu11\"</code>.</p>"},{"location":"plugins/cuml/#usage","title":"Usage","text":""},{"location":"plugins/cuml/#basic-gpu-accelerated-training","title":"Basic GPU-accelerated training","text":"<p>The simplest case: a single-node GPU training with standard scikit-learn code:</p> <pre><code>import skyward as sky\n\n\n@sky.compute\ndef train_on_gpu(n_samples: int) -&gt; dict:\n    from time import perf_counter\n\n    import numpy as np\n    from sklearn.datasets import fetch_openml\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import cross_val_score\n\n    X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n    X = (X[:n_samples] / 255.0).astype(np.float32)\n    y = y[:n_samples].astype(np.int32)\n\n    clf = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=42)\n    start = perf_counter()\n    scores = cross_val_score(clf, X, y, cv=5, n_jobs=-1)\n    elapsed = perf_counter() - start\n\n    return {\"accuracy\": scores.mean(), \"time\": elapsed}\n\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"L4\",\n    nodes=1,\n    plugins=[\n        sky.plugins.cuml(),\n        sky.plugins.sklearn(),\n    ],\n) as pool:\n    result = train_on_gpu(50000) &gt;&gt; pool\n    print(f\"Accuracy: {result['accuracy']:.2%}, Time: {result['time']:.1f}s\")\n</code></pre> <p>The <code>sklearn</code> plugin installs scikit-learn and joblib; the <code>cuml</code> plugin installs cuML and activates GPU acceleration. The function uses only scikit-learn imports \u2014 <code>RandomForestClassifier</code>, <code>cross_val_score</code> \u2014 and cuML transparently routes them to GPU.</p> <p>The data is loaded on the remote worker (<code>fetch_openml</code> downloads from the internet), avoiding the need to serialize and ship large arrays over the SSH tunnel. This is a general best practice for data-heavy workloads.</p>"},{"location":"plugins/cuml/#combining-with-distributed-joblib","title":"Combining with distributed Joblib","text":"<p>cuML accelerates individual estimator operations on GPU. The sklearn plugin distributes parallel operations across the cluster. Together, each parallel task (e.g., each fold of cross-validation) runs on GPU:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"L4\",\n    nodes=4,\n    worker=sky.Worker(concurrency=2),\n    plugins=[\n        sky.plugins.cuml(),\n        sky.plugins.sklearn(),\n    ],\n) as pool:\n    result = run_grid_search() &gt;&gt; pool\n</code></pre> <p>Here, <code>GridSearchCV(n_jobs=-1)</code> distributes fits across the 4-node cluster, and each fit runs on GPU thanks to cuML. This is particularly effective for large grid searches where both the individual fits and the number of candidates are expensive.</p>"},{"location":"plugins/cuml/#without-the-sklearn-plugin","title":"Without the sklearn plugin","text":"<p>If your function does not use scikit-learn's <code>n_jobs</code> parallelism, you can use the <code>cuml</code> plugin alone:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"L4\",\n    nodes=1,\n    plugins=[sky.plugins.cuml()],\n    image=sky.Image(pip=[\"scikit-learn\"]),\n) as pool:\n    result = train_on_gpu(50000) &gt;&gt; pool\n</code></pre> <p>You need to add <code>scikit-learn</code> to the image manually since the <code>cuml</code> plugin does not install it. The <code>cuml</code> plugin only provides the GPU acceleration layer; the <code>sklearn</code> plugin provides the library itself and the distributed joblib backend.</p>"},{"location":"plugins/cuml/#requirements","title":"Requirements","text":"<p>cuML requires an NVIDIA GPU. The plugin is only useful with GPU-equipped instances \u2014 <code>accelerator=\"L4\"</code>, <code>accelerator=\"T4\"</code>, <code>accelerator=\"A100\"</code>, etc. Using it on a CPU-only instance will either fail at import time or silently fall through to CPU scikit-learn.</p> <p>The CUDA version on the worker must be compatible with the <code>cuda</code> parameter. The default <code>\"cu12\"</code> requires CUDA 12.x. Most cloud GPU instances ship with CUDA 12 by default.</p>"},{"location":"plugins/cuml/#next-steps","title":"Next steps","text":"<ul> <li>cuML GPU Acceleration guide \u2014 CPU vs GPU comparison with benchmarks</li> <li>sklearn plugin \u2014 Distributed scikit-learn with the joblib backend</li> <li>Scikit Grid Search guide \u2014 Distributed hyperparameter search</li> <li>What are Plugins? \u2014 How the plugin system works</li> </ul>"},{"location":"plugins/huggingface/","title":"HuggingFace","text":"<p>The HuggingFace ecosystem \u2014 transformers, datasets, tokenizers \u2014 is the dominant interface for working with pre-trained language models. Fine-tuning a model from the Hub is a well-understood workflow: download a checkpoint, tokenize a dataset, configure the Trainer, and run. The friction is not the code but the environment. You need a GPU, you need the right CUDA version, you need the HuggingFace libraries installed, and if the model is gated, you need to authenticate before anything downloads.</p> <p>Skyward's <code>huggingface</code> plugin handles all of this at the pool level. It installs the core HuggingFace libraries on the worker, sets the <code>HF_TOKEN</code> environment variable, and runs <code>huggingface-cli login</code> during bootstrap so that gated model downloads work without any manual intervention inside your compute function. Your function just calls <code>from_pretrained()</code> and it works \u2014 even for gated models like Llama or Mistral.</p>"},{"location":"plugins/huggingface/#what-it-does","title":"What it does","text":"<p>Image transform \u2014 Appends <code>transformers</code>, <code>datasets</code>, and <code>tokenizers</code> to the worker's pip dependencies and sets <code>HF_TOKEN</code> in the environment. This means the libraries are installed during bootstrap and the token is available to every process on the worker. You do not need to specify these packages in the <code>Image</code> yourself, and you do not need to call <code>huggingface_hub.login()</code> inside your function.</p> <p>Bootstrap \u2014 After the base environment is set up, the plugin runs <code>huggingface-cli login --token $HF_TOKEN</code>. This writes the token to the Hub's local credential store (<code>~/.cache/huggingface/token</code>), which is where <code>from_pretrained()</code> looks when downloading gated models. The login happens once during instance bootstrap, not on every task.</p> <p>Together, these two hooks mean that by the time your <code>@sky.compute</code> function runs, the worker has the HuggingFace libraries installed, the CLI authenticated, and the token in the environment. Your function can focus on the actual ML work.</p>"},{"location":"plugins/huggingface/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>token</code> <code>str</code> (required) HuggingFace API token. Used for <code>HF_TOKEN</code> env var and CLI login. Required for gated models; recommended for all usage to avoid rate limits. <p>The token is passed as a plain string. It ends up in the worker's environment as <code>HF_TOKEN</code> and in the Hub's credential store via <code>huggingface-cli login</code>. If you are working with gated models (Llama, Mistral, Gemma), you must accept the model's license on the Hub before the token will grant access.</p>"},{"location":"plugins/huggingface/#how-it-works","title":"How it works","text":"<p>When <code>ComputePool.__enter__</code> runs, the plugin's image transform modifies the <code>Image</code> before bootstrap script generation:</p> <pre><code>Image(\n    pip=(*existing_pip, \"transformers\", \"datasets\", \"tokenizers\"),\n    env={**existing_env, \"HF_TOKEN\": token},\n)\n</code></pre> <p>The bootstrap script generator then picks up these additions: <code>uv</code> installs the three packages, and the environment variable is exported in the shell profile. After the base bootstrap completes, the plugin's bootstrap hook runs a single command:</p> <pre><code>huggingface-cli login --token $HF_TOKEN\n</code></pre> <p>This writes the token to disk. From that point on, any HuggingFace library call that needs authentication \u2014 <code>AutoModel.from_pretrained(\"meta-llama/...\")</code>, <code>load_dataset(\"private/dataset\")</code> \u2014 can find the credentials without any explicit login call in your code.</p>"},{"location":"plugins/huggingface/#usage","title":"Usage","text":""},{"location":"plugins/huggingface/#single-node-fine-tuning","title":"Single-node fine-tuning","text":"<p>The most common pattern is single-node fine-tuning with the Trainer API. The plugin handles dependencies and authentication; HuggingFace's Trainer handles device placement:</p> <pre><code>import skyward as sky\n\n\n@sky.compute\ndef finetune(model_name: str, epochs: int) -&gt; dict:\n    from datasets import load_dataset\n    from transformers import (\n        AutoModelForSequenceClassification,\n        AutoTokenizer,\n        Trainer,\n        TrainingArguments,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name, num_labels=2,\n    )\n\n    dataset = load_dataset(\"imdb\")\n    train = dataset[\"train\"].map(\n        lambda x: tokenizer(x[\"text\"], truncation=True, max_length=256),\n        batched=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=TrainingArguments(\n            output_dir=\"/tmp/finetune\",\n            num_train_epochs=epochs,\n            per_device_train_batch_size=16,\n            fp16=True,\n        ),\n        train_dataset=train,\n    )\n\n    trainer.train()\n    return trainer.evaluate()\n\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=1,\n    accelerator=\"A100\",\n    plugins=[sky.plugins.huggingface(token=\"hf_...\")],\n) as pool:\n    result = finetune(\"distilbert-base-uncased\", epochs=3) &gt;&gt; pool\n    print(result)\n</code></pre> <p>Notice that the function imports <code>transformers</code> and <code>datasets</code> inside the function body. This is deliberate: the function is serialized with cloudpickle and sent to the remote worker, where the imports resolve against the worker's environment. You do not need <code>transformers</code> installed locally.</p> <p>The Trainer handles device placement internally \u2014 it detects the GPU and moves the model there, enables fp16 when appropriate, and manages the training loop. For a single node, no distributed coordination is needed.</p>"},{"location":"plugins/huggingface/#multi-node-distributed-training","title":"Multi-node distributed training","text":"<p>For larger models or faster iteration, distribute training across multiple nodes. The HuggingFace Trainer integrates with PyTorch's DistributedDataParallel when the process group is initialized. Combine the <code>huggingface</code> plugin with the <code>torch</code> plugin:</p> <pre><code>@sky.compute\ndef distributed_finetune(model_name: str) -&gt; dict:\n    from datasets import load_dataset\n    from transformers import (\n        AutoModelForCausalLM,\n        AutoTokenizer,\n        Trainer,\n        TrainingArguments,\n    )\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n\n    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n    # Trainer handles data sharding across DDP ranks\n\n    trainer = Trainer(\n        model=model,\n        args=TrainingArguments(\n            output_dir=\"/tmp/distributed-finetune\",\n            num_train_epochs=1,\n            per_device_train_batch_size=4,\n            gradient_accumulation_steps=4,\n            fp16=True,\n            ddp_find_unused_parameters=False,\n        ),\n        train_dataset=dataset[\"train\"],\n        tokenizer=tokenizer,\n    )\n\n    trainer.train()\n    return trainer.evaluate()\n\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=4,\n    accelerator=\"A100\",\n    plugins=[\n        sky.plugins.torch(backend=\"nccl\"),\n        sky.plugins.huggingface(token=\"hf_...\"),\n    ],\n) as pool:\n    results = distributed_finetune(\"gpt2\") @ pool\n</code></pre> <p>The <code>torch</code> plugin initializes the process group before your function runs \u2014 it sets <code>MASTER_ADDR</code>, <code>WORLD_SIZE</code>, <code>RANK</code>, and calls <code>init_process_group()</code>. The Trainer detects the initialized process group and automatically wraps the model with <code>DistributedDataParallel</code>, shards the data across ranks, and synchronizes gradients. The <code>huggingface</code> plugin contributes the libraries and authentication. Plugin order in the list does not affect behavior \u2014 each plugin's hooks run at their respective lifecycle points.</p>"},{"location":"plugins/huggingface/#inference-with-pipelines","title":"Inference with pipelines","text":"<p>For lighter workloads like batch inference, the plugin is equally useful \u2014 it ensures the libraries and authentication are in place:</p> <pre><code>@sky.compute\ndef classify(texts: list[str]) -&gt; list[dict]:\n    from transformers import pipeline\n\n    classifier = pipeline(\n        \"sentiment-analysis\",\n        model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n        device=0,\n    )\n    return classifier(texts)\n\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=1,\n    accelerator=\"T4\",\n    plugins=[sky.plugins.huggingface(token=\"hf_...\")],\n) as pool:\n    predictions = classify([\"Great movie!\", \"Terrible film.\"]) &gt;&gt; pool\n</code></pre>"},{"location":"plugins/huggingface/#next-steps","title":"Next steps","text":"<ul> <li>HuggingFace Fine-tuning guide \u2014 Complete fine-tuning walkthrough with dataset preparation, training, and evaluation</li> <li>PyTorch Distributed \u2014 How DDP works under the hood with the <code>torch</code> plugin</li> <li>What are Plugins? \u2014 How the plugin system works</li> </ul>"},{"location":"plugins/jax/","title":"JAX","text":"<p>JAX treats multiple machines as one big device mesh. After a single call to <code>jax.distributed.initialize()</code>, every node sees every accelerator across the cluster \u2014 <code>jax.devices()</code> returns the full set, and <code>jit</code> with sharding constraints distributes computation over it. The catch is that every process must call <code>initialize()</code> with the coordinator address, cluster size, and its own index, exactly once, before any distributed operation. <code>sky.plugins.jax()</code> takes care of this: it installs JAX with the correct CUDA wheels on the remote workers and calls <code>jax.distributed.initialize()</code> at startup with the topology from <code>instance_info()</code>.</p>"},{"location":"plugins/jax/#parameters","title":"Parameters","text":"<p>The plugin accepts a single parameter:</p> Parameter Type Default Description <code>cuda</code> <code>str</code> <code>\"cu124\"</code> CUDA version suffix for the JAX wheel <p>The <code>cuda</code> value becomes the extra specifier in the pip requirement \u2014 <code>jax[cu124]</code> \u2014 and controls which CUDA-specific wheels are pulled from Google's JAX release index. If your cluster runs CUDA 12.4, the default works. For other CUDA versions, pass the matching suffix (e.g., <code>\"cu121\"</code> for CUDA 12.1).</p>"},{"location":"plugins/jax/#how-it-works","title":"How it works","text":""},{"location":"plugins/jax/#image-transform","title":"Image transform","text":"<p>The <code>transform</code> hook modifies the worker's <code>Image</code> before bootstrap. It does two things:</p> <ol> <li>Appends <code>jax[{cuda}]</code> to the pip dependency list, where <code>{cuda}</code> is the configured CUDA suffix.</li> <li>Adds Google's JAX CUDA release index (<code>https://storage.googleapis.com/jax-releases/jax_cuda_releases.html</code>) as a pip index, scoped to the <code>jax</code> and <code>jaxlib</code> packages.</li> </ol> <p>This means JAX and its CUDA bindings are installed from Google's official release channel during worker bootstrap. You do not need JAX installed locally \u2014 the plugin adds it to the remote environment.</p>"},{"location":"plugins/jax/#worker-lifecycle-around_app","title":"Worker lifecycle (<code>around_app</code>)","text":"<p>The <code>around_app</code> hook is a context manager that runs once when the worker process starts, before any task executes. It calls:</p> <pre><code>jax.distributed.initialize(\n    coordinator_address=f\"{info.head_addr}:{info.head_port}\",\n    num_processes=info.total_nodes,\n    process_id=info.node,\n)\n</code></pre> <p>The values come from <code>instance_info()</code> \u2014 Skyward's runtime API that exposes the cluster topology to each worker. <code>head_addr</code> is the private IP of node 0 (the coordinator), <code>head_port</code> is the coordination port, <code>total_nodes</code> is the cluster size, and <code>node</code> is this process's index (0 through N-1).</p> <p>After this call returns, JAX's global state is initialized. Every call to <code>jax.devices()</code> returns the full set of accelerators across all nodes, and JAX's compiler can partition computation across the entire mesh.</p>"},{"location":"plugins/jax/#usage","title":"Usage","text":"<pre><code>import skyward as sky\n\n@sky.compute\ndef train():\n    import jax\n    import jax.numpy as jnp\n\n    # jax.distributed is already initialized\n    # all devices across all nodes are visible\n    devices = jax.devices()\n    print(f\"Total devices: {len(devices)}\")\n\n    # distributed computation works out of the box\n    mesh = jax.sharding.Mesh(jax.devices(), axis_names=(\"devices\",))\n    ...\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"A100\",\n    nodes=4,\n    plugins=[sky.plugins.jax()],\n) as pool:\n    results = train() @ pool\n</code></pre> <p>The <code>@</code> operator broadcasts the function to all nodes. Each node executes <code>train()</code>, and by the time the function body runs, <code>jax.distributed.initialize()</code> has already been called by the plugin. The function sees the full device mesh and can use JAX's sharding primitives to partition computation.</p>"},{"location":"plugins/jax/#combining-with-keras","title":"Combining with Keras","text":"<p>JAX is the recommended backend for multi-node Keras training. When using Keras with JAX, stack both plugins:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"A100\",\n    nodes=2,\n    plugins=[\n        sky.plugins.jax(),\n        sky.plugins.keras(backend=\"jax\"),\n    ],\n) as pool:\n    results = train() @ pool\n</code></pre> <p>Order matters here. The JAX plugin's <code>around_app</code> initializes the distributed runtime, and the Keras plugin sets <code>KERAS_BACKEND=jax</code> so Keras uses JAX as its computation backend. Together, they give you multi-node Keras training where JAX handles the distributed device mesh and Keras provides the high-level model API.</p> <p>The Keras Training guide walks through a complete MNIST example using this combination.</p>"},{"location":"plugins/jax/#next-steps","title":"Next steps","text":"<ul> <li>Keras Training \u2014 JAX + Keras on multiple GPUs</li> <li>What are Plugins? \u2014 How the plugin system works</li> <li>PyTorch Distributed \u2014 The PyTorch equivalent for comparison</li> </ul>"},{"location":"plugins/joblib/","title":"Joblib","text":"<p>joblib's <code>Parallel</code> is how Python parallelizes embarrassingly parallel work. scikit-learn uses it for <code>GridSearchCV</code>, <code>cross_val_score</code>, and any estimator with <code>n_jobs</code>. NLTK uses it. Countless data processing pipelines use <code>Parallel(n_jobs=-1)(delayed(fn)(x) for x in data)</code> as the standard idiom for local parallelism. The limitation is that \"all available workers\" means \"all cores on this machine.\" On a laptop, that is 8 or 16. On an expensive workstation, maybe 64. For large hyperparameter searches or batch processing jobs, this is the bottleneck.</p> <p>Skyward's <code>joblib</code> plugin replaces joblib's execution backend with a distributed one. When the plugin is active, <code>n_jobs=-1</code> means \"all workers in the cluster\" \u2014 not local cores. The joblib API is completely unchanged. <code>Parallel</code>, <code>delayed</code>, <code>n_jobs</code> all work as documented. The difference is that each task is serialized with cloudpickle, sent to a remote worker over SSH, executed there, and the result returned. No code changes beyond the pool configuration.</p>"},{"location":"plugins/joblib/#what-it-does","title":"What it does","text":"<p>Image transform \u2014 Appends <code>joblib</code> (optionally at a pinned version) to the worker's pip dependencies. This ensures the remote workers have joblib installed for deserialization.</p> <p>Client lifecycle (<code>around_client</code>) \u2014 This is where the real work happens. When the pool is entered, the plugin does three things:</p> <ol> <li> <p>Registers <code>SkywardBackend</code> as a custom joblib parallel backend. This is a subclass of <code>ParallelBackendBase</code> that replaces joblib's default thread/process backends with one that dispatches tasks to the Skyward cluster.</p> </li> <li> <p>Strips non-stdlib warning filters from <code>warnings.filters</code>. This is a subtle but important fix: sklearn's <code>Parallel</code> pickles the current <code>warnings.filters</code> list into every task payload via cloudpickle. If your local environment has warning filters from third-party packages (pytest, cloud SDKs, monitoring libraries), those filters reference module classes that may not exist on the worker. Deserialization would fail with <code>ModuleNotFoundError</code>. The plugin removes any filter whose category class comes from outside the standard library, keeping only safe builtins like <code>DeprecationWarning</code> and <code>FutureWarning</code>. Third-party packages installed on the worker will re-inject their own filters at import time.</p> </li> <li> <p>Enters the <code>parallel_backend(\"skyward\")</code> context manager, which tells joblib to route all <code>Parallel</code> calls to the Skyward backend for the duration of the pool block. When the pool exits, the default backend is restored.</p> </li> </ol>"},{"location":"plugins/joblib/#how-skywardbackend-works","title":"How SkywardBackend works","text":"<p><code>SkywardBackend</code> is a joblib backend that serializes each task with cloudpickle, wraps it in a <code>@sky.compute</code> function, and dispatches it to the cluster. Each joblib task becomes a Skyward compute task, sent to a remote worker over SSH. The serialization overhead is minimal \u2014 cloudpickle is fast, and payloads are compressed with lz4 on the wire.</p> <p>Effective parallelism is <code>nodes * concurrency</code>. If you have 4 nodes with <code>Worker(concurrency=10)</code>, joblib sees 40 available workers. <code>n_jobs=-1</code> uses all of them. <code>n_jobs=20</code> would use 20.</p>"},{"location":"plugins/joblib/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>version</code> <code>str \\| None</code> <code>None</code> Specific joblib version to install (e.g. <code>\"1.4.0\"</code>). <code>None</code> installs the latest version. <p>Version pinning is useful when you need reproducible environments or when a specific joblib version is required for compatibility with your local scikit-learn version.</p>"},{"location":"plugins/joblib/#usage","title":"Usage","text":""},{"location":"plugins/joblib/#basic-parallel-execution","title":"Basic parallel execution","text":"<p>Any function works with joblib \u2014 the plugin handles serialization and dispatch:</p> <pre><code>from time import sleep\n\nfrom joblib import Parallel, delayed\n\nimport skyward as sky\n\n\ndef slow_task(x):\n    sleep(5)\n    return x * 2\n\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=10,\n    worker=sky.Worker(concurrency=10),\n    plugins=[sky.plugins.joblib()],\n) as pool:\n    results = Parallel(n_jobs=-1)(\n        delayed(slow_task)(i) for i in range(2000)\n    )\n</code></pre> <p>With 10 nodes and <code>concurrency=10</code>, effective parallelism is 100. The 2000 tasks take 5 seconds each. Ideal time: <code>2000 / 100 * 5 = 100s</code>. In practice, overhead from serialization and network round-trips adds a few percent \u2014 expect 97-98% efficiency for tasks of this duration.</p>"},{"location":"plugins/joblib/#tuning-with-worker-concurrency","title":"Tuning with worker concurrency","text":"<p>The <code>Worker(concurrency=N)</code> parameter controls how many tasks each node handles simultaneously. This is the multiplier that makes joblib-on-Skyward practical:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=2,\n    vcpus=64,\n    worker=sky.Worker(concurrency=120),\n    plugins=[sky.plugins.joblib()],\n) as pool:\n    results = Parallel(n_jobs=-1)(\n        delayed(slow_task)(i) for i in range(20000)\n    )\n</code></pre> <p>High concurrency works well for I/O-bound or sleep-heavy tasks (API calls, network requests, waiting on external services). For CPU-bound tasks, match concurrency to the number of available cores. The default executor is threaded, so Python's GIL applies \u2014 for CPU-bound pure-Python work, consider <code>Worker(executor=\"process\")</code> to bypass it.</p>"},{"location":"plugins/joblib/#with-scikit-learn-via-the-sklearn-plugin","title":"With scikit-learn (via the sklearn plugin)","text":"<p>If your workload is scikit-learn-based, prefer the <code>sklearn</code> plugin instead \u2014 it builds on the same <code>SkywardBackend</code> but also installs scikit-learn:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=4,\n    worker=sky.Worker(concurrency=4),\n    plugins=[sky.plugins.sklearn()],\n) as pool:\n    grid = GridSearchCV(SVC(), param_grid, cv=5, n_jobs=-1)\n    grid.fit(X, y)\n</code></pre> <p>See the sklearn plugin documentation for details.</p>"},{"location":"plugins/joblib/#warning-filter-sanitization","title":"Warning filter sanitization","text":"<p>The plugin strips non-stdlib warning filters from <code>warnings.filters</code> before entering the joblib backend context. This prevents <code>ModuleNotFoundError</code> on workers when cloudpickle tries to deserialize warning category classes from packages (pytest, cloud SDKs, etc.) that are installed locally but not on the remote worker.</p>"},{"location":"plugins/joblib/#next-steps","title":"Next steps","text":"<ul> <li>Joblib Concurrency guide \u2014 Throughput analysis, real-world benchmarks, and cost model</li> <li>sklearn plugin \u2014 The scikit-learn plugin that builds on the same backend</li> <li>What are Plugins? \u2014 How the plugin system works</li> </ul>"},{"location":"plugins/keras/","title":"Keras","text":"<p>Keras 3 is backend-agnostic: the same model definition compiles and runs on JAX, PyTorch, or TensorFlow. But Keras decides which backend to use at import time, reading the <code>KERAS_BACKEND</code> environment variable the moment you write <code>import keras</code>. If the variable is not set by then, Keras falls back to its default, and there is no way to change the backend after the fact. This makes environment configuration critical, and it is precisely what the <code>keras</code> plugin handles.</p> <p><code>sky.plugins.keras()</code> ensures that <code>KERAS_BACKEND</code> is set correctly on every worker before any user code runs. It adds <code>keras</code> to the worker's pip dependencies, injects the environment variable into the bootstrap image, and re-sets it at worker startup as a safety net via the <code>around_app</code> hook. On multi-node JAX clusters, it goes further: it discovers all available devices, configures Keras's <code>DataParallel</code> distribution strategy, and synchronizes the random number generator across nodes so that weight initialization and dropout masks are reproducible across the cluster.</p>"},{"location":"plugins/keras/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>backend</code> <code>\"jax\"</code> | <code>\"torch\"</code> | <code>\"tensorflow\"</code> <code>\"jax\"</code> The Keras backend to activate on the remote worker. <p>The default is <code>\"jax\"</code> because JAX offers the tightest integration with Skyward's automatic distribution. When running on multiple nodes with the JAX backend, the plugin configures <code>DataParallel</code> distribution and RNG synchronization without any user code. The <code>\"torch\"</code> and <code>\"tensorflow\"</code> backends work equally well for model definition and single-node training, but multi-node distribution with those backends requires combining the Keras plugin with the corresponding framework plugin and using that framework's native distributed primitives.</p>"},{"location":"plugins/keras/#multi-node-behavior-by-backend","title":"Multi-node behavior by backend","text":""},{"location":"plugins/keras/#jax-backend","title":"JAX backend","text":"<p>The JAX backend is the recommended choice for multi-node Keras training on Skyward. When the plugin detects that <code>total_nodes &gt; 1</code> and the backend is <code>\"jax\"</code>, it performs three steps inside the <code>around_app</code> hook:</p> <ol> <li>Calls <code>keras.distribution.list_devices()</code> to discover all JAX devices visible to this process (after <code>jax.distributed.initialize()</code> has been called by the JAX plugin).</li> <li>Creates a <code>DataParallel</code> distribution with the discovered devices and calls <code>keras.distribution.set_distribution()</code> to activate it. This tells Keras to shard data and replicate model parameters across all devices automatically.</li> <li>Calls <code>initialize_rng()</code> from Keras's internal JAX distribution library to synchronize random number generation across all nodes. Without this step, each node would initialize model weights differently, breaking gradient synchronization.</li> </ol> <p>This means that on a 4-node cluster, <code>model.fit()</code> automatically distributes batches across all 4 nodes and averages gradients, with no changes to your training code.</p> <p>The JAX plugin must come before the Keras plugin in the list so that the distributed runtime is initialized before Keras tries to list devices:</p> <pre><code>plugins=[sky.plugins.jax(), sky.plugins.keras(backend=\"jax\")]  # correct order\n</code></pre>"},{"location":"plugins/keras/#pytorch-backend","title":"PyTorch backend","text":"<p>With <code>backend=\"torch\"</code>, the Keras plugin sets the environment variable and adds <code>keras</code> to pip, but does not configure any distributed strategy. PyTorch's distributed training relies on <code>torch.distributed.init_process_group()</code> and <code>DistributedDataParallel</code>, which the <code>torch</code> plugin handles. Keras models compiled with the PyTorch backend produce standard <code>torch.nn.Module</code> instances under the hood, so you can wrap them with DDP the same way you would with a native PyTorch model.</p> <p>For multi-node training with the PyTorch backend, combine both plugins:</p> <pre><code>plugins=[sky.plugins.torch(), sky.plugins.keras(backend=\"torch\")]\n</code></pre> <p>The <code>torch</code> plugin initializes the process group; the <code>keras</code> plugin ensures the backend is set correctly. Your training code is responsible for wrapping the model with DDP and using <code>DistributedSampler</code>.</p>"},{"location":"plugins/keras/#tensorflow-backend","title":"TensorFlow backend","text":"<p>The <code>\"tensorflow\"</code> backend follows the same pattern as PyTorch: the Keras plugin handles backend configuration, and TensorFlow's native distribution mechanisms (<code>tf.distribute.MultiWorkerMirroredStrategy</code>) handle multi-node coordination. Combine with appropriate TensorFlow distributed setup in your training function.</p>"},{"location":"plugins/keras/#single-node","title":"Single node","text":"<p>On a single node, no distribution configuration is needed regardless of backend. The Keras plugin sets <code>KERAS_BACKEND</code>, installs <code>keras</code>, and your model trains on whatever accelerator the node provides. This is the simplest configuration:</p> <pre><code>plugins=[sky.plugins.keras()]\n</code></pre> <p>No companion framework plugin is required for single-node execution \u2014 the Keras plugin alone is sufficient.</p>"},{"location":"plugins/keras/#usage","title":"Usage","text":""},{"location":"plugins/keras/#jax-backend-on-multiple-nodes-recommended","title":"JAX backend on multiple nodes (recommended)","text":"<p>This is the configuration with the best automatic distribution support. The JAX plugin initializes the distributed runtime, and the Keras plugin layers <code>DataParallel</code> on top:</p> <pre><code>import skyward as sky\n\n@sky.compute\ndef train():\n    import keras\n    from keras import layers\n\n    model = keras.Sequential([\n        layers.Flatten(input_shape=(28, 28)),\n        layers.Dense(128, activation=\"relu\"),\n        layers.Dense(10, activation=\"softmax\"),\n    ])\n\n    model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n\n    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n    x_train, y_train = sky.shard(x_train / 255.0, y_train, shuffle=True, seed=42)\n\n    model.fit(x_train, y_train, epochs=5, batch_size=64)\n    _, accuracy = model.evaluate(x_test / 255.0, y_test)\n    return accuracy\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=2,\n    accelerator=\"T4\",\n    plugins=[sky.plugins.jax(), sky.plugins.keras(backend=\"jax\")],\n) as pool:\n    results = train() @ pool\n</code></pre> <p>Each node trains on its shard of the data. The <code>DataParallel</code> distribution configured by the plugin handles parameter synchronization across the JAX device mesh.</p>"},{"location":"plugins/keras/#pytorch-backend-on-multiple-nodes","title":"PyTorch backend on multiple nodes","text":"<p>When you prefer PyTorch as the execution engine \u2014 perhaps because your pipeline includes PyTorch-specific operations or custom CUDA kernels \u2014 use the <code>torch</code> backend with both plugins:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=4,\n    accelerator=\"A100\",\n    plugins=[sky.plugins.torch(), sky.plugins.keras(backend=\"torch\")],\n) as pool:\n    results = train() @ pool\n</code></pre> <p>Your training function will need to handle distributed wrapping (DDP) and data partitioning (<code>DistributedSampler</code> or <code>sky.shard()</code>) explicitly, as the Keras plugin does not configure automatic distribution for the PyTorch backend.</p>"},{"location":"plugins/keras/#single-node-training","title":"Single-node training","text":"<p>For experimentation, prototyping, or workloads that fit on a single GPU, the Keras plugin alone is enough:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"T4\",\n    plugins=[sky.plugins.keras()],\n) as pool:\n    result = train() &gt;&gt; pool\n</code></pre> <p>This uses the default JAX backend on one node. No distributed setup runs, and <code>model.fit()</code> behaves exactly as it would on your local machine.</p>"},{"location":"plugins/keras/#plugin-combinations","title":"Plugin combinations","text":"<p>The Keras plugin is a backend configurator, not a distributed runtime. For multi-node training, always pair it with the plugin that matches the backend:</p> Backend Plugins Distribution <code>\"jax\"</code> <code>sky.plugins.jax()</code> + <code>sky.plugins.keras(backend=\"jax\")</code> Automatic <code>DataParallel</code> with RNG sync <code>\"torch\"</code> <code>sky.plugins.torch()</code> + <code>sky.plugins.keras(backend=\"torch\")</code> Manual DDP wrapping required <code>\"tensorflow\"</code> <code>sky.plugins.keras(backend=\"tensorflow\")</code> Manual <code>tf.distribute</code> strategy required Any (single node) <code>sky.plugins.keras()</code> None needed <p>The JAX combination is unique in that distribution is fully automatic \u2014 the plugins handle everything. With PyTorch and TensorFlow, the Keras plugin provides backend configuration while the framework's native distributed APIs handle the rest.</p>"},{"location":"plugins/keras/#further-reading","title":"Further reading","text":"<ul> <li>Keras Training guide \u2014 step-by-step MNIST training with Keras and JAX on multiple nodes.</li> <li>What are Plugins? \u2014 How the plugin system works</li> <li>PyTorch Distributed guide \u2014 relevant if using <code>backend=\"torch\"</code> with DDP.</li> </ul>"},{"location":"plugins/mig/","title":"NVIDIA MIG","text":"<p>A single A100 or H100 GPU has enormous compute capacity \u2014 often more than a single workload needs. Running a ResNet-50 inference or a small fine-tuning job on an 80 GB A100 leaves most of the hardware idle: hundreds of SMs without work, gigabytes of memory unaddressed, L2 cache serving a fraction of its bandwidth. You're paying for the full card but using a sliver of it.</p> <p>NVIDIA Multi-Instance GPU (MIG) solves this at the hardware level. It partitions one physical GPU into multiple isolated instances \u2014 each with its own dedicated compute units (SMs), its own memory controller, and its own L2 cache. These are not virtual devices sharing resources through time-slicing or software scheduling. They are physically isolated execution environments carved out of the silicon. Two processes running on two MIG partitions cannot interfere with each other: one cannot access the other's memory, steal its compute cycles, or compete for cache bandwidth. Each partition behaves like a smaller, independent GPU.</p> <p>The operational burden of MIG is what keeps most teams from using it. You need to enable MIG mode (which requires a GPU reset), create GPU instances with the right profile via <code>nvidia-smi</code>, create compute instances inside those GPU instances, enumerate the resulting MIG UUIDs, and assign each process to its partition by setting <code>CUDA_VISIBLE_DEVICES</code> to the correct UUID. Get any step wrong \u2014 mismatched profiles, wrong UUID indexing, MIG mode not enabled \u2014 and CUDA sees the wrong device or no device at all.</p> <p>Skyward's <code>mig</code> plugin handles the full lifecycle: enables MIG mode during bootstrap, creates the requested partitions, and pins each subprocess to its own device before any task executes. Your <code>@sky.compute</code> functions see a normal CUDA device \u2014 they don't know MIG exists.</p>"},{"location":"plugins/mig/#what-it-does","title":"What it does","text":"<p>The plugin configures container-level GPU visibility, enables MIG mode and creates partitions during bootstrap, and assigns each subprocess its own MIG device at runtime.</p>"},{"location":"plugins/mig/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>profile</code> <code>str</code> \u2014 MIG profile name (e.g. <code>\"3g.40gb\"</code>, <code>\"1g.10gb\"</code>, <code>\"7g.80gb\"</code>). Determines the size and maximum count of partitions. <p>The profile string is passed directly to <code>nvidia-smi mig -cgi</code>. Available profiles depend on the GPU model \u2014 an A100 80GB supports seven profiles (from <code>1g.10gb</code> to <code>7g.80gb</code>), while an H100 supports a different set, and an A30 supports fewer still. The first number indicates compute slices (groups of SMs), not a fraction of the GPU: <code>3g.40gb</code> gets about 3/7 of the SMs, which is roughly 42 streaming multiprocessors on an A100. The second number is dedicated memory.</p> <p>See NVIDIA's supported GPUs page for the full profile matrix per GPU, or run <code>nvidia-smi mig -lgip</code> on a MIG-capable node to list what the hardware supports.</p>"},{"location":"plugins/mig/#how-it-works","title":"How it works","text":""},{"location":"plugins/mig/#image-transform","title":"Image transform","text":"<p>The <code>transform</code> hook sets a single environment variable: <code>NVIDIA_VISIBLE_DEVICES=all</code>. This tells the NVIDIA container runtime to expose every MIG partition to the worker process. Without it, a container environment might present only a subset of devices \u2014 or a single device that maps to the whole GPU, bypassing the MIG partitions entirely. The variable is merged into the existing image environment using <code>replace()</code>, preserving any variables already defined in the <code>Image</code> or added by other plugins.</p>"},{"location":"plugins/mig/#bootstrap","title":"Bootstrap","text":"<p>The <code>bootstrap</code> hook generates the shell commands that partition the GPU. It runs after the standard bootstrap phases (apt, pip, Python setup) and produces two kinds of commands:</p> <p>First, it enables MIG mode:</p> <pre><code>nvidia-smi -mig 1\n</code></pre> <p>This is a mode switch on the GPU \u2014 it reconfigures the hardware to support partitioning. On a freshly booted cloud instance, MIG mode is typically off by default. Enabling it does not require a GPU reset on supported drivers (R470+), but it must happen before any GPU instances are created.</p> <p>Then, for each worker subprocess (determined by <code>cluster.spec.worker.concurrency</code>), it creates one partition:</p> <pre><code>nvidia-smi mig -cgi &lt;profile&gt; -C\n</code></pre> <p>The <code>-cgi</code> flag creates a GPU Instance with the given profile, and <code>-C</code> immediately creates a Compute Instance inside it. For <code>concurrency=2</code> and <code>profile=\"3g.40gb\"</code>, the bootstrap produces:</p> <pre><code>nvidia-smi -mig 1\nnvidia-smi mig -cgi 3g.40gb -C\nnvidia-smi mig -cgi 3g.40gb -C\n</code></pre> <p>The number of partitions created equals the worker concurrency. If the GPU does not support that many instances for the given profile \u2014 for example, requesting three <code>3g.40gb</code> partitions on an A100, which only supports two \u2014 <code>nvidia-smi</code> exits with an error and the bootstrap fails. This is intentional: the concurrency and profile must agree, and the failure happens early (during provisioning) rather than silently at runtime.</p>"},{"location":"plugins/mig/#process-lifecycle-around_process","title":"Process lifecycle (<code>around_process</code>)","text":"<p>The <code>around_process</code> hook runs once per subprocess, before the first task executes. It solves the last piece of the MIG puzzle: assigning each subprocess to its specific partition.</p> <p>The hook:</p> <ol> <li>Calls <code>nvidia-smi -L</code> to list all GPU devices and their MIG instances. The output includes MIG UUIDs in the format <code>MIG-xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</code>.</li> <li>Extracts all MIG UUIDs using a regex match.</li> <li>Reads <code>instance_info().worker</code> to determine this subprocess's index (0, 1, 2, ...).</li> <li>Sets <code>CUDA_VISIBLE_DEVICES</code> to the UUID at that index.</li> </ol> <p>After this, CUDA presents the assigned partition as the only available device. <code>torch.device(\"cuda\")</code> resolves to this partition. <code>torch.cuda.device_count()</code> returns 1. The subprocess has no way to access other partitions \u2014 the isolation is enforced by both the environment variable and the hardware.</p> <p>Each subprocess runs this independently in its own process. Worker 0 gets the first MIG UUID, worker 1 gets the second, and so on. Because the hook runs exactly once per process (not per task), the device assignment is stable for the lifetime of the subprocess \u2014 subsequent tasks on the same worker reuse the same partition without re-running the hook.</p>"},{"location":"plugins/mig/#when-to-use-mig","title":"When to use MIG","text":"<p>MIG is specifically valuable when you have a high-end GPU and workloads that don't need its full capacity.</p> <p>Independent training runs are the primary use case. If you're running hyperparameter sweeps, architecture comparisons, or ablation studies where each run is a separate training job, MIG lets you run multiple jobs on a single card. Each job gets guaranteed resources \u2014 no interference, no contention, predictable performance.</p> <p>Concurrent inference works well when each model fits within a partition's memory. Two models serving requests on two <code>3g.40gb</code> partitions of an A100 each get dedicated compute and memory \u2014 better isolation than MPS, with no risk of one model's memory allocation starving the other.</p> <p>Development and experimentation benefits from MIG when you have a powerful GPU but your experiments are small. Instead of wasting 70 GB of memory while fine-tuning a small model, partition the card and run several experiments simultaneously.</p> <p>MIG is generally not useful for:</p> <ul> <li>Workloads that saturate the GPU \u2014 If your training loop uses all SMs and all memory, partitioning reduces performance. A single <code>7g.80gb</code> partition on an A100 has fewer SMs than the full card.</li> <li>Multi-GPU distributed training \u2014 DDP and FSDP expect each rank to own a full GPU. MIG partitions are not designed for gradient synchronization across them. Use the <code>torch</code> plugin for distributed training instead.</li> <li>Unsupported GPUs \u2014 Consumer GPUs do not support MIG. See Requirements for the full constraint list.</li> <li>Dynamic workloads with varying resource needs \u2014 MIG partitions are fixed at setup time. If your workload needs more memory for some tasks and less for others, MPS offers more flexibility.</li> </ul>"},{"location":"plugins/mig/#how-it-differs-from-mps","title":"How it differs from MPS","text":"<p>MIG and MPS both allow multiple processes to share a GPU, but the isolation model is fundamentally different.</p> <p>MIG provides hardware-level isolation. Each partition has its own dedicated SMs, memory, and L2 cache. One partition cannot access another's memory or steal its compute cycles. The trade-off is that partitions are fixed \u2014 you choose a profile at setup time, and all partitions on a GPU must use the same profile. MIG is only available on supported datacenter and professional GPUs.</p> <p>MPS provides software-level sharing. All processes submit kernels through a shared daemon, and the GPU scheduler overlaps their work. There is no memory isolation \u2014 a misbehaving process can consume all available memory. The benefit is flexibility: any number of processes can share the GPU without predefined partitions, and MPS works on any CUDA GPU.</p> <p>Use MIG when you need guaranteed isolation and predictable performance per partition. Use MPS when you need flexible sharing and the processes are trusted. The two are mutually exclusive on the same GPU \u2014 enabling MIG mode disables MPS-style sharing within each partition, though MPS can be used within a single MIG partition if needed.</p>"},{"location":"plugins/mig/#usage","title":"Usage","text":""},{"location":"plugins/mig/#independent-training-on-partitions","title":"Independent training on partitions","text":"<p>The most common pattern: run independent training jobs on separate partitions of a single GPU.</p> <pre><code>import skyward as sky\n\n\n@sky.compute\ndef train_on_partition(epochs: int, lr: float) -&gt; dict:\n    import os\n\n    import torch\n    import torch.nn as nn\n    from torch.utils.data import DataLoader, TensorDataset\n\n    info = sky.instance_info()\n    device = torch.device(\"cuda\")\n\n    model = nn.Sequential(\n        nn.Linear(784, 256),\n        nn.ReLU(),\n        nn.Linear(256, 128),\n        nn.ReLU(),\n        nn.Linear(128, 10),\n    ).to(device)\n\n    x = torch.randn(5000, 784, device=device)\n    y = torch.randint(0, 10, (5000,), device=device)\n    loader = DataLoader(TensorDataset(x, y), batch_size=128, shuffle=True)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    for epoch in range(epochs):\n        epoch_loss = 0.0\n        correct = 0\n        total = 0\n        for batch_x, batch_y in loader:\n            optimizer.zero_grad()\n            output = model(batch_x)\n            loss = criterion(output, batch_y)\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            correct += (output.argmax(1) == batch_y).sum().item()\n            total += batch_y.size(0)\n\n    return {\n        \"worker\": info.worker,\n        \"partition\": os.environ.get(\"CUDA_VISIBLE_DEVICES\", \"unset\"),\n        \"final_loss\": round(epoch_loss / len(loader), 4),\n        \"accuracy\": round(100.0 * correct / total, 1),\n    }\n\n\nPARTITIONS = 2\nPROFILE = \"3g.40gb\"\n\nwith sky.ComputePool(\n    provider=sky.Verda(),\n    nodes=1,\n    accelerator=sky.accelerators.A100(),\n    worker=sky.Worker(concurrency=PARTITIONS, executor=\"process\"),\n    image=sky.Image(pip=[\"torch\"]),\n    plugins=[sky.plugins.mig(profile=PROFILE)],\n) as pool:\n    tasks = [train_on_partition(epochs=10, lr=1e-3) for _ in range(PARTITIONS)]\n    results = list(sky.gather(*tasks, stream=True) &gt;&gt; pool)\n</code></pre> <p>The concurrency and profile must agree. A <code>3g.40gb</code> profile on an A100 supports exactly two partitions, so <code>concurrency=2</code>. Setting concurrency to three would fail during bootstrap because the GPU cannot create a third instance of that profile.</p> <p>Notice that PyTorch is installed via <code>Image(pip=[\"torch\"])</code>, not via <code>sky.plugins.torch()</code>. The torch plugin is designed for multi-node distributed training \u2014 it calls <code>init_process_group()</code>, which MIG partitions don't need. MIG partitions are independent workloads, not a distributed cluster.</p>"},{"location":"plugins/mig/#maximum-partitions","title":"Maximum partitions","text":"<p>For lightweight workloads \u2014 small model inference, quick evaluations, data preprocessing with GPU-accelerated libraries \u2014 you can maximize the number of partitions with a smaller profile:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=1,\n    accelerator=sky.accelerators.A100(memory=\"80GB\"),\n    worker=sky.Worker(concurrency=7, executor=\"process\"),\n    image=sky.Image(pip=[\"torch\"]),\n    plugins=[sky.plugins.mig(profile=\"1g.10gb\")],\n) as pool:\n    tasks = [evaluate(model_id=i) for i in range(7)]\n    results = list(sky.gather(*tasks, stream=True) &gt;&gt; pool)\n</code></pre> <p>Seven partitions from a single A100 80GB, each with ~10 GB of memory and 14 SMs. Each partition can run a small model (DistilBERT, ResNet-18, a lightweight diffusion decoder) independently. This is seven times the throughput of running them sequentially on the full card \u2014 at the cost of reduced per-partition compute.</p> <p>The <code>1g.10gb</code> profile is the smallest available on the A100. Smaller profiles mean more partitions but less compute and memory per partition. If your model needs more than 10 GB, step up to <code>2g.20gb</code> (three partitions) or <code>3g.40gb</code> (two partitions).</p>"},{"location":"plugins/mig/#hyperparameter-sweep","title":"Hyperparameter sweep","text":"<p>Different configurations running simultaneously on separate partitions \u2014 each partition explores a different point in the hyperparameter space:</p> <pre><code>configs = [\n    {\"epochs\": 20, \"lr\": 1e-3},\n    {\"epochs\": 20, \"lr\": 3e-4},\n]\n\nwith sky.ComputePool(\n    provider=sky.Verda(),\n    nodes=1,\n    accelerator=sky.accelerators.A100(),\n    worker=sky.Worker(concurrency=len(configs), executor=\"process\"),\n    image=sky.Image(pip=[\"torch\"]),\n    plugins=[sky.plugins.mig(profile=\"3g.40gb\")],\n) as pool:\n    tasks = [train_on_partition(**cfg) for cfg in configs]\n    results = list(sky.gather(*tasks, stream=True) &gt;&gt; pool)\n\n    best = max(results, key=lambda r: r[\"accuracy\"])\n    print(f\"Best: worker {best['worker']} with acc={best['accuracy']}%\")\n</code></pre> <p>Both configurations run simultaneously with hardware-enforced isolation. Neither run can affect the other's performance, so the results are directly comparable \u2014 no noise from resource contention.</p>"},{"location":"plugins/mig/#multi-node-with-mig","title":"Multi-node with MIG","text":"<p>MIG works per-GPU, not per-cluster. On a multi-node pool, each node independently partitions its own GPU:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=3,\n    accelerator=sky.accelerators.A100(),\n    worker=sky.Worker(concurrency=2, executor=\"process\"),\n    image=sky.Image(pip=[\"torch\"]),\n    plugins=[sky.plugins.mig(profile=\"3g.40gb\")],\n) as pool:\n    # 3 nodes * 2 partitions = 6 independent workers\n    tasks = [train_on_partition(epochs=10, lr=lr) for lr in [1e-2, 3e-3, 1e-3, 3e-4, 1e-4, 3e-5]]\n    results = list(sky.gather(*tasks, stream=True) &gt;&gt; pool)\n</code></pre> <p>Each of the 3 nodes gets its own A100 split into two <code>3g.40gb</code> partitions, giving you 6 independent workers total. Tasks are dispatched round-robin across all 6 workers. This is not distributed training \u2014 there is no gradient synchronization between partitions. Each task runs independently, which is exactly what you want for sweeps, evaluations, and embarrassingly parallel workloads.</p>"},{"location":"plugins/mig/#requirements","title":"Requirements","text":"<ul> <li>MIG-capable GPU \u2014 Supported on datacenter and professional GPUs such as A100, H100, and B200. Consumer GPUs do not support MIG. See NVIDIA's supported GPUs page for the current list.</li> <li>Process executor \u2014 <code>Worker(executor=\"process\")</code> is required. MIG device assignment works by setting <code>CUDA_VISIBLE_DEVICES</code> per subprocess. The thread executor shares a single process (and a single <code>CUDA_VISIBLE_DEVICES</code>), so all threads would see the same partition.</li> <li>Concurrency matches profile \u2014 The <code>concurrency</code> value must not exceed the number of partitions the GPU supports for the given profile. An A100 80GB supports 2 partitions for <code>3g.40gb</code>, 3 for <code>2g.20gb</code>, and 7 for <code>1g.10gb</code>.</li> <li>Single GPU per node \u2014 The current implementation assumes one GPU per node. Multi-GPU nodes with per-GPU MIG partitioning are not yet supported.</li> </ul>"},{"location":"plugins/mig/#next-steps","title":"Next steps","text":"<ul> <li>NVIDIA MIG Guide \u2014 Step-by-step walkthrough with a training example</li> <li>NVIDIA MPS \u2014 Software-level GPU sharing (complementary approach)</li> <li>Worker Executors \u2014 Thread vs process executors and when to use each</li> <li>What are Plugins? \u2014 How the plugin system works</li> </ul>"},{"location":"plugins/mps/","title":"NVIDIA MPS","text":"<p>When multiple CUDA processes share a single GPU \u2014 concurrent inference servers, multiple workers running independent forward passes, parallel data preprocessing pipelines \u2014 the default CUDA behavior is time-slicing. Each process gets exclusive access to the GPU for a time quantum, then yields to the next. Context switches between processes are expensive: the GPU must save and restore execution state, flush caches, and re-establish memory mappings. For workloads that do not saturate the GPU's compute capacity individually, time-slicing wastes significant throughput because the hardware sits idle during context switches and because each process's kernels cannot overlap with another's.</p> <p>NVIDIA Multi-Process Service (MPS) solves this. It provides a shared GPU context that multiple CUDA processes connect to through a single daemon. Instead of each process owning its own CUDA context and time-slicing, all processes submit work through the MPS daemon, which funnels their kernels into a unified execution stream. The GPU's SM (Streaming Multiprocessor) scheduler can then overlap kernels from different processes, filling compute gaps and improving utilization. The result is higher aggregate throughput and lower latency, especially for workloads where individual processes use only a fraction of the GPU's capacity.</p> <p>Skyward's <code>mps</code> plugin starts the MPS daemon during instance bootstrap and configures the environment variables that CUDA uses to connect to it. Every CUDA process on the worker \u2014 including concurrent task threads or processes managed by the <code>Worker</code> executor \u2014 automatically routes through MPS.</p>"},{"location":"plugins/mps/#what-it-does","title":"What it does","text":"<p>Image transform \u2014 Sets environment variables that configure the MPS runtime:</p> <ul> <li><code>CUDA_MPS_PIPE_DIRECTORY</code> \u2014 The directory for the MPS daemon's named pipes (set to <code>/tmp/nvidia-mps</code>). All CUDA processes on the instance use this path to communicate with the daemon.</li> <li><code>CUDA_MPS_LOG_DIRECTORY</code> \u2014 The directory for MPS daemon logs (set to <code>/tmp/nvidia-mps-log</code>).</li> <li><code>CUDA_MPS_ACTIVE_THREAD_PERCENTAGE</code> \u2014 Optionally limits the percentage of GPU compute threads each MPS client can use. This prevents a single client from monopolizing the GPU.</li> <li><code>CUDA_MPS_PINNED_DEVICE_MEM_LIMIT</code> \u2014 Optionally limits the amount of pinned (page-locked) memory each client can allocate per device.</li> </ul> <p>Bootstrap \u2014 After the base environment is set up, the plugin runs two commands: <code>mkdir -p /tmp/nvidia-mps /tmp/nvidia-mps-log</code> to create the pipe and log directories, then <code>nvidia-cuda-mps-control -d</code> to start the MPS daemon in the background. The daemon runs for the lifetime of the instance. Once it is running, any CUDA process that starts on the instance automatically connects to it through the pipe directory.</p>"},{"location":"plugins/mps/#when-to-use-mps","title":"When to use MPS","text":"<p>MPS is not for every workload. It is specifically valuable when multiple independent CUDA processes need to share a GPU concurrently, and each individual process does not fully saturate the GPU on its own.</p> <p>High-concurrency inference is the primary use case. If you are running a model serving workload where many requests arrive simultaneously, each running a forward pass through a relatively small model (ResNet-50, DistilBERT, a small diffusion model), the individual forward passes may only use 10-30% of the GPU's compute capacity. Without MPS, the GPU time-slices between them. With MPS, their kernels overlap on the SMs, and aggregate throughput increases substantially.</p> <p>Multiple workers per GPU is the related pattern. Skyward's <code>Worker(concurrency=N, executor=\"process\")</code> runs N worker processes on each node. If the node has one GPU, all N processes share it. Without MPS, they context-switch. With MPS, they share efficiently. This is useful for workloads that are partially GPU-bound and partially CPU-bound \u2014 each process can use the GPU for its GPU portion while other processes use it for theirs.</p> <p>Batch data preprocessing that uses GPU-accelerated libraries (cuDF, cuPy, torchvision transforms on GPU) can also benefit, especially when multiple preprocessing pipelines run concurrently.</p> <p>MPS is generally not useful for:</p> <ul> <li>Single-process workloads \u2014 If only one process uses the GPU, MPS adds no benefit (and a negligible amount of overhead).</li> <li>Workloads that saturate the GPU \u2014 If a single process uses 90%+ of the GPU's compute, there is nothing to overlap. Multi-node training with DDP, where each node runs one training process that fully utilizes the GPU, does not benefit from MPS.</li> <li>Multi-GPU training \u2014 MPS operates per-GPU. For multi-GPU setups, each GPU has its own MPS daemon. But if the training process already uses all GPUs via NCCL, MPS is irrelevant.</li> </ul>"},{"location":"plugins/mps/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>active_thread_percentage</code> <code>int \\| None</code> <code>None</code> Maximum percentage of GPU compute threads (1-100) each MPS client can use. Useful for fair scheduling across concurrent processes. <code>pinned_memory_limit</code> <code>str \\| None</code> <code>None</code> Per-device pinned memory limit per client (e.g. <code>\"0=2G\"</code> for 2 GB on device 0). Prevents a single client from exhausting pinned memory. <p><code>active_thread_percentage</code> controls how much of the GPU each MPS client can use. If you have 8 concurrent processes sharing a GPU, setting <code>active_thread_percentage=12</code> ensures each one gets roughly 12% of the SMs. Without this, a single client could submit enough work to saturate the GPU, starving others. A common heuristic is <code>100 // concurrency</code>.</p> <p><code>pinned_memory_limit</code> restricts page-locked memory allocation per client. Pinned memory enables fast GPU transfers but is a finite resource. If 8 processes each try to pin 4 GB, the system may run out. The format is <code>\"device_id=limit\"</code> \u2014 for example, <code>\"0=2G\"</code> limits each client to 2 GB of pinned memory on GPU 0.</p>"},{"location":"plugins/mps/#how-it-differs-from-multi-node-training","title":"How it differs from multi-node training","text":"<p>MPS and multi-node distributed training solve fundamentally different problems. Multi-node training (via the <code>torch</code> plugin with DDP, or the <code>jax</code> plugin) splits a single training job across multiple GPUs on multiple machines, synchronizing gradients between them. Each GPU runs one process that fully utilizes it.</p> <p>MPS enables multiple independent processes to share a single GPU efficiently. The processes do not coordinate \u2014 they each run their own workload, and MPS ensures their GPU operations overlap rather than time-slice.</p> <p>You can combine both patterns. A multi-node cluster with the <code>torch</code> plugin for DDP training does not need MPS (each node's GPU is dedicated to one training process). But a multi-node cluster for inference \u2014 where each node handles many concurrent requests \u2014 benefits from MPS on each node.</p>"},{"location":"plugins/mps/#usage","title":"Usage","text":""},{"location":"plugins/mps/#concurrent-inference","title":"Concurrent inference","text":"<p>Run multiple inference tasks concurrently on a single GPU node:</p> <pre><code>import skyward as sky\n\n\n@sky.compute\ndef inference(task_id: int, batch_size: int) -&gt; dict:\n    import torch\n    from torchvision.models import resnet50\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model = resnet50(weights=None).to(device).eval()\n    dummy = torch.randn(batch_size, 3, 224, 224, device=device)\n\n    with torch.no_grad():\n        for _ in range(100):\n            model(dummy)\n    torch.cuda.synchronize()\n\n    return {\"task_id\": task_id, \"device\": device}\n\n\nCONCURRENCY = 8\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=1,\n    accelerator=\"T4\",\n    worker=sky.Worker(concurrency=CONCURRENCY, executor=\"process\"),\n    plugins=[\n        sky.plugins.torch(),\n        sky.plugins.mps(active_thread_percentage=100 // CONCURRENCY),\n    ],\n    image=sky.Image(pip=[\"torchvision\"]),\n) as pool:\n    tasks = [inference(i, batch_size=1) for i in range(CONCURRENCY * 2)]\n    results = list(sky.gather(*tasks, stream=True) &gt;&gt; pool)\n</code></pre> <p>The <code>executor=\"process\"</code> setting means each concurrent task runs in its own process with its own CUDA context. MPS unifies those contexts into a single shared context on the GPU. The <code>active_thread_percentage=100 // 8 = 12</code> ensures fair sharing across 8 concurrent processes.</p> <p>Without MPS, these 8 processes would time-slice on the GPU. With MPS, their kernels overlap, and aggregate throughput is higher. The improvement depends on how much of the GPU each individual process utilizes \u2014 smaller models and smaller batch sizes see greater relative improvement because there is more idle compute to fill.</p>"},{"location":"plugins/mps/#with-higher-concurrency","title":"With higher concurrency","text":"<p>For very high concurrency (many small tasks), increase the worker's concurrency and lower the per-client thread percentage:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=1,\n    accelerator=\"A100\",\n    worker=sky.Worker(concurrency=32, executor=\"process\"),\n    plugins=[\n        sky.plugins.torch(),\n        sky.plugins.mps(\n            active_thread_percentage=3,\n            pinned_memory_limit=\"0=1G\",\n        ),\n    ],\n) as pool:\n    ...\n</code></pre> <p>With 32 concurrent processes, each gets 3% of the GPU's compute threads and at most 1 GB of pinned memory. This is appropriate for very lightweight inference tasks (small models, single-sample batches) where the goal is maximum throughput from a single GPU.</p>"},{"location":"plugins/mps/#next-steps","title":"Next steps","text":"<ul> <li>PyTorch Distributed \u2014 Multi-node training with DDP (complementary to MPS)</li> <li>Worker Executors \u2014 Thread vs process executors and when to use each</li> <li>What are Plugins? \u2014 How the plugin system works</li> </ul>"},{"location":"plugins/sklearn/","title":"Scikit-learn","text":"<p>scikit-learn is built on joblib for parallelism. Every estimator and utility that accepts <code>n_jobs</code> \u2014 <code>GridSearchCV</code>, <code>RandomizedSearchCV</code>, <code>cross_val_score</code>, <code>RFECV</code>, <code>BaggingClassifier</code>, <code>VotingClassifier</code>, and many others \u2014 delegates to <code>joblib.Parallel</code> internally. This means the parallelism strategy is pluggable: replace the joblib backend, and every scikit-learn operation that uses <code>n_jobs</code> distributes automatically.</p> <p>Skyward's <code>sklearn</code> plugin does exactly this. It installs scikit-learn and joblib on the worker, registers the same <code>SkywardBackend</code> that the joblib plugin uses, and enters the <code>parallel_backend(\"skyward\")</code> context for the duration of the pool. Inside the pool block, <code>n_jobs=-1</code> means \"all workers in the cluster.\" No code changes are needed beyond the pool configuration \u2014 your existing scikit-learn code works as-is.</p>"},{"location":"plugins/sklearn/#what-it-does","title":"What it does","text":"<p>Image transform \u2014 Appends <code>scikit-learn</code> (optionally at a pinned version) and <code>joblib</code> to the worker's pip dependencies. Both are needed on the worker because scikit-learn imports joblib internally, and the <code>SkywardBackend</code> dispatches tasks that need to be deserialized in an environment where both packages are available.</p> <p>Client lifecycle (<code>around_client</code>) \u2014 Reuses the joblib plugin's infrastructure: it calls <code>_setup_backend(pool)</code> to register <code>SkywardBackend</code>, calls <code>_strip_local_warning_filters()</code> to sanitize warning filters (see the joblib plugin documentation for why this matters), and enters <code>parallel_backend(\"skyward\")</code>. This is the same machinery as the joblib plugin \u2014 the sklearn plugin is effectively the joblib plugin plus scikit-learn installation.</p>"},{"location":"plugins/sklearn/#relationship-with-the-joblib-plugin","title":"Relationship with the Joblib plugin","text":"<p>The <code>sklearn</code> plugin and the <code>joblib</code> plugin share the same backend. Under the hood, both register <code>SkywardBackend</code> as a custom joblib parallel backend, and both enter the <code>parallel_backend(\"skyward\")</code> context. The difference is what they install on the worker:</p> <ul> <li><code>sky.plugins.joblib()</code> installs only <code>joblib</code>.</li> <li><code>sky.plugins.sklearn()</code> installs <code>scikit-learn</code> and <code>joblib</code>.</li> </ul> <p>If your workload is scikit-learn-based, use the <code>sklearn</code> plugin alone \u2014 it includes everything the <code>joblib</code> plugin provides. You do not need to stack both plugins. If your workload is pure joblib without scikit-learn, use the <code>joblib</code> plugin.</p> <p>If you happen to specify both, nothing breaks \u2014 the backend registration is idempotent, and duplicate pip packages are harmless. But it is unnecessary.</p>"},{"location":"plugins/sklearn/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>version</code> <code>str \\| None</code> <code>None</code> Specific scikit-learn version to install (e.g. <code>\"1.4.0\"</code>). <code>None</code> installs the latest version. <p>Version pinning is important when your local code depends on specific scikit-learn behavior, or when you need reproducibility across runs. The worker's scikit-learn version should match (or be compatible with) the version used to define the estimators and pipelines, because cloudpickle serializes Python objects and deserializes them in the worker's environment.</p>"},{"location":"plugins/sklearn/#what-works-with-n_jobs","title":"What works with <code>n_jobs</code>","text":"<p>Everything in scikit-learn that accepts <code>n_jobs</code> distributes across the cluster without modification:</p> <ul> <li><code>GridSearchCV</code> \u2014 Each combination of hyperparameters and cross-validation fold is a separate task. A grid with 20 candidates and 5-fold CV produces 100 fits, all distributed.</li> <li><code>RandomizedSearchCV</code> \u2014 Same as <code>GridSearchCV</code> but with random sampling. <code>n_iter=50</code> with 5-fold CV produces 250 fits.</li> <li><code>cross_val_score</code> / <code>cross_validate</code> \u2014 Each fold is an independent fit+evaluate. 10-fold CV distributes 10 tasks.</li> <li><code>RFECV</code> (Recursive Feature Elimination with CV) \u2014 Each elimination step and fold is distributed.</li> <li><code>BaggingClassifier</code> / <code>BaggingRegressor</code> \u2014 Each base estimator is fit independently when <code>n_jobs=-1</code>.</li> <li><code>VotingClassifier</code> / <code>VotingRegressor</code> \u2014 Each constituent estimator is fit independently.</li> <li><code>MultiOutputClassifier</code> / <code>MultiOutputRegressor</code> \u2014 Each target's estimator is fit independently.</li> <li><code>Pipeline</code> with parallel steps \u2014 When combined with <code>GridSearchCV</code>, the full pipeline (preprocessing + estimator) is replicated per task.</li> </ul> <p>The pattern is consistent: scikit-learn calls <code>joblib.Parallel(n_jobs=self.n_jobs)</code> internally, the Skyward backend intercepts it, and each unit of work is dispatched to a remote worker.</p>"},{"location":"plugins/sklearn/#usage","title":"Usage","text":""},{"location":"plugins/sklearn/#grid-search","title":"Grid search","text":"<p>The most common use case is distributing hyperparameter search:</p> <pre><code>import skyward as sky\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import GridSearchCV, train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\n\n\n@sky.compute\ndef run_search() -&gt; dict:\n    X, y = load_digits(return_X_y=True)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    pipe = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"clf\", SVC()),\n    ])\n\n    param_grid = {\n        \"clf__C\": [0.1, 1, 10, 100],\n        \"clf__gamma\": [\"scale\", \"auto\", 0.01, 0.001],\n        \"clf__kernel\": [\"rbf\", \"poly\"],\n    }\n\n    grid = GridSearchCV(pipe, param_grid, cv=5, n_jobs=-1, verbose=1)\n    grid.fit(X_train, y_train)\n\n    return {\n        \"best_params\": grid.best_params_,\n        \"best_cv_score\": grid.best_score_,\n        \"test_score\": grid.score(X_test, y_test),\n    }\n\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=4,\n    worker=sky.Worker(concurrency=4),\n    plugins=[sky.plugins.sklearn()],\n) as pool:\n    result = run_search() &gt;&gt; pool\n    print(f\"Best: {result['best_params']}, CV={result['best_cv_score']:.2%}\")\n</code></pre> <p>This grid has 32 candidates and 5-fold CV, producing 160 fits. With 4 nodes and <code>concurrency=4</code>, 16 fits run in parallel. The <code>n_jobs=-1</code> inside <code>GridSearchCV</code> tells joblib to use all available workers, which the Skyward backend reports as 16.</p> <p>Note that the <code>GridSearchCV</code> call happens inside a <code>@sky.compute</code> function. The grid search itself runs on a remote worker \u2014 it is the grid search's internal <code>Parallel</code> calls that distribute across the cluster. The outer <code>&gt;&gt; pool</code> dispatches the function to one node; that node's joblib backend then fans out the 160 individual fits across all nodes.</p>"},{"location":"plugins/sklearn/#cross-validation","title":"Cross-validation","text":"<p>For a quick evaluation without hyperparameter tuning:</p> <pre><code>@sky.compute\ndef evaluate_model() -&gt; dict:\n    from sklearn.datasets import load_digits\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import cross_val_score\n\n    X, y = load_digits(return_X_y=True)\n    clf = RandomForestClassifier(n_estimators=100)\n    scores = cross_val_score(clf, X, y, cv=10, n_jobs=-1)\n\n    return {\"mean\": scores.mean(), \"std\": scores.std()}\n\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=3,\n    worker=sky.Worker(concurrency=4),\n    plugins=[sky.plugins.sklearn()],\n) as pool:\n    result = evaluate_model() &gt;&gt; pool\n</code></pre> <p>Ten-fold CV distributes 10 independent fit+evaluate tasks across 12 workers.</p>"},{"location":"plugins/sklearn/#combining-with-cuml","title":"Combining with cuML","text":"<p>For GPU-accelerated scikit-learn, stack the <code>cuml</code> plugin with <code>sklearn</code>:</p> <pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"L4\",\n    nodes=1,\n    plugins=[\n        sky.plugins.cuml(),\n        sky.plugins.sklearn(),\n    ],\n) as pool:\n    result = train_on_gpu() &gt;&gt; pool\n</code></pre> <p>The <code>cuml</code> plugin intercepts sklearn calls and routes them to GPU. The <code>sklearn</code> plugin ensures scikit-learn and joblib are installed. See the cuML plugin documentation for details.</p>"},{"location":"plugins/sklearn/#next-steps","title":"Next steps","text":"<ul> <li>Scikit Grid Search guide \u2014 Complete example with multiple estimator families and pipeline search</li> <li>joblib plugin \u2014 How <code>SkywardBackend</code> works, warning filter sanitization, and tuning concurrency</li> <li>cuML plugin \u2014 GPU-accelerated scikit-learn with NVIDIA RAPIDS</li> <li>What are Plugins? \u2014 How the plugin system works</li> </ul>"},{"location":"plugins/torch/","title":"PyTorch","text":"<p>PyTorch's distributed training model is built around <code>DistributedDataParallel</code> (DDP). Each process \u2014 typically one per node \u2014 holds a complete copy of the model. During the forward pass, each process computes gradients on its own data shard. During the backward pass, DDP synchronizes gradients across all processes using a collective communication backend (NCCL for GPUs, gloo for CPUs). The optimizer then steps with identical averaged gradients on every process, keeping the model copies in sync without explicit parameter transfers.</p> <p>The hard part is the setup. Before <code>init_process_group()</code> can be called, every process needs five pieces of information: the address of the rendezvous master (<code>MASTER_ADDR</code>), the master port (<code>MASTER_PORT</code>), the total number of processes (<code>WORLD_SIZE</code>), this process's global rank (<code>RANK</code>), and its local rank on the machine (<code>LOCAL_RANK</code>). These must be set as environment variables before any distributed operation. In a traditional setup, you write a launch script or use <code>torchrun</code> to inject these values. With Skyward, the <code>torch</code> plugin reads the cluster topology from <code>instance_info()</code> and sets everything before your function body runs.</p>"},{"location":"plugins/torch/#what-it-does","title":"What it does","text":"<p>The plugin installs PyTorch with the correct CUDA wheels on the remote worker and initializes the distributed process group once per worker process.</p>"},{"location":"plugins/torch/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>backend</code> <code>\"nccl\" \\| \"gloo\" \\| None</code> <code>None</code> Process group backend. Auto-detected if <code>None</code>: <code>nccl</code> when CUDA is available, <code>gloo</code> otherwise. <code>cuda</code> <code>str</code> <code>\"cu128\"</code> CUDA version suffix for the PyTorch wheel index. Determines which prebuilt wheels are pulled from <code>download.pytorch.org</code>. <code>version</code> <code>str</code> <code>\"latest\"</code> PyTorch version. <code>\"latest\"</code> installs the latest release. A bare version string (e.g. <code>\"2.3.0\"</code>) pins with <code>==</code>. Constraint prefixes like <code>\"&gt;=2.3\"</code> are passed through as-is. <code>vision</code> <code>str \\| None</code> <code>None</code> Torchvision version. Same semantics as <code>version</code>. <code>None</code> skips installation, <code>\"latest\"</code> installs the latest release. <code>audio</code> <code>str \\| None</code> <code>None</code> Torchaudio version. Same semantics as <code>version</code>. <code>None</code> skips installation, <code>\"latest\"</code> installs the latest release. <p>The <code>cuda</code> value determines the wheel index URL. When the cluster has a GPU accelerator (one with CUDA support in its metadata), the plugin uses <code>https://download.pytorch.org/whl/{cuda}</code> as the pip index. When the accelerator is <code>None</code> or does not support CUDA, it falls back to <code>https://download.pytorch.org/whl/cpu</code>. This auto-detection happens at image transform time, using the cluster's spec to decide.</p>"},{"location":"plugins/torch/#how-it-works","title":"How it works","text":""},{"location":"plugins/torch/#image-transform","title":"Image transform","text":"<p>The <code>transform</code> hook builds the pip package list and index from the parameters. It assembles the list of PyTorch packages \u2014 always <code>torch</code>, optionally <code>torchvision</code> and <code>torchaudio</code> \u2014 with their version constraints, then selects the correct pip index based on the cluster's accelerator.</p> <p>The accelerator detection uses pattern matching on <code>cluster.spec.accelerator</code>. If the cluster has an <code>Accelerator</code> with CUDA metadata, the CUDA wheel index is used. Otherwise \u2014 no accelerator, or an accelerator without CUDA support \u2014 the CPU index is used. This means you do not need to manually switch between CUDA and CPU wheels; the plugin reads the cluster configuration and does it for you.</p> <p>The packages and index are appended to the existing image using <code>replace()</code>, preserving any packages and indexes already defined in the <code>Image</code> or added by other plugins.</p>"},{"location":"plugins/torch/#worker-lifecycle-around_app","title":"Worker lifecycle (<code>around_app</code>)","text":"<p>The <code>around_app</code> hook initializes PyTorch's distributed process group once per worker process. When the first task arrives, the hook:</p> <ol> <li>Imports <code>torch</code> and <code>torch.distributed</code> (these are remote-only imports \u2014 PyTorch does not need to be installed locally).</li> <li>Reads <code>instance_info()</code> from the hook's parameter to get the cluster topology.</li> <li>If the cluster has fewer than 2 nodes, yields immediately \u2014 no distributed setup needed for single-node pools.</li> <li>Sets the environment variables: <code>MASTER_ADDR</code>, <code>MASTER_PORT</code>, <code>WORLD_SIZE</code>, <code>RANK</code>, <code>LOCAL_RANK</code> (always <code>\"0\"</code> \u2014 Skyward runs one process per node), <code>LOCAL_WORLD_SIZE</code> (always <code>\"1\"</code>), and <code>NODE_RANK</code>.</li> <li>Selects the backend: if explicitly provided, uses that; otherwise, <code>\"nccl\"</code> when <code>torch.cuda.is_available()</code> and <code>\"gloo\"</code> otherwise.</li> <li>Calls <code>dist.init_process_group(backend=..., init_method=\"env://\")</code>.</li> <li>Yields to the worker lifecycle \u2014 subsequent tasks run with the process group already active.</li> <li>On worker shutdown, calls <code>dist.destroy_process_group()</code> in the <code>finally</code> block.</li> </ol> <p>The environment variables come from <code>instance_info()</code>: <code>head_addr</code> becomes <code>MASTER_ADDR</code>, <code>head_port</code> becomes <code>MASTER_PORT</code>, <code>total_nodes</code> becomes <code>WORLD_SIZE</code>, and <code>node</code> becomes <code>RANK</code>. These values are populated from the <code>COMPUTE_POOL</code> environment variable that Skyward injects on each worker at startup.</p>"},{"location":"plugins/torch/#usage","title":"Usage","text":""},{"location":"plugins/torch/#basic-ddp-training","title":"Basic DDP training","text":"<pre><code>import skyward as sky\n\n@sky.compute\n@sky.stdout(only=\"head\")\ndef train() -&gt; dict:\n    import torch\n    import torch.distributed as dist\n    import torch.nn as nn\n    from torch.nn.parallel import DistributedDataParallel as DDP\n    from torch.utils.data import DataLoader, DistributedSampler, TensorDataset\n\n    rank = dist.get_rank()\n    world_size = dist.get_world_size()\n\n    model = nn.Linear(784, 10).cuda()\n    model = DDP(model)\n\n    x = torch.randn(1000, 784)\n    y = torch.randint(0, 10, (1000,))\n    dataset = TensorDataset(x, y)\n    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n    loader = DataLoader(dataset, batch_size=64, sampler=sampler)\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n    loss_fn = nn.CrossEntropyLoss()\n\n    for epoch in range(10):\n        sampler.set_epoch(epoch)\n        for batch_x, batch_y in loader:\n            batch_x, batch_y = batch_x.cuda(), batch_y.cuda()\n            loss = loss_fn(model(batch_x), batch_y)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n        print(f\"Epoch {epoch}: loss={loss.item():.4f}\")\n\n    return {\"final_loss\": loss.item(), \"rank\": rank}\n\nwith sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"A100\",\n    nodes=4,\n    plugins=[sky.plugins.torch()],\n) as pool:\n    results = train() @ pool\n    for r in results:\n        print(f\"Rank {r['rank']}: loss={r['final_loss']:.4f}\")\n</code></pre> <p>The <code>@</code> operator broadcasts <code>train()</code> to all 4 nodes. Each node runs the same function, but <code>dist.get_rank()</code> returns a different value (0 through 3), and <code>DistributedSampler</code> partitions the data accordingly. DDP synchronizes gradients in the backward pass, so all nodes converge on the same model parameters.</p> <p><code>@sky.stdout(only=\"head\")</code> silences print statements on non-head nodes, so you see one set of epoch logs instead of four.</p>"},{"location":"plugins/torch/#with-torchvision-and-torchaudio","title":"With torchvision and torchaudio","text":"<pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"A100\",\n    nodes=2,\n    plugins=[sky.plugins.torch(vision=\"latest\", audio=\"latest\")],\n) as pool:\n    results = train() @ pool\n</code></pre> <p>This installs <code>torch</code>, <code>torchvision</code>, and <code>torchaudio</code> from the CUDA wheel index. Inside the function, you can import <code>torchvision.models</code>, <code>torchvision.transforms</code>, <code>torchaudio</code>, etc.</p>"},{"location":"plugins/torch/#pinning-versions","title":"Pinning versions","text":"<pre><code>plugins=[sky.plugins.torch(version=\"2.3.0\", vision=\"0.18.0\", cuda=\"cu124\")]\n</code></pre> <p>This pins <code>torch==2.3.0</code> and <code>torchvision==0.18.0</code>, installed from the CUDA 12.4 wheel index. Version pinning is important for reproducibility \u2014 different PyTorch versions can produce different training results due to changes in default behaviors, numerical stability, and operator implementations.</p>"},{"location":"plugins/torch/#cpu-only","title":"CPU-only","text":"<pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    nodes=4,\n    plugins=[sky.plugins.torch(backend=\"gloo\")],\n) as pool:\n    results = train() @ pool\n</code></pre> <p>Without an <code>accelerator</code>, the pool uses CPU instances. The plugin detects the absence of a CUDA accelerator and installs the CPU-only PyTorch wheels from <code>download.pytorch.org/whl/cpu</code>. The <code>backend=\"gloo\"</code> is explicit here \u2014 gloo is PyTorch's CPU-compatible collective communication backend.</p>"},{"location":"plugins/torch/#combining-with-huggingface","title":"Combining with HuggingFace","text":"<pre><code>with sky.ComputePool(\n    provider=sky.AWS(),\n    accelerator=\"A100\",\n    nodes=2,\n    plugins=[\n        sky.plugins.torch(),\n        sky.plugins.huggingface(token=\"hf_xxx\"),\n    ],\n) as pool:\n    results = finetune() @ pool\n</code></pre> <p>The <code>torch</code> plugin handles DDP initialization, and the <code>huggingface</code> plugin handles authentication and installs <code>transformers</code>, <code>datasets</code>, and <code>tokenizers</code>. Inside the function, HuggingFace's <code>Trainer</code> auto-detects the distributed environment set up by the torch plugin and uses it for distributed training, gradient synchronization, and distributed evaluation.</p>"},{"location":"plugins/torch/#next-steps","title":"Next steps","text":"<ul> <li>PyTorch Distributed guide \u2014 Step-by-step DDP training walkthrough</li> <li>PyTorch Model Roundtrip guide \u2014 Sending models to and from the cloud</li> <li>HuggingFace plugin \u2014 Fine-tuning Transformers on multiple nodes</li> <li>What are Plugins? \u2014 How the plugin system works</li> <li>JAX plugin \u2014 The JAX equivalent for comparison</li> </ul>"},{"location":"reference/accelerators/","title":"Accelerators","text":""},{"location":"reference/accelerators/#skyward.accelerators","title":"<code>skyward.accelerators</code>","text":"<p>Accelerator specifications for Skyward v2.</p> <p>Provides type-safe accelerator configurations with full IDE autocomplete. Each accelerator factory returns an immutable Accelerator dataclass with defaults from the catalog.</p> <p>Usage:     import skyward as sky</p> <pre><code># Via namespace (recommended)\nsky.accelerators.H100()              # Default: 80GB, count=1\nsky.accelerators.H100(count=4)       # 4x H100\nsky.accelerators.A100(memory=\"40GB\") # A100 40GB variant\n\n# Direct import\nfrom skyward.accelerators import H100, A100, T4\nh100 = H100(count=8)\n\n# Custom accelerator\nfrom skyward.accelerators import Custom\nmy_gpu = Custom(\"My-GPU\", memory=\"48GB\")\n</code></pre> <p>Available accelerators:     - NVIDIA Datacenter: H100, H200, GH200, B100, B200, GB200,       A100, A800, A40, A10, A10G, A2, L4, L40, L40S, T4, V100, P100, K80     - NVIDIA Consumer: RTX_5090-RTX_5060, RTX_4090-RTX_4060,       RTX_3090-RTX_3050, RTX_2080_Ti-RTX_2060, GTX series     - NVIDIA Workstation: RTX_A6000-RTX_A2000, Quadro series     - AMD Instinct: MI300X, MI300A, MI250X, MI250, MI210, MI100, MI50     - AWS: Trainium1-3, Inferentia1-2     - Habana: Gaudi, Gaudi2, Gaudi3     - Google TPU: TPUv2-TPUv6, TPU slices</p>"},{"location":"reference/accelerators/#skyward.accelerators.AMD","title":"<code>AMD = Literal['MI50', 'MI100', 'MI210', 'MI250', 'MI250X', 'MI300A', 'MI300B', 'MI300X', 'Instinct-MI25', 'RadeonPro-V520', 'RadeonPro-V710']</code>  <code>module-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.GPU","title":"<code>GPU = NVIDIA | AMD | None</code>  <code>module-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.NVIDIA","title":"<code>NVIDIA = Literal['K80', 'P4', 'P40', 'P100', 'Quadro P4000', 'Titan Xp', 'V100', 'Titan V', 'T4', 'A2', 'A10', 'A10G', 'A40', 'A100-40', 'A100-80', 'A100-40GB', 'A100-80GB', 'A800', 'L4', 'L40', 'L40S', 'H100-80GB', 'H100-NVL', 'H100-SXM', 'H100-PCIe', 'H200', 'H200-NVL', 'B100', 'B200', 'GB200', 'GH200', 'GTX 1060', 'GTX 1070', 'GTX 1070 Ti', 'GTX 1080', 'GTX 1080 Ti', 'GTX 1660', 'GTX 1660 Super', 'GTX 1660 Ti', 'RTX 2060', 'RTX 2060 Super', 'RTX 2070', 'RTX 2070 Super', 'RTX 2080', 'RTX 2080 Super', 'RTX 2080 Ti', 'RTX 3050', 'RTX 3060', 'RTX 3060 Ti', 'RTX 3060 Laptop', 'RTX 3070', 'RTX 3070 Ti', 'RTX 3080', 'RTX 3080 Ti', 'RTX 3090', 'RTX 3090 Ti', 'RTX 4060', 'RTX 4060 Ti', 'RTX 4070', 'RTX 4070 Super', 'RTX 4070 Ti', 'RTX 4070 Ti Super', 'RTX 4080', 'RTX 4080 Super', 'RTX 4090', 'RTX 4090D', 'RTX 5060', 'RTX 5060 Ti', 'RTX 5070', 'RTX 5070 Ti', 'RTX 5080', 'RTX 5090', 'Quadro RTX 4000', 'Quadro RTX 6000', 'Quadro RTX 8000', 'RTX A2000', 'RTX A4000', 'RTX A5000', 'RTX A6000', 'RTX 5000 Ada', 'RTX 5880 Ada', 'RTX 6000 Ada', 'RTX PRO 4000', 'RTX PRO 6000']</code>  <code>module-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.TPU","title":"<code>TPU = Literal['TPUv2', 'TPUv3', 'TPUv4', 'TPUv5e', 'TPUv5p', 'TPUv6', 'TPUv2-8', 'TPUv3-8', 'TPUv3-32', 'TPUv4-64', 'TPUv5e-4', 'TPUv5p-8']</code>  <code>module-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.Habana","title":"<code>Habana = Literal['Gaudi', 'Gaudi2', 'Gaudi3']</code>  <code>module-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.Inferentia","title":"<code>Inferentia = Literal['Inferentia1', 'Inferentia2']</code>  <code>module-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.Trainium","title":"<code>Trainium = Literal['Trainium1', 'Trainium2', 'Trainium3']</code>  <code>module-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.__all__","title":"<code>__all__ = ['Accelerator', 'NVIDIA', 'AMD', 'TPU', 'Habana', 'Trainium', 'Inferentia', 'GPU', 'Custom', 'H100', 'H200', 'GH200', 'B100', 'B200', 'GB200', 'A100', 'A800', 'A40', 'A10', 'A10G', 'A2', 'L4', 'L40', 'L40S', 'T4', 'T4G', 'V100', 'P100', 'P40', 'P4', 'K80', 'RTX_5090', 'RTX_5080', 'RTX_5070_Ti', 'RTX_5070', 'RTX_5060_Ti', 'RTX_5060', 'RTX_4090', 'RTX_4090D', 'RTX_4080_Super', 'RTX_4080', 'RTX_4070_Ti_Super', 'RTX_4070_Ti', 'RTX_4070_Super', 'RTX_4070', 'RTX_4060_Ti', 'RTX_4060', 'RTX_3090_Ti', 'RTX_3090', 'RTX_3080_Ti', 'RTX_3080', 'RTX_3070_Ti', 'RTX_3070', 'RTX_3060_Ti', 'RTX_3060', 'RTX_3060_Laptop', 'RTX_3050', 'RTX_2080_Ti', 'RTX_2080_Super', 'RTX_2080', 'RTX_2070_Super', 'RTX_2070', 'RTX_2060_Super', 'RTX_2060', 'GTX_1660_Ti', 'GTX_1660_Super', 'GTX_1660', 'GTX_1080_Ti', 'GTX_1080', 'GTX_1070_Ti', 'GTX_1070', 'GTX_1060', 'Titan_Xp', 'Titan_V', 'RTX_6000_Ada', 'RTX_5880_Ada', 'RTX_5000_Ada', 'RTX_PRO_6000', 'RTX_PRO_4000', 'RTX_A6000', 'RTX_A5000', 'RTX_A4000', 'RTX_A2000', 'Quadro_RTX_8000', 'Quadro_RTX_6000', 'Quadro_RTX_4000', 'Quadro_P4000', 'MI300X', 'MI300B', 'MI300A', 'MI250X', 'MI250', 'MI210', 'MI100', 'MI50', 'RadeonPro_V710', 'RadeonPro_V520', 'Instinct_MI25', 'Trainium1', 'Trainium2', 'Trainium3', 'Inferentia1', 'Inferentia2', 'Gaudi', 'Gaudi2', 'Gaudi3', 'TPUv2', 'TPUv3', 'TPUv4', 'TPUv5e', 'TPUv5p', 'TPUv6', 'TPUv2_8', 'TPUv3_8', 'TPUv3_32', 'TPUv4_64', 'TPUv5e_4', 'TPUv5p_8']</code>  <code>module-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.Accelerator","title":"<code>Accelerator</code>  <code>dataclass</code>","text":"<p>Immutable accelerator specification.</p> <p>Represents a GPU/TPU/accelerator configuration with memory, count, and optional metadata (CUDA versions, etc.).</p> <p>Args:     name: Accelerator name (e.g., \"H100\", \"A100\", \"T4\").     memory: VRAM size (e.g., \"80GB\", \"40GB\").     count: Number of accelerators per node.     metadata: Additional info (CUDA versions, form factor, etc.).</p> <p>Examples:     &gt;&gt;&gt; Accelerator(\"H100\")     Accelerator(name='H100', memory='', count=1, metadata=None)</p> <pre><code>&gt;&gt;&gt; Accelerator(\"A100\", memory=\"40GB\", count=4)\nAccelerator(name='A100', memory='40GB', count=4, metadata=None)\n\n&gt;&gt;&gt; Accelerator.from_name(\"H100\", count=8)\nAccelerator(name='H100', memory='80GB', count=8, metadata={'cuda': ...})\n</code></pre>"},{"location":"reference/accelerators/#skyward.accelerators.Accelerator.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.Accelerator.memory","title":"<code>memory = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.Accelerator.count","title":"<code>count = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.Accelerator.metadata","title":"<code>metadata = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.Accelerator.from_name","title":"<code>from_name(name, **overrides)</code>  <code>classmethod</code>","text":"<p>Create an Accelerator from the catalog with optional overrides.</p> <p>Looks up the accelerator in the v1 catalog and applies any provided overrides.</p> <p>Args:     name: Accelerator name from the catalog (e.g., \"H100\", \"T4\").     **overrides: Override memory, count, or metadata.</p> <p>Returns:     Accelerator instance with defaults from catalog.</p> <p>Raises:     ValueError: If the accelerator name is not in the catalog.</p> <p>Examples:     &gt;&gt;&gt; Accelerator.from_name(\"T4\")     Accelerator(name='T4', memory='16GB', count=1, ...)</p> <pre><code>&gt;&gt;&gt; Accelerator.from_name(\"H100\", count=4)\nAccelerator(name='H100', memory='80GB', count=4, ...)\n</code></pre>"},{"location":"reference/accelerators/#skyward.accelerators.Accelerator.with_count","title":"<code>with_count(count)</code>","text":"<p>Return a new Accelerator with a different count.</p> <p>Args:     count: Number of accelerators per node.</p> <p>Returns:     New Accelerator instance with updated count.</p> <p>Example:     &gt;&gt;&gt; h100 = Accelerator.from_name(\"H100\")     &gt;&gt;&gt; h100_x4 = h100.with_count(4)</p>"},{"location":"reference/accelerators/#skyward.accelerators.Accelerator.__str__","title":"<code>__str__()</code>","text":"<p>Human-readable representation.</p>"},{"location":"reference/accelerators/#skyward.accelerators.Accelerator.__repr__","title":"<code>__repr__()</code>","text":"<p>Detailed representation for debugging.</p>"},{"location":"reference/accelerators/#skyward.accelerators.Accelerator.__init__","title":"<code>__init__(name, memory='', count=1, metadata=None)</code>","text":""},{"location":"reference/accelerators/#skyward.accelerators.A2","title":"<code>A2(*, count=1)</code>","text":"<p>NVIDIA A2 - Entry-level Ampere for inference.</p> <p>16GB GDDR6 memory, low power consumption.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for A2.</p>"},{"location":"reference/accelerators/#skyward.accelerators.A10","title":"<code>A10(*, count=1)</code>","text":"<p>NVIDIA A10 - Inference-optimized Ampere GPU.</p> <p>24GB GDDR6 memory, popular for inference workloads.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for A10.</p>"},{"location":"reference/accelerators/#skyward.accelerators.A10G","title":"<code>A10G(*, count=1)</code>","text":"<p>NVIDIA A10G - AWS-specific A10 variant.</p> <p>24GB GDDR6 memory, available on g5 instances.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for A10G.</p>"},{"location":"reference/accelerators/#skyward.accelerators.A40","title":"<code>A40(*, count=1)</code>","text":"<p>NVIDIA A40 - Professional visualization + compute (2020).</p> <p>48GB GDDR6 memory, optimized for virtual workstations.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for A40.</p>"},{"location":"reference/accelerators/#skyward.accelerators.A100","title":"<code>A100(*, memory='80GB', count=1)</code>","text":"<p>NVIDIA A100 - Ampere architecture (2020).</p> <p>The A100 was the first GPU with TF32 and structural sparsity. Available in 40GB PCIe and 80GB SXM variants.</p> <p>Args:     memory: VRAM size - 40GB or 80GB.     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for A100.</p>"},{"location":"reference/accelerators/#skyward.accelerators.A800","title":"<code>A800(*, count=1)</code>","text":"<p>NVIDIA A800 - China-specific A100 variant.</p> <p>Reduced NVLink bandwidth to comply with export restrictions. Same 80GB HBM2e memory as A100.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for A800.</p>"},{"location":"reference/accelerators/#skyward.accelerators.B100","title":"<code>B100(*, count=1)</code>","text":"<p>NVIDIA B100 - Blackwell architecture (2024).</p> <p>Second-generation transformer engine with FP4 support. 192GB HBM3e memory.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for B100.</p>"},{"location":"reference/accelerators/#skyward.accelerators.B200","title":"<code>B200(*, count=1)</code>","text":"<p>NVIDIA B200 - Blackwell architecture (2024).</p> <p>Flagship Blackwell GPU with 192GB HBM3e. Up to 2.5x inference performance vs H100.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for B200.</p>"},{"location":"reference/accelerators/#skyward.accelerators.GB200","title":"<code>GB200(*, count=1)</code>","text":"<p>NVIDIA Grace Blackwell Superchip (2024).</p> <p>Combines Grace CPU with two Blackwell GPUs. 384GB combined HBM3e memory.</p> <p>Args:     count: Number of superchips per node.</p> <p>Returns:     Accelerator specification for GB200.</p>"},{"location":"reference/accelerators/#skyward.accelerators.GH200","title":"<code>GH200(*, count=1)</code>","text":"<p>NVIDIA Grace Hopper Superchip (2023).</p> <p>Combines Grace CPU with Hopper GPU via NVLink-C2C. 96GB HBM3 for GPU, 480GB LPDDR5X for CPU.</p> <p>Args:     count: Number of superchips per node.</p> <p>Returns:     Accelerator specification for GH200.</p>"},{"location":"reference/accelerators/#skyward.accelerators.GTX_1060","title":"<code>GTX_1060(*, count=1)</code>","text":"<p>NVIDIA GTX 1060 - Pascal entry (2016).</p>"},{"location":"reference/accelerators/#skyward.accelerators.GTX_1070","title":"<code>GTX_1070(*, count=1)</code>","text":"<p>NVIDIA GTX 1070 - Pascal (2016).</p>"},{"location":"reference/accelerators/#skyward.accelerators.GTX_1080","title":"<code>GTX_1080(*, count=1)</code>","text":"<p>NVIDIA GTX 1080 - Pascal high-end (2016).</p>"},{"location":"reference/accelerators/#skyward.accelerators.GTX_1660","title":"<code>GTX_1660(*, count=1)</code>","text":"<p>NVIDIA GTX 1660 - Turing without RT cores (2019).</p>"},{"location":"reference/accelerators/#skyward.accelerators.H100","title":"<code>H100(*, memory='80GB', form_factor=None, count=1)</code>","text":"<p>NVIDIA H100 - Hopper architecture (2022).</p> <p>The H100 is NVIDIA's flagship datacenter GPU for AI/ML workloads. Supports FP8, with massive improvements for transformer models.</p> <p>Args:     memory: VRAM size - 40GB (PCIe) or 80GB (SXM/NVL).     form_factor: SXM (high bandwidth), PCIe, or NVL (2x GPU module).     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for H100.</p>"},{"location":"reference/accelerators/#skyward.accelerators.H200","title":"<code>H200(*, memory='141GB', form_factor=None, count=1)</code>","text":"<p>NVIDIA H200 - Hopper architecture with HBM3e (2024).</p> <p>H200 features 141GB HBM3e memory with 4.8TB/s bandwidth. Drop-in replacement for H100 with 1.4-1.9x inference speedup.</p> <p>Args:     memory: VRAM size (141GB for standard, varies for NVL).     form_factor: SXM or NVL variant.     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for H200.</p>"},{"location":"reference/accelerators/#skyward.accelerators.K80","title":"<code>K80(*, count=1)</code>","text":"<p>NVIDIA K80 - Kepler architecture (2014).</p> <p>12GB GDDR5 memory per GPU (24GB total, dual-GPU card). Legacy GPU, CUDA support ends at 10.2.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for K80.</p>"},{"location":"reference/accelerators/#skyward.accelerators.L4","title":"<code>L4(*, count=1)</code>","text":"<p>NVIDIA L4 - Ada Lovelace inference GPU (2023).</p> <p>24GB GDDR6 memory, replaces T4 for inference. Excellent price/performance for video and LLM inference.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for L4.</p>"},{"location":"reference/accelerators/#skyward.accelerators.L40","title":"<code>L40(*, count=1)</code>","text":"<p>NVIDIA L40 - Ada Lovelace professional GPU (2023).</p> <p>48GB GDDR6 memory, for visualization and compute.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for L40.</p>"},{"location":"reference/accelerators/#skyward.accelerators.L40S","title":"<code>L40S(*, count=1)</code>","text":"<p>NVIDIA L40S - Ada Lovelace compute GPU (2023).</p> <p>48GB GDDR6 memory, optimized for AI/ML workloads. Higher power limit than L40 for sustained compute.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for L40S.</p>"},{"location":"reference/accelerators/#skyward.accelerators.MI50","title":"<code>MI50(*, count=1)</code>","text":"<p>AMD Instinct MI50 - Vega (2018).</p>"},{"location":"reference/accelerators/#skyward.accelerators.MI100","title":"<code>MI100(*, count=1)</code>","text":"<p>AMD Instinct MI100 - CDNA 1 (2020).</p>"},{"location":"reference/accelerators/#skyward.accelerators.MI210","title":"<code>MI210(*, count=1)</code>","text":"<p>AMD Instinct MI210 - CDNA 2 (2022).</p>"},{"location":"reference/accelerators/#skyward.accelerators.MI250","title":"<code>MI250(*, count=1)</code>","text":"<p>AMD Instinct MI250 - CDNA 2 (2021).</p>"},{"location":"reference/accelerators/#skyward.accelerators.MI250X","title":"<code>MI250X(*, count=1)</code>","text":"<p>AMD Instinct MI250X - CDNA 2 flagship (2021).</p>"},{"location":"reference/accelerators/#skyward.accelerators.MI300A","title":"<code>MI300A(*, count=1)</code>","text":"<p>AMD Instinct MI300A - CDNA 3 APU (2023).</p> <p>Integrated CPU + GPU on single package.</p>"},{"location":"reference/accelerators/#skyward.accelerators.MI300B","title":"<code>MI300B(*, count=1)</code>","text":"<p>AMD Instinct MI300B - CDNA 3 (2023).</p>"},{"location":"reference/accelerators/#skyward.accelerators.MI300X","title":"<code>MI300X(*, count=1)</code>","text":"<p>AMD Instinct MI300X - CDNA 3 flagship (2023).</p> <p>192GB HBM3 memory, designed for large language models.</p>"},{"location":"reference/accelerators/#skyward.accelerators.P4","title":"<code>P4(*, count=1)</code>","text":"<p>NVIDIA P4 - Pascal inference GPU (2016).</p> <p>8GB GDDR5 memory, low-power inference.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for P4.</p>"},{"location":"reference/accelerators/#skyward.accelerators.P40","title":"<code>P40(*, count=1)</code>","text":"<p>NVIDIA P40 - Pascal inference GPU (2016).</p> <p>24GB GDDR5 memory, INT8 inference support.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for P40.</p>"},{"location":"reference/accelerators/#skyward.accelerators.P100","title":"<code>P100(*, count=1)</code>","text":"<p>NVIDIA P100 - Pascal architecture (2016).</p> <p>16GB HBM2 memory, first HBM GPU for deep learning.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for P100.</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_2060","title":"<code>RTX_2060(*, count=1)</code>","text":"<p>NVIDIA RTX 2060 - Turing entry (2019).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_2070","title":"<code>RTX_2070(*, count=1)</code>","text":"<p>NVIDIA RTX 2070 - Turing (2018).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_2080","title":"<code>RTX_2080(*, count=1)</code>","text":"<p>NVIDIA RTX 2080 - Turing high-end (2018).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_3050","title":"<code>RTX_3050(*, count=1)</code>","text":"<p>NVIDIA RTX 3050 - Ampere entry (2022).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_3060","title":"<code>RTX_3060(*, count=1)</code>","text":"<p>NVIDIA RTX 3060 - Ampere (2021).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_3070","title":"<code>RTX_3070(*, count=1)</code>","text":"<p>NVIDIA RTX 3070 - Ampere (2020).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_3080","title":"<code>RTX_3080(*, count=1)</code>","text":"<p>NVIDIA RTX 3080 - Ampere high-end (2020).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_3090","title":"<code>RTX_3090(*, count=1)</code>","text":"<p>NVIDIA RTX 3090 - Ampere flagship (2020).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_4060","title":"<code>RTX_4060(*, count=1)</code>","text":"<p>NVIDIA RTX 4060 - Ada Lovelace entry (2023).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_4070","title":"<code>RTX_4070(*, count=1)</code>","text":"<p>NVIDIA RTX 4070 - Ada Lovelace (2023).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_4080","title":"<code>RTX_4080(*, count=1)</code>","text":"<p>NVIDIA RTX 4080 - Ada Lovelace high-end (2022).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_4090","title":"<code>RTX_4090(*, count=1)</code>","text":"<p>NVIDIA RTX 4090 - Ada Lovelace consumer flagship (2022).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_4090D","title":"<code>RTX_4090D(*, count=1)</code>","text":"<p>NVIDIA RTX 4090D - China-specific variant.</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_5060","title":"<code>RTX_5060(*, count=1)</code>","text":"<p>NVIDIA RTX 5060 - Blackwell consumer entry (2025).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_5070","title":"<code>RTX_5070(*, count=1)</code>","text":"<p>NVIDIA RTX 5070 - Blackwell consumer (2025).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_5080","title":"<code>RTX_5080(*, count=1)</code>","text":"<p>NVIDIA RTX 5080 - Blackwell consumer high-end (2025).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_5090","title":"<code>RTX_5090(*, count=1)</code>","text":"<p>NVIDIA RTX 5090 - Blackwell consumer flagship (2025).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_A2000","title":"<code>RTX_A2000(*, count=1)</code>","text":"<p>NVIDIA RTX A2000 - Ampere workstation entry (2021).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_A4000","title":"<code>RTX_A4000(*, count=1)</code>","text":"<p>NVIDIA RTX A4000 - Ampere workstation (2021).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_A5000","title":"<code>RTX_A5000(*, count=1)</code>","text":"<p>NVIDIA RTX A5000 - Ampere workstation (2021).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_A6000","title":"<code>RTX_A6000(*, count=1)</code>","text":"<p>NVIDIA RTX A6000 - Ampere workstation flagship (2020).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_PRO_4000","title":"<code>RTX_PRO_4000(*, count=1)</code>","text":"<p>NVIDIA RTX PRO 4000 - Blackwell workstation (2025).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_PRO_6000","title":"<code>RTX_PRO_6000(*, count=1)</code>","text":"<p>NVIDIA RTX PRO 6000 - Blackwell workstation (2025).</p>"},{"location":"reference/accelerators/#skyward.accelerators.T4","title":"<code>T4(*, count=1)</code>","text":"<p>NVIDIA T4 - Turing inference GPU (2018).</p> <p>16GB GDDR6 memory, excellent cost/performance for inference. Widely available on all major clouds (x86_64).</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for T4.</p>"},{"location":"reference/accelerators/#skyward.accelerators.T4G","title":"<code>T4G(*, count=1)</code>","text":"<p>NVIDIA T4G - Turing inference GPU for ARM64 (2021).</p> <p>16GB GDDR6 memory, same as T4 but for Graviton instances. Available on AWS g5g instances.</p> <p>Args:     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for T4G.</p>"},{"location":"reference/accelerators/#skyward.accelerators.V100","title":"<code>V100(*, memory='32GB', count=1)</code>","text":"<p>NVIDIA V100 - Volta architecture (2017).</p> <p>First GPU with Tensor Cores. Available in 16GB and 32GB variants. Still widely used for training workloads.</p> <p>Args:     memory: VRAM size - 16GB or 32GB.     count: Number of GPUs per node.</p> <p>Returns:     Accelerator specification for V100.</p>"},{"location":"reference/accelerators/#skyward.accelerators.Custom","title":"<code>Custom(name, *, memory='', count=1, cuda_min=None, cuda_max=None)</code>","text":"<p>Create a custom accelerator specification.</p> <p>Use this for accelerators not in the catalog, or when you need to override the default specifications.</p> <p>Args:     name: Accelerator name (e.g., \"Custom-GPU\", \"My-TPU\").     memory: VRAM size (e.g., \"48GB\").     count: Number of accelerators per node.     cuda_min: Minimum CUDA version (e.g., \"11.8\").     cuda_max: Maximum CUDA version (e.g., \"13.1\").</p> <p>Returns:     Accelerator specification with custom parameters.</p> <p>Examples:     &gt;&gt;&gt; Custom(\"My-GPU\", memory=\"48GB\")     Accelerator(name='My-GPU', memory='48GB')</p> <pre><code>&gt;&gt;&gt; Custom(\"Experimental-TPU\", memory=\"128GB\", count=8)\nAccelerator(name='Experimental-TPU', memory='128GB', count=8)\n\n&gt;&gt;&gt; Custom(\"H100-Custom\", memory=\"80GB\", cuda_min=\"12.0\", cuda_max=\"13.1\")\nAccelerator(name='H100-Custom', memory='80GB', ...)\n</code></pre>"},{"location":"reference/accelerators/#skyward.accelerators.Gaudi","title":"<code>Gaudi(*, count=1)</code>","text":"<p>Habana Gaudi - First-gen training accelerator (2019).</p>"},{"location":"reference/accelerators/#skyward.accelerators.Gaudi2","title":"<code>Gaudi2(*, count=1)</code>","text":"<p>Habana Gaudi2 - Second-gen training accelerator (2022).</p> <p>96GB HBM2e memory, 2x performance vs Gaudi.</p>"},{"location":"reference/accelerators/#skyward.accelerators.Gaudi3","title":"<code>Gaudi3(*, count=1)</code>","text":"<p>Habana Gaudi3 - Third-gen training accelerator (2024).</p> <p>128GB HBM2e memory.</p>"},{"location":"reference/accelerators/#skyward.accelerators.GTX_1070_Ti","title":"<code>GTX_1070_Ti(*, count=1)</code>","text":"<p>NVIDIA GTX 1070 Ti - Pascal (2017).</p>"},{"location":"reference/accelerators/#skyward.accelerators.GTX_1080_Ti","title":"<code>GTX_1080_Ti(*, count=1)</code>","text":"<p>NVIDIA GTX 1080 Ti - Pascal flagship (2017).</p>"},{"location":"reference/accelerators/#skyward.accelerators.GTX_1660_Super","title":"<code>GTX_1660_Super(*, count=1)</code>","text":"<p>NVIDIA GTX 1660 Super - Turing refresh (2019).</p>"},{"location":"reference/accelerators/#skyward.accelerators.GTX_1660_Ti","title":"<code>GTX_1660_Ti(*, count=1)</code>","text":"<p>NVIDIA GTX 1660 Ti - Turing without RT cores (2019).</p>"},{"location":"reference/accelerators/#skyward.accelerators.Inferentia1","title":"<code>Inferentia1(*, count=1)</code>","text":"<p>AWS Inferentia v1 - First-gen inference accelerator.</p> <p>8GB memory, available on inf1 instances.</p>"},{"location":"reference/accelerators/#skyward.accelerators.Inferentia2","title":"<code>Inferentia2(*, count=1)</code>","text":"<p>AWS Inferentia v2 - Second-gen inference accelerator.</p> <p>32GB HBM memory, available on inf2 instances.</p>"},{"location":"reference/accelerators/#skyward.accelerators.Instinct_MI25","title":"<code>Instinct_MI25(*, count=1)</code>","text":"<p>AMD Instinct MI25 - Vega (2017).</p>"},{"location":"reference/accelerators/#skyward.accelerators.Quadro_P4000","title":"<code>Quadro_P4000(*, count=1)</code>","text":"<p>NVIDIA Quadro P4000 - Pascal workstation (2016).</p>"},{"location":"reference/accelerators/#skyward.accelerators.Quadro_RTX_4000","title":"<code>Quadro_RTX_4000(*, count=1)</code>","text":"<p>NVIDIA Quadro RTX 4000 - Turing workstation (2018).</p>"},{"location":"reference/accelerators/#skyward.accelerators.Quadro_RTX_6000","title":"<code>Quadro_RTX_6000(*, count=1)</code>","text":"<p>NVIDIA Quadro RTX 6000 - Turing workstation (2018).</p>"},{"location":"reference/accelerators/#skyward.accelerators.Quadro_RTX_8000","title":"<code>Quadro_RTX_8000(*, count=1)</code>","text":"<p>NVIDIA Quadro RTX 8000 - Turing workstation (2018).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RadeonPro_V520","title":"<code>RadeonPro_V520(*, count=1)</code>","text":"<p>AMD Radeon Pro V520 - Streaming GPU.</p>"},{"location":"reference/accelerators/#skyward.accelerators.RadeonPro_V710","title":"<code>RadeonPro_V710(*, count=1)</code>","text":"<p>AMD Radeon Pro V710 - Streaming GPU.</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_2060_Super","title":"<code>RTX_2060_Super(*, count=1)</code>","text":"<p>NVIDIA RTX 2060 Super - Turing refresh (2019).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_2070_Super","title":"<code>RTX_2070_Super(*, count=1)</code>","text":"<p>NVIDIA RTX 2070 Super - Turing refresh (2019).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_2080_Super","title":"<code>RTX_2080_Super(*, count=1)</code>","text":"<p>NVIDIA RTX 2080 Super - Turing refresh (2019).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_2080_Ti","title":"<code>RTX_2080_Ti(*, count=1)</code>","text":"<p>NVIDIA RTX 2080 Ti - Turing flagship (2018).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_3060_Laptop","title":"<code>RTX_3060_Laptop(*, count=1)</code>","text":"<p>NVIDIA RTX 3060 Laptop - Mobile Ampere.</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_3060_Ti","title":"<code>RTX_3060_Ti(*, count=1)</code>","text":"<p>NVIDIA RTX 3060 Ti - Ampere (2020).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_3070_Ti","title":"<code>RTX_3070_Ti(*, count=1)</code>","text":"<p>NVIDIA RTX 3070 Ti - Ampere (2021).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_3080_Ti","title":"<code>RTX_3080_Ti(*, count=1)</code>","text":"<p>NVIDIA RTX 3080 Ti - Ampere high-end (2021).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_3090_Ti","title":"<code>RTX_3090_Ti(*, count=1)</code>","text":"<p>NVIDIA RTX 3090 Ti - Ampere flagship (2022).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_4060_Ti","title":"<code>RTX_4060_Ti(*, count=1)</code>","text":"<p>NVIDIA RTX 4060 Ti - Ada Lovelace (2023).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_4070_Super","title":"<code>RTX_4070_Super(*, count=1)</code>","text":"<p>NVIDIA RTX 4070 Super - Ada Lovelace refresh (2024).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_4070_Ti","title":"<code>RTX_4070_Ti(*, count=1)</code>","text":"<p>NVIDIA RTX 4070 Ti - Ada Lovelace (2023).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_4070_Ti_Super","title":"<code>RTX_4070_Ti_Super(*, count=1)</code>","text":"<p>NVIDIA RTX 4070 Ti Super - Ada Lovelace refresh (2024).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_4080_Super","title":"<code>RTX_4080_Super(*, count=1)</code>","text":"<p>NVIDIA RTX 4080 Super - Ada Lovelace refresh (2024).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_5000_Ada","title":"<code>RTX_5000_Ada(*, count=1)</code>","text":"<p>NVIDIA RTX 5000 Ada - Workstation (2023).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_5060_Ti","title":"<code>RTX_5060_Ti(*, count=1)</code>","text":"<p>NVIDIA RTX 5060 Ti - Blackwell consumer (2025).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_5070_Ti","title":"<code>RTX_5070_Ti(*, count=1)</code>","text":"<p>NVIDIA RTX 5070 Ti - Blackwell consumer (2025).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_5880_Ada","title":"<code>RTX_5880_Ada(*, count=1)</code>","text":"<p>NVIDIA RTX 5880 Ada - Workstation (2023).</p>"},{"location":"reference/accelerators/#skyward.accelerators.RTX_6000_Ada","title":"<code>RTX_6000_Ada(*, count=1)</code>","text":"<p>NVIDIA RTX 6000 Ada - Workstation flagship (2022).</p>"},{"location":"reference/accelerators/#skyward.accelerators.Titan_V","title":"<code>Titan_V(*, count=1)</code>","text":"<p>NVIDIA Titan V - Volta consumer (2017).</p>"},{"location":"reference/accelerators/#skyward.accelerators.Titan_Xp","title":"<code>Titan_Xp(*, count=1)</code>","text":"<p>NVIDIA Titan Xp - Pascal workstation (2017).</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv2","title":"<code>TPUv2(*, count=1)</code>","text":"<p>Google TPU v2 - Second-gen tensor processing unit (2017).</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv2_8","title":"<code>TPUv2_8(*, count=1)</code>","text":"<p>Google TPU v2-8 - 8-chip slice.</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv3","title":"<code>TPUv3(*, count=1)</code>","text":"<p>Google TPU v3 - Third-gen TPU (2018).</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv3_8","title":"<code>TPUv3_8(*, count=1)</code>","text":"<p>Google TPU v3-8 - 8-chip slice.</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv3_32","title":"<code>TPUv3_32(*, count=1)</code>","text":"<p>Google TPU v3-32 - 32-chip slice.</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv4","title":"<code>TPUv4(*, count=1)</code>","text":"<p>Google TPU v4 - Fourth-gen TPU (2021).</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv4_64","title":"<code>TPUv4_64(*, count=1)</code>","text":"<p>Google TPU v4-64 - 64-chip slice.</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv5e","title":"<code>TPUv5e(*, count=1)</code>","text":"<p>Google TPU v5e - Efficiency-optimized TPU (2023).</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv5e_4","title":"<code>TPUv5e_4(*, count=1)</code>","text":"<p>Google TPU v5e-4 - 4-chip slice.</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv5p","title":"<code>TPUv5p(*, count=1)</code>","text":"<p>Google TPU v5p - Performance-optimized TPU (2023).</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv5p_8","title":"<code>TPUv5p_8(*, count=1)</code>","text":"<p>Google TPU v5p-8 - 8-chip slice.</p>"},{"location":"reference/accelerators/#skyward.accelerators.TPUv6","title":"<code>TPUv6(*, count=1)</code>","text":"<p>Google TPU v6 - Sixth-gen TPU (2024).</p>"},{"location":"reference/accelerators/#skyward.accelerators.Trainium1","title":"<code>Trainium1(*, count=1)</code>","text":"<p>AWS Trainium v1 - First-gen training accelerator.</p> <p>32GB HBM memory, available on trn1 instances.</p>"},{"location":"reference/accelerators/#skyward.accelerators.Trainium2","title":"<code>Trainium2(*, count=1)</code>","text":"<p>AWS Trainium v2 - Second-gen training accelerator.</p> <p>64GB HBM memory, 4x performance vs Trainium1.</p>"},{"location":"reference/accelerators/#skyward.accelerators.Trainium3","title":"<code>Trainium3(*, count=1)</code>","text":"<p>AWS Trainium v3 - Third-gen training accelerator.</p> <p>128GB HBM memory.</p>"},{"location":"reference/config/","title":"Configuration","text":""},{"location":"reference/config/#skyward.PoolSpec","title":"<code>skyward.PoolSpec</code>  <code>dataclass</code>","text":"<p>Pool specification - what the user wants.</p> <p>Defines the cluster configuration including number of nodes, hardware requirements, and instance allocation strategy.</p> <p>Args:     nodes: Number of nodes in the cluster.     accelerator: GPU/accelerator type - either a string (e.g., \"A100\", \"H100\")         or an Accelerator instance from skyward.accelerators.     region: Cloud region for instances.     vcpus: Minimum vCPUs per node.     memory_gb: Minimum memory in GB per node.     architecture: CPU architecture (\"x86_64\" or \"arm64\"), or None for cheapest.     allocation: Spot/on-demand strategy.     image: Environment specification.     ttl: Auto-shutdown timeout in seconds (0 = disabled).     provider: Override provider (usually inferred from context).</p> <p>Example:     &gt;&gt;&gt; spec = PoolSpec(     ...     nodes=4,     ...     accelerator=\"H100\",     ...     region=\"us-east-1\",     ...     allocation=\"spot-if-available\",     ...     image=Image(pip=[\"torch\"]),     ... )</p> <pre><code>&gt;&gt;&gt; from skyward.accelerators import H100\n&gt;&gt;&gt; spec = PoolSpec(\n...     nodes=4,\n...     accelerator=H100(count=8),\n...     region=\"us-east-1\",\n... )\n</code></pre>"},{"location":"reference/config/#skyward.PoolSpec.nodes","title":"<code>nodes</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.accelerator","title":"<code>accelerator</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.region","title":"<code>region</code>  <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.vcpus","title":"<code>vcpus = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.memory_gb","title":"<code>memory_gb = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.architecture","title":"<code>architecture = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.allocation","title":"<code>allocation = 'spot-if-available'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.image","title":"<code>image = field(default_factory=(lambda: Image()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.ttl","title":"<code>ttl = 600</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.worker","title":"<code>worker = field(default_factory=Worker)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.provider","title":"<code>provider = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.max_hourly_cost","title":"<code>max_hourly_cost = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.ssh_timeout","title":"<code>ssh_timeout = 300.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.ssh_retry_interval","title":"<code>ssh_retry_interval = 5.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.provision_retry_delay","title":"<code>provision_retry_delay = 10.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.max_provision_attempts","title":"<code>max_provision_attempts = 10</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.volumes","title":"<code>volumes = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.min_nodes","title":"<code>min_nodes = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.max_nodes","title":"<code>max_nodes = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.autoscale_cooldown","title":"<code>autoscale_cooldown = 30.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.autoscale_idle_timeout","title":"<code>autoscale_idle_timeout = 60.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.reconcile_tick_interval","title":"<code>reconcile_tick_interval = 15.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.plugins","title":"<code>plugins = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.accelerator_name","title":"<code>accelerator_name</code>  <code>property</code>","text":"<p>Get the canonical accelerator name for provider matching.</p>"},{"location":"reference/config/#skyward.PoolSpec.accelerator_count","title":"<code>accelerator_count</code>  <code>property</code>","text":"<p>Get the number of accelerators per node.</p>"},{"location":"reference/config/#skyward.PoolSpec.auto_scaling","title":"<code>auto_scaling</code>  <code>property</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.__post_init__","title":"<code>__post_init__()</code>","text":""},{"location":"reference/config/#skyward.PoolSpec.__init__","title":"<code>__init__(nodes, accelerator, region, vcpus=None, memory_gb=None, architecture=None, allocation='spot-if-available', image=(lambda: Image())(), ttl=600, worker=Worker(), provider=None, max_hourly_cost=None, ssh_timeout=300.0, ssh_retry_interval=5.0, provision_retry_delay=10.0, max_provision_attempts=10, volumes=(), min_nodes=None, max_nodes=None, autoscale_cooldown=30.0, autoscale_idle_timeout=60.0, reconcile_tick_interval=15.0, plugins=())</code>","text":""},{"location":"reference/config/#skyward.Image","title":"<code>skyward.Image</code>  <code>dataclass</code>","text":"<p>Declarative image specification.</p> <p>Defines the environment (Python version, packages, etc.) in a declarative way. The bootstrap() method generates idempotent shell scripts that work across all cloud providers.</p> <p>Args:     python: Python version to use.     pip: List of pip packages to install.     pip_indexes: Scoped package indexes. Each PipIndex maps specific         packages to a custom index URL via uv's explicit index support.     apt: List of apt packages to install.     env: Environment variables to export.     shell_vars: Shell commands for dynamic variable capture.     includes: Paths relative to CWD to sync to workers (dirs or .py files).     excludes: Glob patterns to ignore within includes (e.g., \"pycache\", \"*.pyc\").     skyward_source: Where to install skyward from. \"auto\" detects editable         installs as \"local\", otherwise \"pypi\".     metrics: Metrics to collect (CPU, GPU, Memory, etc.). Use None to disable.</p> <p>Example:     image = Image(         python=\"3.13\",         pip=[\"torch\", \"transformers\"],         apt=[\"git\", \"ffmpeg\"],         env={\"HF_TOKEN\": \"xxx\"},     )</p> <pre><code># Custom metrics\nfrom skyward.spec.metrics import CPU, GPU\nimage = Image(\n    pip=[\"torch\"],\n    metrics=[CPU(interval=0.5), GPU()],\n)\n\n# Disable metrics\nimage = Image(metrics=None)\n\n# Generate bootstrap script\nscript = image.bootstrap(ttl=3600)\n</code></pre>"},{"location":"reference/config/#skyward.Image.python","title":"<code>python = 'auto'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.pip","title":"<code>pip = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.pip_indexes","title":"<code>pip_indexes = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.apt","title":"<code>apt = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.env","title":"<code>env = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.shell_vars","title":"<code>shell_vars = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.includes","title":"<code>includes = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.excludes","title":"<code>excludes = ()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.skyward_source","title":"<code>skyward_source = 'auto'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.metrics","title":"<code>metrics = field(default_factory=(lambda: DefaultMetrics()))</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.bootstrap_timeout","title":"<code>bootstrap_timeout = 300</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.Image.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Convert lists to tuples for immutability.</p>"},{"location":"reference/config/#skyward.Image.content_hash","title":"<code>content_hash()</code>","text":"<p>Generate hash for AMI/snapshot caching.</p>"},{"location":"reference/config/#skyward.Image.generate_bootstrap","title":"<code>generate_bootstrap(ttl=0, shutdown_command='shutdown -h now', preamble=None, postamble=None)</code>","text":"<p>Generate bootstrap script for cloud-init/user_data.</p>"},{"location":"reference/config/#skyward.Image.__init__","title":"<code>__init__(python='auto', pip=(), pip_indexes=(), apt=(), env=dict(), shell_vars=dict(), includes=(), excludes=(), skyward_source='auto', metrics=(lambda: DefaultMetrics())(), bootstrap_timeout=300)</code>","text":""},{"location":"reference/config/#skyward.DEFAULT_IMAGE","title":"<code>skyward.DEFAULT_IMAGE = Image()</code>  <code>module-attribute</code>","text":""},{"location":"reference/config/#skyward.AllocationStrategy","title":"<code>skyward.AllocationStrategy = Literal['spot', 'on-demand', 'spot-if-available', 'cheapest']</code>","text":""},{"location":"reference/config/#skyward.api.spec.PoolState","title":"<code>skyward.api.spec.PoolState</code>","text":"<p>Pool lifecycle states.</p>"},{"location":"reference/config/#skyward.api.spec.PoolState.INIT","title":"<code>INIT = 'init'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.api.spec.PoolState.REQUESTING","title":"<code>REQUESTING = 'requesting'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.api.spec.PoolState.PROVISIONING","title":"<code>PROVISIONING = 'provisioning'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.api.spec.PoolState.READY","title":"<code>READY = 'ready'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.api.spec.PoolState.SHUTTING_DOWN","title":"<code>SHUTTING_DOWN = 'shutting_down'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/config/#skyward.api.spec.PoolState.DESTROYED","title":"<code>DESTROYED = 'destroyed'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/distributed/","title":"Distributed collections","text":""},{"location":"reference/distributed/#skyward.dict","title":"<code>skyward.dict(name, *, consistency=None)</code>","text":""},{"location":"reference/distributed/#skyward.set","title":"<code>skyward.set(name, *, consistency=None)</code>","text":""},{"location":"reference/distributed/#skyward.counter","title":"<code>skyward.counter(name, *, consistency=None)</code>","text":""},{"location":"reference/distributed/#skyward.queue","title":"<code>skyward.queue(name)</code>","text":""},{"location":"reference/distributed/#skyward.barrier","title":"<code>skyward.barrier(name, n)</code>","text":""},{"location":"reference/distributed/#skyward.lock","title":"<code>skyward.lock(name)</code>","text":""},{"location":"reference/events/","title":"Events","text":""},{"location":"reference/events/#skyward.Event","title":"<code>skyward.Event = Request | Fact</code>","text":""},{"location":"reference/events/#skyward.Request","title":"<code>skyward.Request = ClusterRequested | InstanceRequested | ShutdownRequested | BootstrapRequested</code>","text":""},{"location":"reference/events/#skyward.Fact","title":"<code>skyward.Fact = ClusterProvisioned | InstanceLaunched | InstanceRunning | InstanceProvisioned | InstanceBootstrapped | InstancePreempted | InstanceReplaced | InstanceDestroyed | ClusterReady | ClusterDestroyed | TaskStarted | TaskCompleted | Metric | Log | BootstrapConsole | BootstrapPhase | BootstrapCommand | BootstrapFailed | Error</code>","text":""},{"location":"reference/events/#type-aliases","title":"Type aliases","text":""},{"location":"reference/events/#skyward.RequestId","title":"<code>skyward.RequestId = str</code>","text":""},{"location":"reference/events/#skyward.ClusterId","title":"<code>skyward.ClusterId = str</code>","text":""},{"location":"reference/events/#skyward.InstanceId","title":"<code>skyward.InstanceId = str</code>","text":""},{"location":"reference/events/#skyward.NodeId","title":"<code>skyward.NodeId = int</code>","text":""},{"location":"reference/events/#skyward.ProviderName","title":"<code>skyward.ProviderName = Literal['aws', 'gcp', 'vastai', 'verda', 'runpod', 'container']</code>","text":""},{"location":"reference/events/#requests","title":"Requests","text":""},{"location":"reference/events/#skyward.ClusterRequested","title":"<code>skyward.ClusterRequested</code>  <code>dataclass</code>","text":"<p>Pool requests a new cluster from a provider.</p>"},{"location":"reference/events/#skyward.ClusterRequested.request_id","title":"<code>request_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ClusterRequested.provider","title":"<code>provider</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ClusterRequested.spec","title":"<code>spec</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ClusterRequested.reply_to","title":"<code>reply_to = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ClusterRequested.__init__","title":"<code>__init__(request_id, provider, spec, reply_to=None)</code>","text":""},{"location":"reference/events/#skyward.InstanceRequested","title":"<code>skyward.InstanceRequested</code>  <code>dataclass</code>","text":"<p>Node requests an instance (new or replacement).</p>"},{"location":"reference/events/#skyward.InstanceRequested.request_id","title":"<code>request_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceRequested.provider","title":"<code>provider</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceRequested.cluster_id","title":"<code>cluster_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceRequested.node_id","title":"<code>node_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceRequested.reply_to","title":"<code>reply_to = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceRequested.replacing","title":"<code>replacing = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceRequested.__init__","title":"<code>__init__(request_id, provider, cluster_id, node_id, reply_to=None, replacing=None)</code>","text":""},{"location":"reference/events/#skyward.ShutdownRequested","title":"<code>skyward.ShutdownRequested</code>  <code>dataclass</code>","text":"<p>Pool requests cluster shutdown.</p>"},{"location":"reference/events/#skyward.ShutdownRequested.cluster_id","title":"<code>cluster_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ShutdownRequested.reply_to","title":"<code>reply_to = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ShutdownRequested.__init__","title":"<code>__init__(cluster_id, reply_to=None)</code>","text":""},{"location":"reference/events/#facts","title":"Facts","text":""},{"location":"reference/events/#skyward.ClusterProvisioned","title":"<code>skyward.ClusterProvisioned</code>  <code>dataclass</code>","text":"<p>Cluster infrastructure is ready.</p>"},{"location":"reference/events/#skyward.ClusterProvisioned.request_id","title":"<code>request_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ClusterProvisioned.cluster_id","title":"<code>cluster_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ClusterProvisioned.provider","title":"<code>provider</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ClusterProvisioned.__init__","title":"<code>__init__(request_id, cluster_id, provider)</code>","text":""},{"location":"reference/events/#skyward.ClusterReady","title":"<code>skyward.ClusterReady</code>  <code>dataclass</code>","text":""},{"location":"reference/events/#skyward.ClusterReady.cluster","title":"<code>cluster</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ClusterReady.__init__","title":"<code>__init__(cluster)</code>","text":""},{"location":"reference/events/#skyward.ClusterDestroyed","title":"<code>skyward.ClusterDestroyed</code>  <code>dataclass</code>","text":"<p>Cluster was fully shut down.</p>"},{"location":"reference/events/#skyward.ClusterDestroyed.cluster_id","title":"<code>cluster_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.ClusterDestroyed.__init__","title":"<code>__init__(cluster_id)</code>","text":""},{"location":"reference/events/#skyward.InstanceProvisioned","title":"<code>skyward.InstanceProvisioned</code>  <code>dataclass</code>","text":"<p>Instance was created (not yet bootstrapped).</p>"},{"location":"reference/events/#skyward.InstanceProvisioned.request_id","title":"<code>request_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceProvisioned.instance","title":"<code>instance</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceProvisioned.__init__","title":"<code>__init__(request_id, instance)</code>","text":""},{"location":"reference/events/#skyward.InstanceBootstrapped","title":"<code>skyward.InstanceBootstrapped</code>  <code>dataclass</code>","text":"<p>Instance finished bootstrap, ready for work.</p>"},{"location":"reference/events/#skyward.InstanceBootstrapped.instance","title":"<code>instance</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceBootstrapped.__init__","title":"<code>__init__(instance)</code>","text":""},{"location":"reference/events/#skyward.InstancePreempted","title":"<code>skyward.InstancePreempted</code>  <code>dataclass</code>","text":"<p>Instance was preempted (spot interruption).</p>"},{"location":"reference/events/#skyward.InstancePreempted.instance","title":"<code>instance</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstancePreempted.reason","title":"<code>reason</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstancePreempted.__init__","title":"<code>__init__(instance, reason)</code>","text":""},{"location":"reference/events/#skyward.InstanceReplaced","title":"<code>skyward.InstanceReplaced</code>  <code>dataclass</code>","text":"<p>Instance was successfully replaced after preemption.</p>"},{"location":"reference/events/#skyward.InstanceReplaced.request_id","title":"<code>request_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceReplaced.old_id","title":"<code>old_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceReplaced.new","title":"<code>new</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceReplaced.__init__","title":"<code>__init__(request_id, old_id, new)</code>","text":""},{"location":"reference/events/#skyward.InstanceDestroyed","title":"<code>skyward.InstanceDestroyed</code>  <code>dataclass</code>","text":"<p>Instance was terminated.</p>"},{"location":"reference/events/#skyward.InstanceDestroyed.instance_id","title":"<code>instance_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.InstanceDestroyed.__init__","title":"<code>__init__(instance_id)</code>","text":""},{"location":"reference/events/#skyward.NodeInstance","title":"<code>skyward.NodeInstance</code>  <code>dataclass</code>","text":"<p>Instance bound to a node \u2014 infrastructure context + offer.</p>"},{"location":"reference/events/#skyward.NodeInstance.instance","title":"<code>instance</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.NodeInstance.node","title":"<code>node</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.NodeInstance.provider","title":"<code>provider</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.NodeInstance.ssh_user","title":"<code>ssh_user</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.NodeInstance.ssh_key_path","title":"<code>ssh_key_path</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.NodeInstance.network_interface","title":"<code>network_interface = ''</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.NodeInstance.__init__","title":"<code>__init__(instance, node, provider, ssh_user, ssh_key_path, network_interface='')</code>","text":""},{"location":"reference/events/#skyward.TaskStarted","title":"<code>skyward.TaskStarted</code>  <code>dataclass</code>","text":"<p>Task execution started on an instance.</p>"},{"location":"reference/events/#skyward.TaskStarted.task_id","title":"<code>task_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.TaskStarted.instance","title":"<code>instance</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.TaskStarted.function_name","title":"<code>function_name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.TaskStarted.__init__","title":"<code>__init__(task_id, instance, function_name)</code>","text":""},{"location":"reference/events/#skyward.TaskCompleted","title":"<code>skyward.TaskCompleted</code>  <code>dataclass</code>","text":"<p>Task execution completed.</p>"},{"location":"reference/events/#skyward.TaskCompleted.task_id","title":"<code>task_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.TaskCompleted.instance","title":"<code>instance</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.TaskCompleted.duration","title":"<code>duration</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.TaskCompleted.success","title":"<code>success</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.TaskCompleted.error","title":"<code>error = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.TaskCompleted.__init__","title":"<code>__init__(task_id, instance, duration, success, error=None)</code>","text":""},{"location":"reference/events/#skyward.Metric","title":"<code>skyward.Metric</code>  <code>dataclass</code>","text":"<p>Metric value from an instance.</p>"},{"location":"reference/events/#skyward.Metric.instance","title":"<code>instance</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.Metric.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.Metric.value","title":"<code>value</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.Metric.timestamp","title":"<code>timestamp</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.Metric.__init__","title":"<code>__init__(instance, name, value, timestamp)</code>","text":""},{"location":"reference/events/#skyward.Log","title":"<code>skyward.Log</code>  <code>dataclass</code>","text":"<p>Log line from an instance.</p>"},{"location":"reference/events/#skyward.Log.instance","title":"<code>instance</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.Log.line","title":"<code>line</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.Log.stream","title":"<code>stream = 'stdout'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.Log.__init__","title":"<code>__init__(instance, line, stream='stdout')</code>","text":""},{"location":"reference/events/#skyward.Error","title":"<code>skyward.Error</code>  <code>dataclass</code>","text":"<p>Something went wrong.</p>"},{"location":"reference/events/#skyward.Error.request_id","title":"<code>request_id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.Error.message","title":"<code>message</code>  <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.Error.fatal","title":"<code>fatal = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/events/#skyward.Error.__init__","title":"<code>__init__(request_id, message, fatal=False)</code>","text":""},{"location":"reference/plugins/","title":"Plugins","text":""},{"location":"reference/plugins/#skyward.plugins","title":"<code>skyward.plugins</code>","text":"<p>Skyward plugins for third-party integrations.</p> <p>All imports are lazy to avoid requiring optional dependencies at import time.</p> <p>Usage:     import skyward as sky</p> <pre><code>with sky.ComputePool(\n    plugins=[sky.plugins.torch(backend=\"nccl\")],\n) as pool:\n    ...\n</code></pre>"},{"location":"reference/plugins/#skyward.plugins.__all__","title":"<code>__all__ = ['Plugin', 'torch', 'jax', 'keras', 'cuml', 'huggingface', 'joblib', 'mig', 'mps', 'sklearn']</code>  <code>module-attribute</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin","title":"<code>Plugin</code>  <code>dataclass</code>","text":"<p>Declarative third-party integration bundle.</p> <p>Bundles environment setup, bootstrap ops, worker lifecycle hooks, client-side hooks, and per-task wrapping into a single composable unit.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Plugin identifier.</p> required <code>transform</code> <code>ImageTransform[Any] | None</code> <p>(Image, Cluster[S]) -&gt; Image transformer. Receives current Image and cluster metadata, returns modified copy.</p> <code>None</code> <code>bootstrap</code> <code>BootstrapFactory[Any] | None</code> <p>Factory that receives Cluster[S] and returns extra shell ops appended after Image-driven bootstrap phases.</p> <code>None</code> <code>decorate</code> <code>TaskDecorator | None</code> <p>Classic Python decorator: (fn) -&gt; fn. Wraps each @sky.compute function at execution time on the remote worker.</p> <code>None</code> <code>around_app</code> <code>AppLifecycle | None</code> <p>Worker lifecycle context manager: InstanceInfo -&gt; ContextManager[None]. Entered once in the main worker process.</p> <code>None</code> <code>around_process</code> <code>ProcessLifecycle | None</code> <p>Subprocess lifecycle context manager: InstanceInfo -&gt; ContextManager[None]. Entered once per subprocess when executor=\"process\". Lazy \u2014 enters on the first task execution in each subprocess, after env vars are propagated.</p> <code>None</code> <code>around_client</code> <code>ClientLifecycle[Any] | None</code> <p>Client lifecycle context manager: (ComputePool, Cluster[S]) -&gt; ContextManager[None].</p> <code>None</code>"},{"location":"reference/plugins/#skyward.plugins.Plugin.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.transform","title":"<code>transform = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.bootstrap","title":"<code>bootstrap = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.decorate","title":"<code>decorate = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.around_app","title":"<code>around_app = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.around_process","title":"<code>around_process = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.around_client","title":"<code>around_client = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.create","title":"<code>create(name)</code>  <code>staticmethod</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.with_image_transform","title":"<code>with_image_transform(transform)</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.with_bootstrap","title":"<code>with_bootstrap(factory)</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.with_decorator","title":"<code>with_decorator(decorate)</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.with_around_app","title":"<code>with_around_app(around)</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.with_around_process","title":"<code>with_around_process(around)</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.with_around_client","title":"<code>with_around_client(around)</code>","text":""},{"location":"reference/plugins/#skyward.plugins.Plugin.__init__","title":"<code>__init__(name, transform=None, bootstrap=None, decorate=None, around_app=None, around_process=None, around_client=None)</code>","text":""},{"location":"reference/plugins/#skyward.plugins.__getattr__","title":"<code>__getattr__(name)</code>","text":""},{"location":"reference/pool/","title":"Pool &amp; compute","text":""},{"location":"reference/pool/#skyward.ComputePool","title":"<code>skyward.ComputePool</code>","text":"<p>Provision cloud compute and execute functions remotely.</p> <p>Two usage modes:</p> <pre><code># Single provider (legacy)\nwith ComputePool(provider=AWS(), accelerator=\"A100\", nodes=4) as pool:\n    result = train(data) &gt;&gt; pool\n\n# Multi-spec with fallback\nwith ComputePool(\n    Spec(provider=VastAI(), accelerator=\"A100\"),\n    Spec(provider=AWS(), accelerator=\"A100\"),\n    selection=\"cheapest\",\n) as pool:\n    result = train(data) &gt;&gt; pool\n</code></pre>"},{"location":"reference/pool/#skyward.ComputePool.selection","title":"<code>selection = selection</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.image","title":"<code>image = image</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.worker","title":"<code>worker = worker or Worker()</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.logging","title":"<code>logging = logging</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.default_compute_timeout","title":"<code>default_compute_timeout = default_compute_timeout</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.provision_timeout","title":"<code>provision_timeout = provision_timeout</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.ssh_timeout","title":"<code>ssh_timeout = ssh_timeout</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.ssh_retry_interval","title":"<code>ssh_retry_interval = ssh_retry_interval</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.provision_retry_delay","title":"<code>provision_retry_delay = provision_retry_delay</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.max_provision_attempts","title":"<code>max_provision_attempts = max_provision_attempts</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.volumes","title":"<code>volumes = tuple(volumes)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.autoscale_cooldown","title":"<code>autoscale_cooldown = autoscale_cooldown</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.autoscale_idle_timeout","title":"<code>autoscale_idle_timeout = autoscale_idle_timeout</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.reconcile_tick_interval","title":"<code>reconcile_tick_interval = reconcile_tick_interval</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.concurrency","title":"<code>concurrency</code>  <code>property</code>","text":"<p>Number of concurrent task slots per node.</p>"},{"location":"reference/pool/#skyward.ComputePool.is_active","title":"<code>is_active</code>  <code>property</code>","text":"<p>True if pool is ready for execution.</p>"},{"location":"reference/pool/#skyward.ComputePool.__init__","title":"<code>__init__(*specs, provider=None, nodes=1, accelerator=None, vcpus=None, memory_gb=None, architecture=None, allocation='spot-if-available', selection='cheapest', image=DEFAULT_IMAGE, ttl=600, worker=None, logging=True, max_hourly_cost=None, default_compute_timeout=300.0, provision_timeout=300, ssh_timeout=300, ssh_retry_interval=2, provision_retry_delay=10.0, max_provision_attempts=10, volumes=(), autoscale_cooldown=30.0, autoscale_idle_timeout=60.0, reconcile_tick_interval=15.0, plugins=())</code>","text":"<pre><code>__init__(\n    *,\n    provider: ProviderConfig,\n    nodes: int | tuple[int, int] = ...,\n    accelerator: str | Accelerator | None = ...,\n    vcpus: float | None = ...,\n    memory_gb: float | None = ...,\n    architecture: Literal[\"x86_64\", \"arm64\"] | None = ...,\n    allocation: Literal[\n        \"spot\", \"on-demand\", \"spot-if-available\"\n    ] = ...,\n    image: Image = ...,\n    ttl: int = ...,\n    worker: Worker | None = ...,\n    logging: LogConfig | bool = ...,\n    max_hourly_cost: float | None = ...,\n    default_compute_timeout: float = ...,\n    provision_timeout: int = ...,\n    ssh_timeout: int = ...,\n    ssh_retry_interval: int = ...,\n    provision_retry_delay: float = ...,\n    max_provision_attempts: int = ...,\n    volumes: list[Volume] | tuple[Volume, ...] = ...,\n    autoscale_cooldown: float = ...,\n    autoscale_idle_timeout: float = ...,\n    reconcile_tick_interval: float = ...,\n    plugins: list[Plugin] | tuple[Plugin, ...] = ...,\n) -&gt; None\n</code></pre><pre><code>__init__(\n    *specs: Spec,\n    selection: SelectionStrategy = ...,\n    image: Image = ...,\n    worker: Worker | None = ...,\n    logging: LogConfig | bool = ...,\n    default_compute_timeout: float = ...,\n    provision_timeout: int = ...,\n    ssh_timeout: int = ...,\n    ssh_retry_interval: int = ...,\n    provision_retry_delay: float = ...,\n    max_provision_attempts: int = ...,\n    volumes: list[Volume] | tuple[Volume, ...] = ...,\n    autoscale_cooldown: float = ...,\n    autoscale_idle_timeout: float = ...,\n    reconcile_tick_interval: float = ...,\n    plugins: list[Plugin] | tuple[Plugin, ...] = ...,\n) -&gt; None\n</code></pre>"},{"location":"reference/pool/#skyward.ComputePool.__enter__","title":"<code>__enter__()</code>","text":"<p>Start pool and provision resources.</p>"},{"location":"reference/pool/#skyward.ComputePool.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Stop pool and release resources.</p>"},{"location":"reference/pool/#skyward.ComputePool.run","title":"<code>run(pending)</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.run_async","title":"<code>run_async(pending)</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.broadcast","title":"<code>broadcast(pending)</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.run_parallel","title":"<code>run_parallel(group)</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.map","title":"<code>map(fn, items)</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.dict","title":"<code>dict(name, *, consistency=None)</code>","text":"<p>Get or create a distributed dict.</p>"},{"location":"reference/pool/#skyward.ComputePool.set","title":"<code>set(name, *, consistency=None)</code>","text":"<p>Get or create a distributed set.</p>"},{"location":"reference/pool/#skyward.ComputePool.counter","title":"<code>counter(name, *, consistency=None)</code>","text":"<p>Get or create a distributed counter.</p>"},{"location":"reference/pool/#skyward.ComputePool.queue","title":"<code>queue(name)</code>","text":"<p>Get or create a distributed queue.</p>"},{"location":"reference/pool/#skyward.ComputePool.barrier","title":"<code>barrier(name, n)</code>","text":"<p>Get or create a distributed barrier.</p>"},{"location":"reference/pool/#skyward.ComputePool.lock","title":"<code>lock(name)</code>","text":"<p>Get or create a distributed lock.</p>"},{"location":"reference/pool/#skyward.ComputePool.current_nodes","title":"<code>current_nodes()</code>","text":"<p>Return the number of ready nodes in the pool.</p>"},{"location":"reference/pool/#skyward.ComputePool.__repr__","title":"<code>__repr__()</code>","text":""},{"location":"reference/pool/#skyward.ComputePool.Named","title":"<code>Named(name)</code>  <code>classmethod</code>","text":""},{"location":"reference/pool/#skyward.Spec","title":"<code>skyward.Spec</code>  <code>dataclass</code>","text":"<p>User-facing hardware preference for ComputePool fallback chains.</p>"},{"location":"reference/pool/#skyward.Spec.provider","title":"<code>provider</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Spec.accelerator","title":"<code>accelerator = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Spec.nodes","title":"<code>nodes = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Spec.vcpus","title":"<code>vcpus = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Spec.memory_gb","title":"<code>memory_gb = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Spec.architecture","title":"<code>architecture = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Spec.allocation","title":"<code>allocation = 'spot-if-available'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Spec.region","title":"<code>region = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Spec.max_hourly_cost","title":"<code>max_hourly_cost = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Spec.ttl","title":"<code>ttl = 600</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Spec.__init__","title":"<code>__init__(provider, accelerator=None, nodes=1, vcpus=None, memory_gb=None, architecture=None, allocation='spot-if-available', region=None, max_hourly_cost=None, ttl=600)</code>","text":""},{"location":"reference/pool/#skyward.SelectionStrategy","title":"<code>skyward.SelectionStrategy = Literal['first', 'cheapest']</code>","text":""},{"location":"reference/pool/#skyward.compute","title":"<code>skyward.compute(fn=None, *, timeout=None)</code>","text":"<pre><code>compute(\n    fn: Callable[P, T],\n) -&gt; Callable[P, PendingCompute[T]]\n</code></pre><pre><code>compute(\n    *, timeout: float\n) -&gt; Callable[\n    [Callable[P, T]], Callable[P, PendingCompute[T]]\n]\n</code></pre>"},{"location":"reference/pool/#skyward.sky","title":"<code>skyward.sky = _Sky()</code>  <code>module-attribute</code>","text":""},{"location":"reference/pool/#skyward.gather","title":"<code>skyward.gather(*pendings, stream=False, ordered=True)</code>","text":"<p>Group computations for parallel execution.</p> <p>Example:     results = gather(task1(), task2(), task3()) &gt;&gt; sky     # results is a tuple of (result1, result2, result3)</p> <pre><code>for result in gather(task1(), task2(), task3(), stream=True) &gt;&gt; sky:\n    print(result)  # yields results as they complete\n\nfor result in gather(task1(), task2(), task3(), stream=True, ordered=False) &gt;&gt; sky:\n    print(result)  # yields results as they complete, unordered\n</code></pre>"},{"location":"reference/pool/#skyward.PendingCompute","title":"<code>skyward.PendingCompute</code>  <code>dataclass</code>","text":"<p>Lazy computation wrapper.</p> <p>Represents a function call that will be executed remotely when sent to a pool via the &gt;&gt; or @ operator.</p> <p>Example:     @compute     def train(data):         return model.fit(data)</p> <pre><code>pending = train(data)  # Returns PendingCompute, doesn't execute\nresult = pending &gt;&gt; sky  # Executes remotely on pool\nresults = pending @ sky  # Broadcasts to all nodes\n\n# Override timeout\nresult = train(data).with_timeout(600) &gt;&gt; sky\n</code></pre>"},{"location":"reference/pool/#skyward.PendingCompute.fn","title":"<code>fn</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.PendingCompute.args","title":"<code>args</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.PendingCompute.kwargs","title":"<code>kwargs</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.PendingCompute.timeout","title":"<code>timeout = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.PendingCompute.with_timeout","title":"<code>with_timeout(timeout)</code>","text":""},{"location":"reference/pool/#skyward.PendingCompute.__rshift__","title":"<code>__rshift__(target)</code>","text":""},{"location":"reference/pool/#skyward.PendingCompute.__gt__","title":"<code>__gt__(target)</code>","text":""},{"location":"reference/pool/#skyward.PendingCompute.__matmul__","title":"<code>__matmul__(target)</code>","text":"<p>Broadcast to all nodes using @ operator.</p>"},{"location":"reference/pool/#skyward.PendingCompute.__and__","title":"<code>__and__(other)</code>","text":"<p>Combine with another computation for parallel execution.</p>"},{"location":"reference/pool/#skyward.PendingCompute.__init__","title":"<code>__init__(fn, args, kwargs, timeout=None)</code>","text":""},{"location":"reference/pool/#skyward.PendingComputeGroup","title":"<code>skyward.PendingComputeGroup</code>  <code>dataclass</code>","text":"<p>Group of computations for parallel execution.</p> <p>Created by using the &amp; operator:     group = task1() &amp; task2() &amp; task3()     a, b, c = group &gt;&gt; sky</p> <p>Or using gather():     group = gather(task1(), task2(), task3())     results = group &gt;&gt; sky</p>"},{"location":"reference/pool/#skyward.PendingComputeGroup.items","title":"<code>items</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.PendingComputeGroup.stream","title":"<code>stream = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.PendingComputeGroup.ordered","title":"<code>ordered = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.PendingComputeGroup.timeout","title":"<code>timeout = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.PendingComputeGroup.with_timeout","title":"<code>with_timeout(timeout)</code>","text":""},{"location":"reference/pool/#skyward.PendingComputeGroup.__and__","title":"<code>__and__(other)</code>","text":"<p>Add another computation to the group.</p>"},{"location":"reference/pool/#skyward.PendingComputeGroup.__rshift__","title":"<code>__rshift__(target)</code>","text":"<p>Execute all computations in parallel using &gt;&gt; operator.</p>"},{"location":"reference/pool/#skyward.PendingComputeGroup.__len__","title":"<code>__len__()</code>","text":""},{"location":"reference/pool/#skyward.PendingComputeGroup.__iter__","title":"<code>__iter__()</code>","text":""},{"location":"reference/pool/#skyward.PendingComputeGroup.__init__","title":"<code>__init__(items, stream=False, ordered=True, timeout=None)</code>","text":""},{"location":"reference/pool/#skyward.Offer","title":"<code>skyward.Offer</code>  <code>dataclass</code>","text":"<p>Ephemeral availability + pricing for a specific instance type.</p>"},{"location":"reference/pool/#skyward.Offer.id","title":"<code>id</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Offer.instance_type","title":"<code>instance_type</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Offer.spot_price","title":"<code>spot_price</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Offer.on_demand_price","title":"<code>on_demand_price</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Offer.billing_unit","title":"<code>billing_unit</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Offer.specific","title":"<code>specific</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.Offer.__init__","title":"<code>__init__(id, instance_type, spot_price, on_demand_price, billing_unit, specific)</code>","text":""},{"location":"reference/pool/#skyward.InstanceType","title":"<code>skyward.InstanceType</code>  <code>dataclass</code>","text":"<p>Normalized machine type \u2014 cacheable, provider-agnostic hardware description.</p>"},{"location":"reference/pool/#skyward.InstanceType.name","title":"<code>name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.InstanceType.accelerator","title":"<code>accelerator</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.InstanceType.vcpus","title":"<code>vcpus</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.InstanceType.memory_gb","title":"<code>memory_gb</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.InstanceType.architecture","title":"<code>architecture</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.InstanceType.specific","title":"<code>specific</code>  <code>instance-attribute</code>","text":""},{"location":"reference/pool/#skyward.InstanceType.__init__","title":"<code>__init__(name, accelerator, vcpus, memory_gb, architecture, specific)</code>","text":""},{"location":"reference/runtime/","title":"Runtime","text":""},{"location":"reference/runtime/#skyward.InstanceInfo","title":"<code>skyward.InstanceInfo</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Information about the current compute pool, parsed from COMPUTE_POOL env var.</p> <p>This class provides a typed interface to access pool information inside a distributed function.</p> <p>Example:     from skyward import compute_pool</p> <pre><code>pool = compute_pool()\nif pool and pool.is_head:\n    print(f\"I am head node of {pool.total_nodes}\")\n</code></pre>"},{"location":"reference/runtime/#skyward.InstanceInfo.node","title":"<code>node = Field(description='Index of this node (0 to total_nodes - 1)')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.worker","title":"<code>worker = Field(default=0, description='Worker index within this node (0 to workers_per_node - 1)')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.total_nodes","title":"<code>total_nodes = Field(description='Total number of nodes in the pool')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.workers_per_node","title":"<code>workers_per_node = Field(default=1, description='Number of workers per node (e.g., 2 for MIG 3g.40gb)')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.accelerators","title":"<code>accelerators = Field(description='Number of accelerators on this node')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.total_accelerators","title":"<code>total_accelerators = Field(description='Total accelerators in the pool')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.head_addr","title":"<code>head_addr = Field(description='IP address of the head node')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.head_port","title":"<code>head_port = Field(description='Port for head node coordination')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.job_id","title":"<code>job_id = Field(description='Unique identifier for this pool execution')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.peers","title":"<code>peers = Field(description='Information about all peers')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.accelerator","title":"<code>accelerator = Field(default=None, description='Accelerator configuration')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.network","title":"<code>network = Field(description='Network configuration')</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/runtime/#skyward.InstanceInfo.global_worker_index","title":"<code>global_worker_index</code>  <code>property</code>","text":"<p>Global index of this worker (0 to total_workers - 1).</p>"},{"location":"reference/runtime/#skyward.InstanceInfo.total_workers","title":"<code>total_workers</code>  <code>property</code>","text":"<p>Total number of workers across all nodes.</p>"},{"location":"reference/runtime/#skyward.InstanceInfo.is_head","title":"<code>is_head</code>  <code>property</code>","text":"<p>True if this is the head worker (global_worker_index == 0).</p>"},{"location":"reference/runtime/#skyward.InstanceInfo.hostname","title":"<code>hostname</code>  <code>property</code>","text":"<p>Current instance hostname.</p>"},{"location":"reference/runtime/#skyward.InstanceInfo.current","title":"<code>current()</code>  <code>classmethod</code>","text":"<p>Get pool info from COMPUTE_POOL environment variable.</p>"},{"location":"reference/runtime/#skyward.instance_info","title":"<code>skyward.instance_info()</code>","text":"<p>Get information about the current instance.</p> <p>Must be called from within a @compute function running on a remote node.</p> <p>Returns:     InstanceInfo with cluster topology, or None if not in a pool.</p> <p>Example:     @compute     def distributed_task(data):         info = sky.instance_info()         if info.is_head:             print(f\"Head node of {info.total_nodes} nodes\")         return process(data)</p>"},{"location":"reference/runtime/#skyward.shard","title":"<code>skyward.shard(*data, shuffle=False, seed=0, drop_last=False, node=None, total_nodes=None)</code>","text":"<pre><code>shard(\n    data: list[T],\n    /,\n    *,\n    shuffle: bool = False,\n    seed: int = 0,\n    drop_last: bool = False,\n    node: int | None = None,\n    total_nodes: int | None = None,\n) -&gt; list[T]\n</code></pre><pre><code>shard(\n    data: tuple[T, ...],\n    /,\n    *,\n    shuffle: bool = False,\n    seed: int = 0,\n    drop_last: bool = False,\n    node: int | None = None,\n    total_nodes: int | None = None,\n) -&gt; tuple[T, ...]\n</code></pre><pre><code>shard(\n    data: npt.NDArray[Any],\n    /,\n    *,\n    shuffle: bool = False,\n    seed: int = 0,\n    drop_last: bool = False,\n    node: int | None = None,\n    total_nodes: int | None = None,\n) -&gt; npt.NDArray[Any]\n</code></pre><pre><code>shard(\n    data: torch.Tensor,\n    /,\n    *,\n    shuffle: bool = False,\n    seed: int = 0,\n    drop_last: bool = False,\n    node: int | None = None,\n    total_nodes: int | None = None,\n) -&gt; torch.Tensor\n</code></pre><pre><code>shard(\n    data1: T1,\n    data2: T2,\n    /,\n    *,\n    shuffle: bool = False,\n    seed: int = 0,\n    drop_last: bool = False,\n    node: int | None = None,\n    total_nodes: int | None = None,\n) -&gt; tuple[T1, T2]\n</code></pre><pre><code>shard(\n    data1: T1,\n    data2: T2,\n    data3: T3,\n    /,\n    *,\n    shuffle: bool = False,\n    seed: int = 0,\n    drop_last: bool = False,\n    node: int | None = None,\n    total_nodes: int | None = None,\n) -&gt; tuple[T1, T2, T3]\n</code></pre><pre><code>shard(\n    data1: T1,\n    data2: T2,\n    data3: T3,\n    data4: T4,\n    /,\n    *,\n    shuffle: bool = False,\n    seed: int = 0,\n    drop_last: bool = False,\n    node: int | None = None,\n    total_nodes: int | None = None,\n) -&gt; tuple[T1, T2, T3, T4]\n</code></pre><pre><code>shard(\n    data1: T1,\n    data2: T2,\n    data3: T3,\n    data4: T4,\n    data5: T5,\n    /,\n    *,\n    shuffle: bool = False,\n    seed: int = 0,\n    drop_last: bool = False,\n    node: int | None = None,\n    total_nodes: int | None = None,\n) -&gt; tuple[T1, T2, T3, T4, T5]\n</code></pre> <p>Shard data across distributed nodes, preserving input type.</p> <p>Returns ONLY this node's portion of the data. Supports: list, tuple, np.ndarray, torch.Tensor, and any Sequence.</p> <p>Can accept multiple arrays at once - they will all be sharded with the same indices (useful for keeping x and y aligned).</p> <p>Args:     *data: One or more arrays/sequences to shard.     shuffle: Shuffle with synchronized seed across all nodes.     seed: Random seed for reproducible shuffling.     drop_last: Drop tail items so all nodes get equal count.     node: Override node index (for testing).     total_nodes: Override total_nodes (for testing).</p> <p>Returns:     If single argument: This node's shard with same type as input.     If multiple arguments: Tuple of shards.</p> <p>Example:     # Single array     my_data = shard(full_dataset, shuffle=True, seed=42)</p> <pre><code># Multiple arrays (keeps alignment)\nx_train, y_train = shard(x_train, y_train)\n\n# Four arrays at once\nx_train, y_train, x_test, y_test = shard(x_train, y_train, x_test, y_test)\n</code></pre>"},{"location":"reference/runtime/#skyward.stdout","title":"<code>skyward.stdout(only)</code>","text":"<p>Control stdout emission in distributed execution.</p> <p>Silences stdout for workers that don't match the predicate. stderr is NOT affected - errors from any worker are always visible.</p> <p>Args:     only: Predicate or \"head\" shortcut. Workers matching this emit stdout.         - \"head\": Only head worker (node == 0)         - Callable[[InstanceInfo], bool]: Custom predicate</p> <p>Returns:     Decorator that wraps the function with stdout control.</p>"},{"location":"reference/runtime/#skyward.stderr","title":"<code>skyward.stderr(only)</code>","text":"<p>Control stderr emission in distributed execution.</p> <p>Silences stderr for workers that don't match the predicate. Use with caution - silencing errors can hide problems.</p> <p>Args:     only: Predicate or \"head\" shortcut. Workers matching this emit stderr.</p>"},{"location":"reference/runtime/#skyward.silent","title":"<code>skyward.silent(fn)</code>","text":"<p>Silence both stdout and stderr completely.</p> <p>Useful for functions that should never emit output regardless of rank.</p>"},{"location":"reference/runtime/#skyward.is_head","title":"<code>skyward.is_head(info)</code>","text":"<p>True if this is the head worker (node == 0).</p>"},{"location":"reference/runtime/#skyward.CallbackWriter","title":"<code>skyward.CallbackWriter</code>","text":"<p>               Bases: <code>TextIO</code></p>"},{"location":"reference/runtime/#skyward.CallbackWriter.__init__","title":"<code>__init__(callback)</code>","text":""},{"location":"reference/runtime/#skyward.CallbackWriter.write","title":"<code>write(s)</code>","text":""},{"location":"reference/runtime/#skyward.CallbackWriter.getvalue","title":"<code>getvalue()</code>","text":""},{"location":"reference/runtime/#skyward.CallbackWriter.read","title":"<code>read(n=-1)</code>","text":""},{"location":"reference/runtime/#skyward.CallbackWriter.readline","title":"<code>readline(limit=-1)</code>","text":""},{"location":"reference/runtime/#skyward.CallbackWriter.flush","title":"<code>flush()</code>","text":""},{"location":"reference/runtime/#skyward.CallbackWriter.close","title":"<code>close()</code>","text":""},{"location":"reference/runtime/#skyward.CallbackWriter.seekable","title":"<code>seekable()</code>","text":""},{"location":"reference/runtime/#skyward.CallbackWriter.readable","title":"<code>readable()</code>","text":""},{"location":"reference/runtime/#skyward.CallbackWriter.writable","title":"<code>writable()</code>","text":""},{"location":"reference/runtime/#skyward.redirect_output","title":"<code>skyward.redirect_output(callback)</code>","text":""},{"location":"reference/providers/aws/","title":"AWS","text":""},{"location":"reference/providers/aws/#skyward.AWS","title":"<code>skyward.AWS</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ProviderConfig</code></p> <p>AWS provider configuration.</p> <p>Immutable configuration that defines how to connect to AWS and provision resources. All fields have sensible defaults.</p> <p>Example:     &gt;&gt;&gt; from skyward.providers.aws import AWS     &gt;&gt;&gt; config = AWS(region=\"us-west-2\")</p> <p>Args:     region: AWS region for resources. Default: us-east-1     ami: Custom AMI ID. If None, resolves via SSM Parameter Store.     ubuntu_version: Ubuntu LTS version for auto-resolved AMIs.     subnet_id: Specific subnet. If None, uses default VPC subnets.     security_group_id: Specific SG. If None, creates one.     instance_profile_arn: IAM instance profile ARN. <code>\"auto\"</code> creates         a scoped role with S3 permissions for the configured volumes         (cleaned up on teardown). <code>None</code> means no instance profile.     username: SSH username. Auto-detected from AMI if None.     instance_timeout: Safety timeout in seconds. Default: 300.     allocation_strategy: EC2 Fleet allocation strategy.     exclude_burstable: Exclude burstable instances (t3, t4g, etc.).</p>"},{"location":"reference/providers/aws/#skyward.AWS.region","title":"<code>region = 'us-east-1'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.ami","title":"<code>ami = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.ubuntu_version","title":"<code>ubuntu_version = '24.04'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.subnet_id","title":"<code>subnet_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.security_group_id","title":"<code>security_group_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.instance_profile_arn","title":"<code>instance_profile_arn = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.username","title":"<code>username = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.instance_timeout","title":"<code>instance_timeout = 300</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.request_timeout","title":"<code>request_timeout = 30</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.allocation_strategy","title":"<code>allocation_strategy = 'price-capacity-optimized'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.exclude_burstable","title":"<code>exclude_burstable = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.type","title":"<code>type</code>  <code>property</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.create_provider","title":"<code>create_provider()</code>  <code>async</code>","text":""},{"location":"reference/providers/aws/#skyward.AWS.__init__","title":"<code>__init__(region='us-east-1', ami=None, ubuntu_version='24.04', subnet_id=None, security_group_id=None, instance_profile_arn=None, username=None, instance_timeout=300, request_timeout=30, allocation_strategy='price-capacity-optimized', exclude_burstable=False)</code>","text":""},{"location":"reference/providers/gcp/","title":"GCP","text":""},{"location":"reference/providers/gcp/#skyward.GCP","title":"<code>skyward.GCP</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ProviderConfig</code></p> <p>GCP Compute Engine provider configuration.</p> <p>Immutable configuration that defines how to connect to GCP and provision Compute Engine resources. All fields have sensible defaults.</p> <p>SSH keys are injected into instance metadata during provisioning. The project is auto-detected from Application Default Credentials or the GOOGLE_CLOUD_PROJECT environment variable if not specified.</p> <p>Example:     &gt;&gt;&gt; from skyward.providers.gcp import GCP     &gt;&gt;&gt; config = GCP(zone=\"us-central1-a\")</p> <p>Args:     project: GCP project ID. Auto-detected from ADC or GOOGLE_CLOUD_PROJECT.     zone: Compute Engine zone for resources. Default: us-central1-a.     network: VPC network name. Default: default.     subnet: Specific subnet. If None, uses auto-mode subnet for the region.     disk_size_gb: Boot disk size in GB. Default: 200.     disk_type: Boot disk type. Default: pd-balanced.     instance_timeout: Safety timeout in seconds. Default: 300.     service_account: GCE service account email. If None, uses default.</p>"},{"location":"reference/providers/gcp/#skyward.GCP.project","title":"<code>project = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.zone","title":"<code>zone = 'us-central1-a'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.network","title":"<code>network = 'default'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.subnet","title":"<code>subnet = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.disk_size_gb","title":"<code>disk_size_gb = 200</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.disk_type","title":"<code>disk_type = 'pd-balanced'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.instance_timeout","title":"<code>instance_timeout = 300</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.service_account","title":"<code>service_account = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.thread_pool_size","title":"<code>thread_pool_size = 8</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.type","title":"<code>type</code>  <code>property</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.create_provider","title":"<code>create_provider()</code>  <code>async</code>","text":""},{"location":"reference/providers/gcp/#skyward.GCP.__init__","title":"<code>__init__(project=None, zone='us-central1-a', network='default', subnet=None, disk_size_gb=200, disk_type='pd-balanced', instance_timeout=300, service_account=None, thread_pool_size=8)</code>","text":""},{"location":"reference/providers/runpod/","title":"RunPod","text":""},{"location":"reference/providers/runpod/#skyward.RunPod","title":"<code>skyward.RunPod</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ProviderConfig</code></p> <p>RunPod GPU Pods provider configuration.</p> <p>SSH keys are automatically detected from ~/.ssh/id_ed25519.pub or ~/.ssh/id_rsa.pub.</p> <p>Features:     - Auto data center: If not specified, RunPod selects best location.</p> <p>Example:     &gt;&gt;&gt; from skyward.providers.runpod import RunPod     &gt;&gt;&gt; config = RunPod(data_center_ids=(\"EU-RO-1\",))</p> <p>Args:     api_key: RunPod API key. Falls back to RUNPOD_API_KEY env var.     cloud_type: Cloud type (SECURE or COMMUNITY). Default: SECURE.     container_disk_gb: Container disk size in GB. Default: 50.     volume_gb: Persistent volume size in GB. Default: 20.     volume_mount_path: Volume mount path. Default: /workspace.     data_center_ids: Preferred data center IDs or \"global\" for auto-selection.     ports: Port mappings (e.g., [\"22/tcp\", \"8888/http\"]). Default: [\"22/tcp\"].     provision_timeout: Instance provision timeout in seconds. Default: 300.     bootstrap_timeout: Bootstrap timeout in seconds. Default: 600.     instance_timeout: Auto-shutdown in seconds (safety timeout). Default: 300.     registry_auth: Name of the container registry credential registered in RunPod         account settings. Authenticates Docker Hub pulls to avoid rate limits.         Set to None to skip. Default: \"docker hub\".</p>"},{"location":"reference/providers/runpod/#skyward.RunPod.cluster_mode","title":"<code>cluster_mode = 'individual'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.global_networking","title":"<code>global_networking = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.api_key","title":"<code>api_key = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.cloud_type","title":"<code>cloud_type = CloudType.SECURE</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.ubuntu","title":"<code>ubuntu = 'newest'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.container_disk_gb","title":"<code>container_disk_gb = 50</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.volume_gb","title":"<code>volume_gb = 20</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.volume_mount_path","title":"<code>volume_mount_path = '/workspace'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.data_center_ids","title":"<code>data_center_ids = 'global'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.ports","title":"<code>ports = ('22/tcp',)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.provision_timeout","title":"<code>provision_timeout = 300.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.bootstrap_timeout","title":"<code>bootstrap_timeout = 600.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.instance_timeout","title":"<code>instance_timeout = 300</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.request_timeout","title":"<code>request_timeout = 30</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.cpu_clock","title":"<code>cpu_clock = '3c'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.bid_multiplier","title":"<code>bid_multiplier = 1</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.registry_auth","title":"<code>registry_auth = 'docker hub'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.type","title":"<code>type</code>  <code>property</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.region","title":"<code>region</code>  <code>property</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.create_provider","title":"<code>create_provider()</code>  <code>async</code>","text":""},{"location":"reference/providers/runpod/#skyward.RunPod.__init__","title":"<code>__init__(cluster_mode='individual', global_networking=None, api_key=None, cloud_type=CloudType.SECURE, ubuntu='newest', container_disk_gb=50, volume_gb=20, volume_mount_path='/workspace', data_center_ids='global', ports=('22/tcp',), provision_timeout=300.0, bootstrap_timeout=600.0, instance_timeout=300, request_timeout=30, cpu_clock='3c', bid_multiplier=1, registry_auth='docker hub')</code>","text":""},{"location":"reference/providers/runpod/#skyward.providers.runpod.config.CloudType","title":"<code>skyward.providers.runpod.config.CloudType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>RunPod cloud type options.</p>"},{"location":"reference/providers/runpod/#skyward.providers.runpod.config.CloudType.SECURE","title":"<code>SECURE = 'secure'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/runpod/#skyward.providers.runpod.config.CloudType.COMMUNITY","title":"<code>COMMUNITY = 'community'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/","title":"VastAI","text":""},{"location":"reference/providers/vastai/#skyward.VastAI","title":"<code>skyward.VastAI</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ProviderConfig</code></p> <p>Vast.ai provider configuration.</p> <p>Vast.ai is a GPU marketplace with dynamic offers from various hosts. Unlike traditional cloud providers, instances are Docker containers running on marketplace hosts with varying reliability.</p> <p>SSH keys are automatically detected from ~/.ssh/id_ed25519.pub or ~/.ssh/id_rsa.pub and registered on Vast.ai if needed.</p> <p>Example:     &gt;&gt;&gt; from skyward.providers.vastai import VastAI     &gt;&gt;&gt; config = VastAI(min_reliability=0.95, geolocation=\"US\")</p> <p>Args:     api_key: Vast.ai API key. Falls back to VAST_API_KEY env var.     min_reliability: Minimum host reliability score (0.0-1.0).     min_cuda: Minimum CUDA version (e.g., 12.0). Filters out old/broken hosts.     geolocation: Preferred region/country code (e.g., \"US\", \"EU\").     bid_multiplier: For spot pricing, multiply min bid by this.     instance_timeout: Auto-shutdown in seconds (safety timeout).     docker_image: Base Docker image for containers.     disk_gb: Disk space in GB.     use_overlay: Enable overlay networking for multi-node clusters.     overlay_timeout: Timeout for overlay operations in seconds.     require_direct_port: Only select offers with direct port access (no SSH proxy).     verified_only: Only select offers from verified hosts (default True).</p>"},{"location":"reference/providers/vastai/#skyward.VastAI.api_key","title":"<code>api_key = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.min_reliability","title":"<code>min_reliability = 0.95</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.verified_only","title":"<code>verified_only = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.min_cuda","title":"<code>min_cuda = 12.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.geolocation","title":"<code>geolocation = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.bid_multiplier","title":"<code>bid_multiplier = 1.2</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.instance_timeout","title":"<code>instance_timeout = 300</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.request_timeout","title":"<code>request_timeout = 30</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.docker_image","title":"<code>docker_image = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.disk_gb","title":"<code>disk_gb = 100</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.use_overlay","title":"<code>use_overlay = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.overlay_timeout","title":"<code>overlay_timeout = 120</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.require_direct_port","title":"<code>require_direct_port = False</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.type","title":"<code>type</code>  <code>property</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.create_provider","title":"<code>create_provider()</code>  <code>async</code>","text":""},{"location":"reference/providers/vastai/#skyward.VastAI.ubuntu","title":"<code>ubuntu(version='24.04', cuda='12.9.1', cuda_dist='runtime')</code>  <code>classmethod</code>","text":"<p>Generate NVIDIA CUDA Docker image name.</p>"},{"location":"reference/providers/vastai/#skyward.VastAI.__init__","title":"<code>__init__(api_key=None, min_reliability=0.95, verified_only=True, min_cuda=12.0, geolocation=None, bid_multiplier=1.2, instance_timeout=300, request_timeout=30, docker_image=None, disk_gb=100, use_overlay=True, overlay_timeout=120, require_direct_port=False)</code>","text":""},{"location":"reference/providers/verda/","title":"Verda","text":""},{"location":"reference/providers/verda/#skyward.Verda","title":"<code>skyward.Verda</code>  <code>dataclass</code>","text":"<p>               Bases: <code>ProviderConfig</code></p> <p>Verda Cloud provider configuration.</p> <p>SSH keys are automatically detected from ~/.ssh/id_ed25519.pub or ~/.ssh/id_rsa.pub and registered on Verda if needed.</p> <p>Features:     - Auto-region discovery: If the requested region doesn't have capacity,       automatically finds another region with availability.     - Spot instances: Supports spot pricing for cost savings.</p> <p>Example:     &gt;&gt;&gt; from skyward.providers.verda import Verda     &gt;&gt;&gt; config = Verda(region=\"FIN-01\")</p> <p>Args:     region: Preferred region (e.g., \"FIN-01\"). Default: FIN-01.     client_id: Verda client ID. Falls back to VERDA_CLIENT_ID env var.     client_secret: Verda client secret. Falls back to VERDA_CLIENT_SECRET env var.     ssh_key_id: Specific SSH key ID to use (optional).     instance_timeout: Safety timeout in seconds. Default: 300.</p>"},{"location":"reference/providers/verda/#skyward.Verda.region","title":"<code>region = 'FIN-01'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/verda/#skyward.Verda.client_id","title":"<code>client_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/verda/#skyward.Verda.client_secret","title":"<code>client_secret = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/verda/#skyward.Verda.ssh_key_id","title":"<code>ssh_key_id = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/verda/#skyward.Verda.instance_timeout","title":"<code>instance_timeout = 300</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/verda/#skyward.Verda.request_timeout","title":"<code>request_timeout = 30</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"reference/providers/verda/#skyward.Verda.type","title":"<code>type</code>  <code>property</code>","text":""},{"location":"reference/providers/verda/#skyward.Verda.create_provider","title":"<code>create_provider()</code>  <code>async</code>","text":""},{"location":"reference/providers/verda/#skyward.Verda.__init__","title":"<code>__init__(region='FIN-01', client_id=None, client_secret=None, ssh_key_id=None, instance_timeout=300, request_timeout=30)</code>","text":""}]}